[[ch-admin-drbdmanage]]
== DRBD Manage

IMPORTANT: DRBD Manage will reach its EoL end of 2018 and will be replaced by LINSTOR. Currently, LINSTOR is
in alpha state. As soon as it is usable for customers we will add a section to the Users Guide describing LINSTOR.

DRBD Manage is an abstraction layer which takes over management of logical
volumes (LVM) and management of configuration files for DRBD. Features of
DRBD Manage include creating, resizing, and removing of replicated volumes.
Additionally, DRBD Manage handles taking snapshots and creating volumes in
consistency groups.

This chapter outlines typical administrative tasks encountered during
day-to-day operations. It does not cover troubleshooting tasks, these
are covered in detail in <<ch-troubleshooting>>. If you plan to use 'LVM' as storage plugin, please see read
section <<s-config-lvm>> *now*, and then return to this point.


[[s-dm-init-cluster]]
=== Initializing your cluster
We assume that the following steps are accomplished on *all* cluster nodes:

. The DRBD9 kernel module is installed and loaded
. `drbd-utils` are installed
. `LVM` tools are installed
. `drbdmanage` and its dependencies are installed

Note that drbdmanage uses dbus-activation to start its server component when necessary, do not start the
server manually.

The first step is to review the configuration file of `drbdmanage`
(`/etc/drbdmanaged.cfg`) and to create an _LVM volume_ group with the name
specified in the configuration. In the following we use the default name,
which is `drbdpool`, and assume that the volume group consists of `/dev/sda6`
and `/dev/sda7`. Creating the volume group is a step that has to be executed on
*every* cluster node:

----------------------------
# vgcreate drbdpool /dev/sda6 /dev/sda7
----------------------------

The second step is to initialize the so called control volume, which is then
used to redundantly store your cluster configuration. If the
node has multiple interfaces, you have to specify the IP address of the
network interface that DRBD should use to communicate with other nodes in the
cluster, otherwise the IP is optional. This step must only be done on exactly one cluster node.

----------------------------
# drbdmanage init 10.43.70.2
----------------------------

We recommend using 'drbdpool' as the name of your LVM volume group as it is
the default value and makes your administration life easier. If, for whatever
reason, you decide to use a different name, make sure that the option
*drbdctrl-vg* is set accordingly in `/etc/drbdmanaged.cfg`. Configuration will
be discussed in <<s-dm-set-config>>.

[[s-dm-add-node]]
=== Adding nodes to your cluster
Adding nodes to your cluster is easy and requires a single command with two parameters:

. A node name which *must* match the output of `uname -n`
. The IP address of the node.
+
--

.Note
If DNS is configured properly, the tab-completion of `drbdmanage` is able to 
complete the IP of the given node name.
--

----------------------------
# drbdmanage add-node bravo 10.43.70.3
----------------------------

Here we assume that the command was executed on node 'alpha'. If the 'root'
user is allowed to execute commands as 'root' on 'bravo' via `ssh`, then the
node 'bravo' will automatically join your cluster.

If `ssh` access with public-key authentication is not possible, `drbdmanage`
will print a join command that has to be executed on node 'bravo'. You can
always query `drbdmanage` to output the join command for a specific node:

----------------------------
# drbdmanage howto-join bravo
# drbdmanage join -p 6999 10.43.70.3 1 alpha 10.43.70.2 0 cOQutgNMrinBXb09A3io
----------------------------

[[s-types_of_drbd_manage_nodes]]
==== Types of DRBD Manage nodes

There are quite a few different types of DRBD Manage nodes; please see the diagram below.

.DRBD Manage node types
image::images/drbdmanage-venn.svg[]

The rational behind the different types of nodes is as follows:
Currently, a DRBD9/DRBD Manage cluster is limited to ~30 nodes. This is the current DRBD9 limit of nodes for a
replicated resource. As DRBD Manage uses a DRBD volume itself (i.e., the control volume) to distribute the
cluster information, DRBD Manage was also limited by the maximum number of DRBD9 nodes per resource.

The satellites concept relaxes that limit by splitting the cluster into:

* Control nodes: Nodes having direct access to the control volume.
* Satellite nodes: Nodes that are not directly connected to to the control volume, but are able to receive the
content of the control volume via a control node.

In a cluster there is one special node, which we call the "leader". The leader is selected from the set of
control nodes and it is the only node that writes data to the control volume (i.e., it has the control volume
in DRBD Primary role). All the other control nodes in the cluster automatically switch their role to a
"satellite node" and receive their cluster information via TCP/IP, like ordinary satellite nodes. If the
current leader node fails, the cluster automatically selects a new leader node among the control nodes.

*Control nodes* have:

* Direct access to the control volume
* One of them is in the leader role, the rest act like satellite nodes.
* Local storage if it is a normal control node
* No local storage if it is a pure controller

*Satellite nodes* have:

* No direct access to the control volume, they receive a copy of the cluster configuration via TCP/IP.
* Local storage if it is a normal satellite node
* No local storage if it is a pure client

.Cluster consisting of satellite nodes
image::images/satellitecluster.svg[]

*External nodes*:

* Have no access to the control volume at all (no dedicated TCP/IP connection to a control node) and no local storage
* Gets its configuration via a different channel (e.g., DRBD configuration via `scp`)
* These are not the droids you are looking for, if you are not sure if you want to use that type of nodes.

[[s-adding_a_control_node]]
==== Adding a control node

----------------------------
# drbdmanage add-node bravo 10.43.70.3
----------------------------

[[s-adding_a_pure_controller_node]]
==== Adding a pure controller node

----------------------------
# drbdmanage add-node --no-storage bravo 10.43.70.3
----------------------------

[[s-adding_a_satellite_node]]
==== Adding a satellite node
Here we assume that the node charlie was not added to the cluster so far. The following command adds charlie
as a satellite node.

----------------------------
# drbdmanage add-node --satellite charlie 10.43.70.4
----------------------------

[[s-adding_a_pure_client_node]]
==== Adding a pure client node

----------------------------
# drbdmanage add-node --satellite --no-storage charlie 10.43.70.4
----------------------------

[[s-adding_an_external_node]]
==== Adding an external node

----------------------------
# drbdmanage add-node --external delta 10.43.70.5
----------------------------


[[s-dm-set-config]]
=== Cluster configuration
Drbdmanage knows many configuration settings like the log-level or the storage
plugin that should be used (i.e., LVM, ThinLV, ThinPool, ZPool, or ThinZpool). Executing
`drbdmanage modify-config` starts an editor that is used to specify theses
settings. The configuration is split in several sections. If an option is
specified in the `[GLOBAL]` section, this setting is used in the entire
cluster. Additionally, it is possible to specify settings per node and per
site. Node sections follow a syntax of `[Node:nodename]`. If an option is set
globally and per node, the node setting overrules the global setting.

It is also possible to group nodes into *sites*. In order to make node 'alpha'
part of site 'mysite', you have to specify the 'site' option in alpha's node
section:

----------------------------
# drbdmanage modify-config
[Node:alpha]
site = mysite
----------------------------

It is then also possible to specify drbdmanage settings per site using
`[Site:]` sections. Lets assume that you want to set the 'loglevel' option in
general to 'INFO', for site 'mysite' to 'WARN' and for node alpha, which is also
part of site 'mysite' to DEBUG. This would result in the following
configuration:

----------------------------
# drbdmanage modify-config
[GLOBAL]
loglevel = INFO

[Site:mysite]
loglevel = WARN

[Node:alpha]
site = mysite
loglevel = DEBUG
----------------------------

By executing `drbdmanage modify-config` without any options, you can edit
global, per site and per node settings. It is also possible to execute
'modify-config' for a specific node. In this per-node view, it is possible to
set further per-node specific settings like the storage plugin discussed in
<<s-drbdmanage-storage-plugins>>.

[[s-drbdmanage-storage-plugins]]
=== Configuring storage plugins
Storage plugins are *per* *node* settings that are set with the help of the 'modify-config' sub command.

Lets assume you want to use the 'ThinLV' plugin for node 'bravo', where you want to set the 'pool-name' option
to 'mythinpool':

----------------------------
# drbdmanage modify-config --node bravo
[GLOBAL]
loglevel = INFO

[Node:bravo]
storage-plugin = drbdmanage.storage.lvm_thinlv.LvmThinLv

[Plugin:ThinLV]
pool-name = mythinpool
----------------------------

[[s-config-lvm]]
==== Configuring LVM
More recent versions of the 'LVM tools' support detecting of file system signatures. Unfortunately the feature
set of `lvcreate` varies a lot between distributions: Some of them support `--wipesignatures`, some support
`--yes`, and that in all possible combinations. None of them supports a generic force flag. If
`lvcreate` detects an existing file system signature, it prompts for input and therefore halts processing. If
you use modern 'LVM tools', set this option in `/etc/lvm/lvm.conf`: `wipe_signatures_when_zeroing_new_lvs = 0`.
Drbdmanage itself executes `wipefs` on created block devices.

If you use a version of 'LVM' where resources from snapshots are not activated, which we saw for the
'LvmThinPool' plugin, also set `auto_set_activation_skip = 0` in `/etc/lvm/lvm.conf`.

[[s-configuring_zfs]]
==== Configuring ZFS
For ZFS the same configuration steps apply, like setting the 'storage-plugin' for the node that should make
use of ZFS volumes. Please note that we don't make use of ZFS as a file system, but of ZFS as a logical volume
manager. The admin is then free to create any file system she/he desires on top of the DRBD device backed by a
ZFS volume. It is also important to note that if you make use of the ZFS plugin, all DRBD resources are
created on ZFS, but in case this node is a control node, it still needs LVM for it's control volume.

In the most common case only the following steps are necessary.

----------------------------
# zpool create drbdpool /dev/sdX /dev/sdY
# drbdmanage modify-config --node bravo
[Node:bravo]
storage-plugin = drbdmanage.storage.zvol2.Zvol2
----------------------------

CAUTION: Currently it is not supported to switch storage plugins on the fly.
The workflow is: Add a new node, modify the configuration for that node, make
use of the node. Changing other settings (like the log-level) on the fly is
perfectly fine.

[[s-discussion_of_the_storage_plugins]]
==== Discussion of the storage plugins

indexterm:[drbdmanage, storage plugins]

DRBD Manage has four supported storage plugins as of this writing:

  * Thick LVM (`drbdmanage.storage.lvm.Lvm`);

  * Thin LVM with a single thin pool (`drbdmanage.storage.lvm_thinlv.LvmThinLv`)

  * Thin LVM with thin pools for each volume (`drbdmanage.storage.lvm_thinpool.LvmThinPool`)

  * Thick ZFS (`drbdmanage.storage.zvol2.Zvol2`)

  * Thin ZFS (`drbdmanage.storage.zvol2_thinlv.ZvolThinLv2`)

For ZFS also legacy plugins (without the "2") exist. New users, and users that did not uses ZFS snapshots
should use/switch to the newer version. An on-the-fly storage plugin switch is supported in this particular
case.

Here's a short discussion of the relative advantages and disadvantages of these plugins.


[[t-drbdmanage-storage-plugins]]
.DRBD Manage storage plugins, comparison
[cols="^e,^,^,^", options="header"]
|===================================
|Topic | `lvm.Lvm` | `lvm_thinlv.LvmThinLv` | `lvm_thinpool.LvmThinPool`
|Pools | the VG is the pool | a single Thin pool | one Thin pool for each volume
|Free Space reporting | Exact | Free space goes down as per written data and snapshots, needs monitoring | Each pool carves some space out of the VG, but still needs to be monitored if snapshots are used 
|Allocation | Fully pre-allocated   2+| thinly allocated, needs nearly zero space initially
|Snapshots | -- not supported --  2+| Fast, efficient (copy-on-write)
|Stability | Well established, known code, very stable  2+| Some kernel versions have bugs re Thin LVs, destroying data
|Recovery | Easiest - text editor, and/or lvm configuration archives in `/etc/lvm/`, in the worst case `dd` with offset/length | All data in one pool, might incur running `thin_check` across *everything* (needs CPU, memory, time) | Independent Pools, so not all volumes damaged at the same time, faster `thin_check` (less CPU, memory, time)
|===================================


[[s-dm-new-volume]]
=== Creating and deploying resources/volumes
In the following scenario we assume that the goal is to create a resource
'backups' with a size of '500 GB' that is replicated among 3 cluster nodes.
First we show how to achieve the goal in individual steps, and then show a
short-cut how to achieve it in a single step:

First, we create a new resource:

----------------------------
# drbdmanage add-resource backups
----------------------------

Second, we create a new volume within that resource:

----------------------------
# drbdmanage add-volume backups 500GB
----------------------------
In case we would not have used 'add-resource' in the first step, `drbdmanage`
would have known that the resource did not exist and it would have created it.

The third step is to deploy the resource to 3 cluster nodes:

----------------------------
# drbdmanage deploy-resource backups 3
----------------------------

In this case `drbdmanage` chooses 3 nodes that fit all requirements best,
which is by default the set of nodes with the most free space in the
`drbdpool` volume group. We will see how to manually assign resources to
specific nodes in a moment.

As deploying a new resource/volume to a set of nodes is a very common task,
`drbdmanage` provides the following short-cut:
----------------------------
# drbdmanage add-volume backups 500GB --deploy 3
----------------------------

Manual deployment can be achieved by *assigning* a resource to specific nodes.
For example if you decide to assign the 'backups' resource to 'bravo' and
'charlie', you should execute the following steps:

----------------------------
# drbdmanage add-volume backups 500GB
# drbdmanage assign-resource backups bravo
# drbdmanage assign-resource backups charlie
----------------------------

[[s-dm-snapshots]]
=== Managing snapshots
In the following we assume that the _ThinLV_ plugin is used on all nodes that
have deployed resources from which snapshots should be taken. For further
information on how to configure the storage plugin, please refer to
<<s-dm-set-config>>.

[[s-creating_a_snapshot]]
==== Creating a snapshot
Here we continue the example presented in the previous sections, namely nodes
'alpha', 'bravo', 'charlie', and 'delta' with a resource 'backups' deployed on
the first three nodes. The name of the snapshot will be 'snap_backups', and we
want the snapshot to be taken on nodes 'bravo' and 'charlie'.

----------------------------
# drbdmanage create-snapshot snap_backups backups bravo charlie
----------------------------

[[s-restoring_a_snapshot]]
==== Restoring a snapshot
In the following we want to restore the content of the snapshot 'snap_backups'
to a new resource named 'res_backup_from_snap'.

----------------------------
# drbdmanage restore-snapshot res_backup_from_snap backups snap_backups
----------------------------

This will create a new resource with the name 'res_backup_from_snap'. This
resource then gets automatically deployed to these nodes where currently the
resource 'backups' is deployed.

[[s-removing_a_snapshot]]
==== Removing a snapshot
An existing snapshot can be removed as follows:
----------------------------
# drbdmanage remove-snapshot backups snap_backups
----------------------------


[[s-dm-status]]
=== Checking the state of your cluster
`Drbdmanage` provides various commands to check the state of your cluster.
These commands start with a 'list-' prefix and provide various filtering and
sorting options. The '--groupby' option can be used to group and sort the
output in multiple dimensions. Additional output can be turned on by using the
'--show' option. In the following we show some typical examples:

----------------------------
# drbdmanage list-nodes
# drbdmanage list-volumes --groupby Size
# drbdmanage list-volumes --groupby Size --groupby Minor
# drbdmanage list-volumes --groupby Size --show Port
----------------------------

[[s-dm-setupopts]]
=== Setting options for resources
Currently, it is possible to set the following `drbdsetup` options:

. net-options
. peer-device-options
. disk-options
. resource-options

Additionally, it is possible to set DRBD event handler.

As for example _net-options_ are allowed in the 'common' section as well as per
resource, these commands then provide the according switches.

Setting `max-buffers` for a resource 'backups' looks like this:

----------------------------
# drbdmanage net-options --max-buffers 2048 --resource backups
----------------------------

Setting this option in the common section looks like this:

----------------------------
# drbdmanage net-options --max-buffers 2048 --common
----------------------------

Additionally, there is always an '--unset-' option for every option that can
be specified. So, unsetting `max-buffers` for a resource 'backups' looks like
this:

----------------------------
# drbdmanage net-options --unset-max-buffers --resource backups
----------------------------

It is possible to visualize currently set options with the 'show-options'
subcommand.

Setting _net-options_ per site is also supported. Lets assume 'alpha' and
'bravo' should be part of site 'first' and 'charlie' and 'delta' should be part of
site 'second'. Further, we want to use DRBD protocol 'C' within the two sites, and
protocol 'A' between the sites 'first' and 'second'. This would be set up as follows:

----------------------------
# drbdmanage modify-config
[Node:alpha]
site = first

[Node:bravo]
site = first

[Node:charlie]
site = second

[Node:delta]
site = second
----------------------------

----------------------------
# drbdmanage net-options --protocol C --sites 'first:first'
# drbdmanage net-options --protocol C --sites 'second:second'
# drbdmanage net-options --protocol A --sites 'first:second'
----------------------------

The '--sites' parameter follows a 'from:to' syntax, where currently 'from' and
'to' have a symetric semantic. Setting an option from 'first:second' also sets this
option from 'second:first'.

DRBD event handler can be set in the 'common' section and per resource:
----------------------------
# drbdmanage handlers --common --after-resync-target /path/to/script.sh
----------------------------
----------------------------
# drbdmanage handlers --common --unset-after-resync-target
----------------------------
----------------------------
# drbdmanage handlers --resource backups --after-resync-target /path/to/script.sh
----------------------------


[[s-dm-rebalance]]
=== Rebalancing data with DRBD Manage

indexterm:[rebalance]Rebalancing data means moving some assignments around, to make better use of 
the available resources. We'll discuss the same example as for the
<<s-rebalance-workflow,manual workflow>>.


Given is an example policy that data needs to be available on 3 nodes,
so you need at least 3 servers for your setup.

Now, as your storage demands grow, you will encounter the need for 
additional servers. Rather than having to buy 3 more servers at the same 
time, you can _rebalance_ your data across a single additional node.

.DRBD data rebalancing
image::images/rebalance.svg[]

First, you need to add the new machine to the cluster; see <<s-dm-add-node>> 
for the commands.

The next step is to add the assignment:

---------------------
# drbdmanage assign <resource> <new-node>
---------------------

Now you need to wait for the (initial) sync to finish; you can eg. use the 
command `drbdadm status` with (optionally) the resource name.

One of the nodes that _still_ has the data will show a status like

--------
replication:SyncSource peer-disk:Inconsistent done:5.34
--------

while the target node will have a state of _SyncTarget_.


When the target assignment reaches a state of _UpToDate_, you have a full 
additional copy of your data on this node; now it is safe to remove the 
assignment from another node:

---------------------
# drbdmanage unassign <resource> <old-node>
---------------------

And voilà - you moved one assignment, in twofootnote:[Or three, if you count 
waiting for the _UpToDate_ state.] easy steps!


[[s-dm-getting-help]]
=== Getting help
The easiest way to get an overview about drbdmanage's subcommands is to read
the main man-page (`man drbdmanage`).

A quick way to list available commands on the command line is to type
`drbdmanage list`.

Further information on subcommands (e.g., list-nodes) can be retrieved in
three ways:
----------------------------
# man drbdmanage-list-nodes
# drbdmanage list-nodes -h
# drbdmanage help list-nodes
----------------------------

Using the 'help' subcommand is especially helpful when drbdmanage is executed
in interactive mode (`drbdmanage interactive`).

One of the most helpful features of drbdmanage is its rich tab-completion,
which can be used to complete basically every object drbdmanage knows about
(e.g., node names, IP addresses, resource names, ...).
In the following we show some possible completions, and their results:

----------------------------
# drbdmanage add-node alpha 1<tab> # completes the IP address if hostname can be resolved
# drbdmanage assign-resource b<tab> c<tab> # drbdmanage assign-resource backups charlie
----------------------------

If tab-completion does not work out of the box, please try to source the
according file:

----------------------------
# source /etc/bash_completion.d/drbdmanage # or
# source /usr/share/bash_completion/completions/drbdmanage
----------------------------


