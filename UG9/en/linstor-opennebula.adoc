[[ch-opennebula-linstor]]
== LINSTOR Volumes in OpenNebula

indexterm:[OpenNebula]This chapter describes DRBD in OpenNebula using
the https://github.com/OpenNebula/addon-linstor[LINSTOR storage driver
add-on].

Detailed installation and configuration instructions and be found in the
https://github.com/OpenNebula/addon-linstor/blob/master/README.md[README.md]
file of the driver's source.

[[s-opennebula-linstor-overview]]
=== Introduction to OpenNebula

http://opennebula.org/[OpenNebula] is a flexible and open source cloud
management platform which allows its functionality to be extended using
add-ons.

The LINSTOR add-on allows the deployment of virtual machines with highly
available images backed by DRBD and attached across the network through DRBD's
own transport protocol.

[[s-opennebula-linstor-install]]
=== Installing the OpenNebula Add-on

Installation of the LINSTOR storage add-on for OpenNebula requires a working OpenNebula cluster as well as a working LINSTOR cluster.

With access to LINBIT's customer repositories you can install the linstor-opennebula package with:

----
# apt install linstor-opennebula
----

or

----
# yum install linstor-opennebula
----

Without access to LINBIT's prepared packages you need to fall back to instructions on the
https://github.com/OpenNebula/addon-linstor[OpenNebula LINSTOR Add-on GitHub page].

A DRBD cluster with LINSTOR can be installed and configured by following the
instructions in this guide, see <<s-linstor-init-cluster>>.

The OpenNebula and DRBD clusters can be somewhat independent of one another
with the following exception: OpenNebula's Front-End and Host nodes must be
included in both clusters.

Host nodes do not need a local LINSTOR storage pool, as virtual machine
images are attached to them across the network footnote:[If a host is also a
storage node, it will use a local copy of an image if that is available].

[[s-opennebula-deployment-options]]
=== Deployment Options

It is recommended to use LINSTOR resource groups to configure the deployment
how you like it, see <<s-opennebula-resource-group>>.
Previous auto-place and deployment nodes modes are deprecated.


[[s-opennebula-configuration]]
=== Configuring the OpenNebula Add-on

==== Adding the Driver to OpenNebula

Modify the following sections of `/etc/one/oned.conf`

Add linstor to the list of drivers in the `TM_MAD` and `DATASTORE_MAD`
sections:

----
TM_MAD = [
    EXECUTABLE = "one_tm",
    ARGUMENTS = "-t 15 -d dummy,lvm,shared,fs_lvm,qcow2,ssh,vmfs,ceph,linstor"
]
----
----
DATASTORE_MAD = [
    EXECUTABLE = "one_datastore",
    ARGUMENTS  = "-t 15 -d dummy,fs,lvm,ceph,dev,iscsi_libvirt,vcenter,linstor -s shared,ssh,ceph,fs_lvm,qcow2,linstor"
]
----

NOTE: `linstor` is specified *twice* in the `DATASTORE_MAD` arguments.

Add new TM_MAD_CONF and DS_MAD_CONF sections:

----
TM_MAD_CONF = [
    NAME = "linstor", LN_TARGET = "NONE", CLONE_TARGET = "SELF", SHARED = "yes", ALLOW_ORPHANS="yes",
    TM_MAD_SYSTEM = "ssh,shared", LN_TARGET_SSH = "NONE", CLONE_TARGET_SSH = "SELF", DISK_TYPE_SSH = "BLOCK",
    LN_TARGET_SHARED = "NONE", CLONE_TARGET_SHARED = "SELF", DISK_TYPE_SHARED = "BLOCK"
]
----
----
DS_MAD_CONF = [
    NAME = "linstor", REQUIRED_ATTRS = "BRIDGE_LIST", PERSISTENT_ONLY = "NO",
    MARKETPLACE_ACTIONS = "export"
]
----
After making these changes, restart the OpenNebula service.

[[s-opennebula-configuring-nodes]]
==== Configuring the Nodes

The Front-End node issues commands to the Storage and Host nodes through LINSTOR.

Storage nodes hold disk images of VMs locally.

Host nodes are responsible for running instantiated VMs and typically have the
storage for the images they need attached across the network through LINSTOR
diskless mode.

All nodes must have DRBD9 and LINSTOR installed. This process is detailed in the
http://docs.linbit.com/doc/users-guide-90/ch-admin-linstor/[User's Guide for DRBD9]

It is possible to have Front-End and Host nodes act as storage nodes in
addition to their primary role provided that they the meet all the requirements
for both roles.


===== Configuring the Front-end Node

Please verify that the control node(s) that you hope to communicate with are
reachable from the front-end node. `linstor node list` for locally running
LINSTOR controllers and `linstor --controllers "<IP:PORT>" node list` for
remotely running LINSTOR Controllers is a handy way to test this.

===== Configuring Host Nodes

Host nodes must have LINSTOR satellite processes running on them and be members
of the same LINSTOR cluster as the Front-End and Storage nodes, and may optionally
have storage locally. If the `oneadmin` user is able to passwordlessly SSH between
hosts then live migration may be used with the even with the SSH system datastore.

===== Configuring Storage Nodes

Only the front-end and host nodes require OpenNebula to be installed, but the
`oneadmin` user must be able to passwordlessly access storage nodes. Refer to
the OpenNebula installation guide for your distribution on how to manually
configure the `oneadmin` user account.

The storage nodes must use storage pools created with a driver that's capable
of making snapshots, such as the thin LVM plugin.

Given two physical devices (`/dev/sdX` and `/dev/sdY`), the following example shows how you
would prepare thin-provisioned storage, by using LVM commands, to back a LINSTOR storage pool or
pools. The example uses generic names for the volume group and thin pool.

----
pvcreate /dev/sdX /dev/sdY
vgcreate drbdpool /dev/sdX /dev/sdY
lvcreate -l 95%VG --poolmetadatasize 8g -T /dev/drbdpool/drbdthinpool
----

IMPORTANT: Set the thin-provisioned logical volume's metadata space to a reasonable size
because if it becomes full it can be difficult to resize. For this reason, you might also want
to configure monitoring or automatic extension of your LVM thin-provisioned logical volumes.
Refer to the "Size of pool metadata LV" section in `man lvmthin` for more details.

Next, you would create a LINSTOR storage pool or pools using the `/dev/drbdpool/drbdthinpool`
device as the backing storage.

==== Permissions for the Administrative Account

The `oneadmin`, "Cloud Administrator", user account must have passwordless sudo access to the `mkfs` command on
the Storage nodes

----
oneadmin ALL=(root) NOPASSWD: /sbin/mkfs
----

===== Groups

Be sure to consider the groups that `oneadmin` should be added to to
gain access to the devices and programs needed to access storage and
instantiate VMs. For this add-on, the `oneadmin` user must belong to the `disk`
group on all nodes to access the DRBD devices where images are held.

----
usermod -a -G disk oneadmin
----

==== Creating a New LINSTOR Datastore

Create a datastore configuration file named ds.conf and use the `onedatastore`
tool to create a new datastore based on that configuration. There are two
mutually exclusive deployment options: LINSTOR_AUTO_PLACE and
LINSTOR_DEPLOYMENT_NODES. If both are configured, LINSTOR_AUTO_PLACE is ignored.
For both of these options, BRIDGE_LIST must be a space
separated list of all storage nodes in the LINSTOR cluster.

[[s-opennebula-resource-group]]
==== Creating Resource Groups

Since version 1.0.0 LINSTOR supports resource groups. A resource group is a
centralized point for settings that all resources linked to that resource group
share.

Create a resource group and volume group for your datastore, it is mandatory to specify a storage-pool
within the resource group, otherwise monitoring space for OpenNebula will not work.
Here we create one with 2 node redundancy and use a created `opennebula-storagepool`:

----
linstor resource-group create OneRscGrp --place-count 2 --storage-pool opennebula-storagepool
linstor volume-group create OneRscGrp
----

Now add a OpenNebula datastore using the LINSTOR plug-in:

----
cat >ds.conf <<EOI
NAME = linstor_datastore
DS_MAD = linstor
TM_MAD = linstor
TYPE = IMAGE_DS
DISK_TYPE = BLOCK
LINSTOR_RESOURCE_GROUP = "OneRscGrp"
COMPATIBLE_SYS_DS = 0
BRIDGE_LIST = "alice bob charlie"  #node names
EOI

onedatastore create ds.conf
----

==== Plug-in Attributes

===== LINSTOR_CONTROLLERS

`LINSTOR_CONTROLLERS` can be used to pass a comma separated list of controller
ips and ports to the LINSTOR client in the case where a LINSTOR controller
process is not running locally on the Front-End, e.g.:

`LINSTOR_CONTROLLERS = "192.168.1.10:8080,192.168.1.11:6000"`

===== LINSTOR_RESOURCE_GROUP

`LINSTOR_RESOURCE_GROUP` attribute is used to associate an OpenNebula datastore with a LINSTOR resource group.

==== Deprecated Attributes

The following attributes are deprecated and were removed in version 2.0.

[[s-clone-mode]]
===== LINSTOR_CLONE_MODE

LINSTOR now automatically decides which clone mode it should use.

LINSTOR supports two different clone modes: `snapshot` and `copy`. These modes are set through the `LINSTOR_CLONE_MODE` attribute.

The default mode is `snapshot`. It uses a linstor snapshot and restores a new resource from this snapshot, which is then a clone of the image. This mode is usually faster than using the `copy` mode as snapshots are cheap copies.

The second mode is `copy`. It creates a new resource with the same size as the original and copies the data with `dd` to the new resource. This mode will be slower than `snapshot`, but is more robust as it doesn't rely on any snapshot mechanism. It is also used if you are cloning an image into a different LINSTOR datastore.

===== LINSTOR_STORAGE_POOL

`LINSTOR_STORAGE_POOL` attribute is used to select the LINSTOR storage pool your datastore
should use. If resource groups are used this attribute isn't needed as the storage pool
can be select by the auto select filter options.

If `LINSTOR_AUTO_PLACE` or `LINSTOR_DEPLOYMENT_NODES` is used and `LINSTOR_STORAGE_POOL`
is not set, it will fallback to the `DfltStorPool` in LINSTOR.

===== LINSTOR_AUTO_PLACE

The `LINSTOR_AUTO_PLACE` option takes a level of redundancy which is a number between
one and the total number of storage nodes. Resources are assigned to storage
nodes automatically based on the level of redundancy.

===== LINSTOR_DEPLOYMENT_NODES

Using `LINSTOR_DEPLOYMENT_NODES` allows you to select a group of nodes that
resources will always be assigned to. Please note that the
bridge list still contains all of the storage nodes in the LINSTOR cluster.

==== Configuring LINSTOR as a System Datastore

LINSTOR driver can also be used as a system datastore,
configuration is pretty similar to normal datastores, with a few changes:

----
cat >system_ds.conf <<EOI
NAME = linstor_system_datastore
TM_MAD = linstor
TYPE = SYSTEM_DS
DISK_TYPE = BLOCK
LINSTOR_RESOURCE_GROUP = "OneSysRscGrp"
BRIDGE_LIST = "alice bob charlie"  # node names
EOI

onedatastore create system_ds.conf
----

Also add the new sys datastore id to the `COMPATIBLE_SYS_DS` to your image datastores (COMMA separated), otherwise the scheduler will ignore them.

If you want live migration with volatile disks you need to enable the `--unsafe` option for KVM, see:
https://docs.opennebula.org/5.8/deployment/open_cloud_host_setup/kvm_driver.html#live-migration-for-other-cache-settings[opennebula-doc]

[[s-opennebula-linstor-live-migration]]
=== Live Migration

Live migration is supported even with the use of the SSH system datastore, as
well as the nfs shared system datastore.

[[s-opennebula-linstor-free-space]]
=== Free Space Reporting

Free space is calculated differently depending on whether resources are
deployed automatically or on a per node basis.

For datastores which place per node, free space is reported based on
the most restrictive storage pools from all nodes where resources are being
deployed. For example, the capacity of the node with the smallest amount of
total storage space is used to determine the total size of the datastore and
the node with the least free space is used to determine the remaining space in
the datastore.

For a datastore which uses automatic placement, size and remaining space are
determined based on the aggregate storage pool used by the datastore as
reported by LINSTOR.
