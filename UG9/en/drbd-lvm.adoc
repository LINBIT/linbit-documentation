[[ch-lvm]]
== Using LVM with DRBD

indexterm:[LVM]indexterm:[Logical Volume Management]This chapter deals with managing DRBD for
use with LVM2. In particular, this chapter covers how to:

* Use LVM Logical Volumes as backing devices for DRBD.

* Use DRBD devices as Physical Volumes for LVM.

* Combine these two concepts to implement a layered LVM approach using DRBD.

If you are unfamiliar with these terms, the next section, <<s-lvm-primer>>, may serve as a
starting point to learn about LVM concepts. However, you are also encouraged to familiarize
yourself with LVM in more detail than this section provides.

[[s-lvm-primer]]
=== Introduction to LVM

LVM2 is an implementation of logical volume management in the context
of the Linux device mapper framework. It has practically nothing in
common, other than the name and acronym, with the original LVM
implementation. The old implementation (now retroactively named
"LVM1") is considered obsolete; it is not covered in this section.

When working with LVM, it is important to understand its most basic
concepts:

.Physical Volume (PV)
indexterm:[LVM]indexterm:[Physical Volume (LVM)]A PV is an underlying
block device exclusively managed by LVM. PVs can either be entire hard
disks or individual partitions. It is common practice to create a
partition table on the hard disk where one partition is dedicated to
the use by the Linux LVM.

NOTE: The partition type "Linux LVM" (signature `0x8E`) can be used to
identify partitions for exclusive use by LVM. This, however, is not
required -- LVM recognizes PVs by way of a signature written to the
device upon PV initialization.

.Volume Group (VG)
indexterm:[LVM]indexterm:[Volume Group (LVM)]A VG is the basic
administrative unit of the LVM. A VG may include one or more several
PVs. Every VG has a unique name. A VG may be extended during runtime
by adding additional PVs, or by enlarging an existing PV.

.Logical Volume (LV)
indexterm:[LVM]indexterm:[Logical Volume (LVM)]LVs may be created
during runtime within VGs and are available to the other parts of the
kernel as regular block devices. As such, they may be used to hold a
file system, or for any other purpose block devices may be used
for. LVs may be resized while they are online, and they may also be
moved from one PV to another (provided that the PVs are part of the same
VG).

.Snapshot Logical Volume (SLV)
indexterm:[snapshots (LVM)]indexterm:[LVM]Snapshots are temporary
point-in-time copies of LVs. Creating snapshots is an operation that
completes almost instantly, even if the original LV (the _origin
volume_) has a size of several hundred GiByte. Usually, a snapshot
requires significantly less space than the original LV.

[[f-lvm-overview]]
.LVM overview
image::images/lvm.svg[]


[[s-lvm-lv-as-drbd-backing-dev]]
=== Using a Logical Volume as a DRBD Backing Device

indexterm:[LVM]indexterm:[Logical Volume (LVM)]Because an existing LVM logical volume is simply
a block device in Linux terms, you can use one as a DRBD backing device. To use an LVM logical
volume in this way, you simply create one, and then reference it in a DRBD resource
configuration just as you would a physical backing disk.

[IMPORTANT]
====
When using DRBD together with LVM, set the `global_filter` value in the LVM global configuration file (`/etc/lvm/lvm.conf` on RHEL) to:

----
global_filter = [ "r|^/dev/drbd|" ]
----

This setting tells LVM to reject DRBD devices from operations such as scanning or opening attempts. In some cases, not setting this filter might lead to increased CPU load or stuck LVM operations.
====

The following example assumes that an LVM volume group named `foo` already exists on
two nodes of in your LVM-enabled cluster, and that you want to create
a DRBD resource named `r0` using a logical volume in that volume
group.

First, you create the logical volume on both nodes in your cluster:
indexterm:[LVM]indexterm:[lvcreate (LVM command)]
----
# lvcreate --name bar --size 10G foo
Logical volume "bar" created
----

After this, you will have a block device named
`/dev/foo/bar` on both nodes.

Then, you can simply enter the newly created volumes in your resource
configuration:

[source,drbd]
----
resource r0 {
  ...
  on alice {
    device /dev/drbd0;
    disk   /dev/foo/bar;
    ...
  }
  on bob {
    device /dev/drbd0;
    disk   /dev/foo/bar;
    ...
  }
}
----

Now you can <<s-first-time-up,continue to bring your resource up>>,
just as you would if you were using non-LVM block devices.

[[s-lvm-snapshots]]
=== Using Automated LVM Snapshots During DRBD Synchronization

While DRBD is synchronizing, the __SyncTarget__'s state is
_Inconsistent_ until the synchronization completes. If in this
situation the _SyncSource_ happens to fail (beyond repair), this puts
you in an unfortunate position: the node with good data is dead, and
the surviving node has bad (inconsistent) data.

When serving DRBD off an LVM Logical Volume, you can mitigate this
problem by creating an automated snapshot when synchronization starts,
and automatically removing that same snapshot once synchronization has
completed successfully.

To enable automated snapshotting during resynchronization,
add the following lines to your resource configuration:

.Automating snapshots before DRBD synchronization
----
resource r0 {
  handlers {
    before-resync-target "/usr/lib/drbd/snapshot-resync-target-lvm.sh";
    after-resync-target "/usr/lib/drbd/unsnapshot-resync-target-lvm.sh";
  }
}
----

The two scripts parse the `$DRBD_RESOURCE` environment variable which
DRBD automatically passes to any `handler` it invokes. The
`snapshot-resync-target-lvm.sh` script then creates an LVM snapshot for
any volume the resource contains, then synchronization
kicks off. In case the script fails, the synchronization _does not
commence_.

Once synchronization completes, the `unsnapshot-resync-target-lvm.sh`
script removes the snapshot, which is then no longer needed. In case
unsnapshotting fails, the snapshot continues to linger around.

IMPORTANT: You should review dangling snapshots as soon as
possible. A full snapshot causes both the snapshot itself _and its
origin volume_ to fail.

If at any time your _SyncSource_ does fail beyond repair and you
decide to revert to your latest snapshot on the peer, you may do so by
issuing the `lvconvert -M` command.

[[s-lvm-drbd-as-pv]]
=== Configuring a DRBD Resource as a Physical Volume

indexterm:[LVM]indexterm:[Physical Volume (LVM)]To prepare a
DRBD resource for use as a Physical Volume, it is necessary to create
a PV signature on the DRBD device. To do this, issue one of the
following commands on the node where the resource is currently in the
primary role: indexterm:[LVM]indexterm:[pvcreate (LVM command)]

----
# pvcreate /dev/drbdX
----

or

----
# pvcreate /dev/drbd/by-res/<resource>/0
----

NOTE: This example assumes a single-volume resource.

Now, it is necessary to include this device in the list of devices LVM
scans for PV signatures. To do this, you must edit the LVM
configuration file, normally named
indexterm:[LVM]`/etc/lvm/lvm.conf`. Find the line in the
`devices` section that contains the `filter` keyword and edit it
accordingly. If _all_ your PVs are to be stored on DRBD devices, the
following is an appropriate `filter` option:
indexterm:[LVM]indexterm:[filter expression (LVM)]

[source,drbd]
----
filter = [ "a|drbd.*|", "r|.*|" ]
----

This filter expression accepts PV signatures found on any DRBD
devices, while rejecting (ignoring) all others.

NOTE: By default, LVM scans all block devices found in `/dev` for PV
signatures. This is equivalent to `filter = [ "a|.*|" ]`.

If you want to use stacked resources as LVM PVs, then you will need a
more explicit filter configuration. You need to verify that LVM
detects PV signatures on stacked resources, while ignoring them on the
corresponding lower-level resources and backing devices. This example
assumes that your lower-level DRBD resources use device minors 0
through 9, whereas your stacked resources are using device minors from
10 upwards:

[source,drbd]
----
filter = [ "a|drbd1[0-9]|", "r|.*|" ]
----

This filter expression accepts PV signatures found only on the DRBD
devices `/dev/drbd10` through `/dev/drbd19`, while rejecting
(ignoring) all others.

After modifying the `lvm.conf` file, you must run the
indexterm:[LVM]indexterm:[vgscan (LVM command)]`vgscan` command so LVM
discards its configuration cache and re-scans devices for PV
signatures.

You may of course use a different `filter` configuration to match your
particular system configuration. What is important to remember,
however, is that you need to:

* Accept (include) the DRBD devices that you want to use as PVs.

* Reject (exclude) the corresponding lower-level devices, so as to avoid LVM finding duplicate
PV signatures.

In addition, you should disable the LVM cache by setting:

[source,drbd]
----
write_cache_state = 0
----

After disabling the LVM cache, remove any stale cache
entries by deleting `/etc/lvm/cache/.cache`.

You must repeat the above steps on the peer nodes, too.

IMPORTANT: If your system has its root filesystem on LVM, Volume
Groups will be activated from your initial RAM disk (initrd) during
boot. In doing so, the LVM tools will evaluate an `lvm.conf` file
included in the initrd image. Therefore, after you make any changes to your
`lvm.conf`, you should be certain to update your initrd with the
utility appropriate for your distribution (`mkinitrd`,
`update-initramfs`, and so on).

When you have configured your new PV, you may proceed to add it to a
Volume Group, or create a new Volume Group from it. The DRBD resource
must, of course, be in the primary role while doing
so. indexterm:[LVM]indexterm:[vgcreate (LVM command)]

----
# vgcreate <name> /dev/drbdX
----

NOTE: While it is possible to mix DRBD and non-DRBD Physical Volumes
within the same Volume Group, doing so is not recommended and unlikely
to be of any practical value.

When you have created your VG, you may start carving Logical Volumes
out of it, using the indexterm:[LVM]indexterm:[lvcreate (LVM
command)]`lvcreate` command (as with a non-DRBD-backed Volume Group).

[[s-lvm-add-pv]]
=== Adding a New DRBD Volume to an Existing Volume Group

Occasionally, you may want to add new DRBD-backed Physical Volumes to
a Volume Group. Whenever you do so, a new volume should be added to an
existing resource configuration. This preserves the replication stream
and ensures write fidelity across all PVs in the VG.

ifndef::drbd-only[]
IMPORTANT: if your LVM volume group is managed by Pacemaker as
explained in <<s-lvm-pacemaker>>, it is _imperative_ to place the
cluster in maintenance mode prior to making changes to the DRBD
configuration.
endif::drbd-only[]

Extend your resource configuration to include an additional volume, as
in the following example:

----
resource r0 {
  volume 0 {
    device    /dev/drbd1;
    disk      /dev/disk/by-id/nvme-eui.daac3bd0da3b9b2b;
    meta-disk internal;
  }
  volume 1 {
    device    /dev/drbd2;
    disk      /dev/disk/by-id/nvme-eui.9ecf6fb61f451198;
    meta-disk internal;
  }
  on alice {
    address   10.1.1.31:7789;
  }
  on bob {
    address   10.1.1.32:7789;
  }
}
----

IMPORTANT: When not using LVM or ZFS logical volumes to back DRBD volumes, use persistent block
device names, that is, the block device names that you find in `/dev/disk/by-id/`, in your DRBD
resource configurations, as shown in this configuration. This prevents a bus-based name change
from possibly affecting your DRBD resources. You can enter a `lsblk -o Name,UUID,WWN` command to
determine a particular disk's ID.

Verify that your DRBD configuration is identical across nodes, then
issue:

----
# drbdadm adjust r0
----

This will implicitly call `drbdsetup new-minor r0 1` to enable the new volume `1` in the resource `r0`. Once the new
volume has been added to the replication stream, you may initialize
and add it to the volume group:

----
# pvcreate /dev/drbd/by-res/<resource>/1
# vgextend <name> /dev/drbd/by-res/<resource>/1
----

This will add the new PV `/dev/drbd/by-res/<resource>/1` to the
`<name>` VG, preserving write fidelity across the entire VG.

ifndef::drbd-only[]
[[s-lvm-pacemaker]]
=== Highly Available LVM with Pacemaker

The process of transferring volume groups between peers and making the
corresponding logical volumes available can be automated. The
Pacemaker LVM resource agent is designed for exactly that purpose.

To put an existing, DRBD-backed volume group under Pacemaker
management, run the following commands in the `crm` shell:

.Pacemaker configuration for DRBD-backed LVM Volume Group
----
primitive p_drbd_r0 ocf:linbit:drbd \
  params drbd_resource="r0" \
  op monitor interval="29s" role="Master" \
  op monitor interval="31s" role="Slave"
ms ms_drbd_r0 p_drbd_r0 \
  meta master-max="1" master-node-max="1" \
       clone-max="2" clone-node-max="1" \
       notify="true"
primitive p_lvm_r0 ocf:heartbeat:LVM \
  params volgrpname="r0"
colocation c_lvm_on_drbd inf: p_lvm_r0 ms_drbd_r0:Master
order o_drbd_before_lvm inf: ms_drbd_r0:promote p_lvm_r0:start
commit
----

After you have committed this configuration, Pacemaker will
automatically make the `r0` volume group available on whichever node
currently has the Primary (Master) role for the DRBD resource.
endif::drbd-only[]

=== Using DRBD and LVM Without a Cluster Resource Manager

The typical high availability use case for DRBD is to use a cluster resource manager (CRM) to handle the promoting and demoting of resources, such as DRBD replicated storage volumes. However, it is possible to use DRBD without a CRM.

You might want to do this in a situation when you know that you always want a particular node to promote a DRBD resource and you know that the peer nodes are never going to take over but are only being replicated to for disaster recovery purposes.

In this case, you can use a couple of systemd unit files to handle DRBD resource promotion and make sure that back-end LVM logical volumes are activated first. You also need to make the DRBD systemd unit file for your DRBD resource a dependency of whatever file system mount might be using the DRBD resource as a backing device.

To set this up, for example, given a hypothetical DRBD resource named `webdata` and a file system mount point of `/var/lib/www`, you might enter the following commands:

----
# systemctl enable drbd-lvchange@webdata.service
# systemctl enable drbd-wait-promotable@webdata.service
# echo "/dev/drbdX /var/lib/www xfs defaults,nofail,x-systemd.requires=drbd@webdata.target 0 0" \
    >> /etc/fstab
----

In this example, the `X` in `drbdX` is the volume number of your DRBD backing device for the `webdata` resource.

The `drbd-wait-promotable@<DRBD-resource-name>.service` is a systemd unit file that is used to wait for DRBD to connect to its peers and establish access to good data, before DRBD promotes the resource on the node.

