[[ap-upgrading-8.4-to-9.x]]
[appendix]
== Upgrading DRBD From 8.4 to 9.x

This section covers the process of upgrading DRBD from version 8.4.x to 9.x in detail. For
upgrades within version 9, and for special considerations when upgrading to a particular DRBD
9.x version, refer to the <<drbd-upgrading.adoc#s-upgrading-drbd, Upgrading DRBD>> chapter in this guide.

=== Compatibility

DRBD 9.a.b releases are generally protocol compatible with DRBD 8.c.d. In particular, all DRBD
9.a.b releases other than DRBD 9.1.0 to 9.1.7 inclusive are compatible with DRBD 8.c.d.

[[s-upgrade-8.4-to-9.x-overview]]
=== General Overview

The general process for upgrading 8.4 to 9.x is as follows:

* Configure the <<s-updating-your-repo-v8-to-v9,new repositories>> (if using packages from LINBIT).
* Verify that the current situation <<s-upgrade-check-v8-to-v9,is okay>>.
* <<s-upgrade-pausing-the-cluster-v8-to-v9,Pause>> any cluster manager.
* Upgrade packages to install <<s-upgrading-the-packages-v8-to-v9,new versions>> .
* If you want to move to more than two nodes, you will need to resize the lower-level storage to
provide room for the additional metadata. This topic is discussed in the <<ch-lvm,LVM Chapter>>.
* Unconfigure resources, unload DRBD 8.4, and <<s-upgrade-reload-kernel-mod-v8-to-v9,load the v9 kernel
module>>.
* <<s-upgrade-convert-v8-to-v9,Convert DRBD metadata>> to format `v09`, perhaps changing the number of
bitmaps in the same step.
* <<s-upgrade-start-drbd-v8-to-v9,Start the DRBD resources>> and bring the cluster node _online_
again if you are using a cluster manager.

ifndef::de-brand[]
[[s-updating-your-repo-v8-to-v9]]
=== Updating Your Repository

Due to the number of changes between the 8.4 and 9.x branches, LINBIT has created separate
repositories for each. The best way to get LINBIT's software installed on your machines, if you
have a LINBIT customer or evaluation account, is to download a small
https://my.linbit.com/linbit-manage-node.py[Python helper script] and run it on your target
machines.

[[s-linbit-manage-node-script-for-upgrading-drbd]]
==== Using the LINBIT Manage Node Helper Script to Enable LINBIT Repositories

Running the LINBIT helper script will allow you to enable certain LINBIT package repositories. When upgrading
from DRBD 8.4, it is recommended that you enable the `drbd-9` package repository.

NOTE: While the helper script does give you the option of enabling a `drbd-9.0` package
repository, this is not recommended as a way to upgrade from DRBD 8.4, as that branch only contains DRBD 9.0 and related software. It will
likely be discontinued in the future and the DRBD versions 9.1+ that are available in the `drbd-9` package repository are protocol compatible with version
8.4.

To use the script to enable the `drbd-9` repository, refer to the instructions in this guide for
<<drbd-install-packages.adoc#s-linbit-manage-node-script, Using a LINBIT Helper Script to
Register Nodes and Configure Package Repositories>>

[[s-Debian-Systems]]
==== Debian/Ubuntu Systems

When using LINBIT package repositories to update DRBD 8.4 to 9.1+, note that LINBIT currently
only keeps two LTS Ubuntu versions up-to-date: Focal (20.04) and Jammy (22.04). If you are
running DRBD v8.4, you are likely on an older version of Ubuntu Linux than these. Before using
the helper script to add LINBIT package repositories to update DRBD, you would first need to
update your system to a LINBIT supported LTS version.
endif::de-brand[]

[[s-upgrade-check-v8-to-v9]]
=== Checking the DRBD State

Before you update DRBD, verify that your resources are in sync. The output of `cat /proc/drbd`
should show an _UpToDate/UpToDate_ status for your resources.

----
node-2# cat /proc/drbd

version: 8.4.9-1 (api:1/proto:86-101)
GIT-hash: [...] build by linbit@buildsystem, 2016-11-18 14:49:21
GIT-hash: [...] build by linbit@buildsystem, 2016-11-18 14:49:21

 0: cs:Connected ro:Secondary/Secondary ds:UpToDate/UpToDate C r-----
     ns:0 nr:211852 dw:211852 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:d oos:0
----

NOTE: The `cat /proc/drbd` command is deprecated in DRBD versions 9.x for getting resource
status information. After upgrading DRBD, use the `drbdadm status` command to get resource
status information.

[[s-upgrade-pausing-the-cluster-v8-to-v9]]
=== Pausing the Services

Now that you know the resources are in sync, start by upgrading the
secondary node.
This can be done manually or according to your cluster manager's documentation.
ifndef::drbd-only[]
Both processes are covered
below. If you are running Pacemaker as your cluster manager do not use the manual method.
endif::drbd-only[]

==== Manual Method

----
node-2# systemctl stop drbd@<resource>.target
----

IMPORTANT: To use the `systemctl stop` command with a DRBD resource target, you would have
needed to have enabled the `drbd.service` previously. You can verify this by using the
`systemctl is-enabled drbd.service` command.

ifndef::drbd-only[]
==== Pacemaker

Put the secondary node into standby mode. In this example `node-2` is secondary.

----
node-2# crm node standby node-2
----

NOTE: You can watch the status of your cluster using `crm_mon -rf` or watch
`cat /proc/drbd` until it shows _Unconfigured_ for your resources.
endif::drbd-only[]

[[s-upgrading-the-packages-v8-to-v9]]
=== Upgrading the Packages

Now update your packages.

RHEL/CentOS:

----
node-2# dnf -y upgrade
----

Debian/Ubuntu:

----
node-2# apt-get update && apt-get upgrade
----

Once the upgrade is finished you will have the latest DRBD 9.x kernel
module and `drbd-utils` installed on your secondary node, `node-2`.

But the kernel module is not active yet.

[[s-upgrade-reload-kernel-mod-v8-to-v9]]
=== Loading the New Kernel Module

By now the DRBD module should not be in use anymore, so unload it by entering the following
command:

----
node-2# rmmod drbd_transport_tcp; rmmod drbd
----

If there is a message like `ERROR: Module drbd is in use`, then not all
resources have been correctly stopped.

Retry <<s-upgrading-the-packages-v8-to-v9, upgrading packages>>, or run the command `drbdadm down all` to find
out which resources are still active.

Some typical issues that might prevent you from unloading the kernel module are:

  * NFS export on a DRBD-backed filesystem (see `exportfs -v` output)
  * Filesystem still mounted - check `grep drbd /proc/mounts`
  * Loopback device active (`losetup -l`)
  * Device mapper using DRBD, directly or indirectly (`dmsetup ls --tree`)
  * LVM with a DRBD-PV (`pvs`)

NOTE: This list is not complete. These are just the most common examples.

Now you can load the new DRBD module.

----
node-2# modprobe drbd
----

Next, you can verify that the version of the DRBD kernel module that is loaded is the updated
9.x version. If the installed package is for the wrong kernel version, the `modprobe` would be
successful, but output from a `drbdadm --version` command would show that the DRBD kernel
version (`DRBD_KERNEL_VERSION_CODE`) was still at the older 8.4 (`0x08040` in hexadecimal)
version.

The output of `drbdadm --version` should show 9.x.y and look similar
to this:

----
DRBDADM_BUILDTAG=GIT-hash:\ [...]\ build\ by\ @buildsystem\,\ 2022-09-19\ 12:15:10
DRBDADM_API_VERSION=2
DRBD_KERNEL_VERSION_CODE=0x09010b
DRBD_KERNEL_VERSION=9.1.11
DRBDADM_VERSION_CODE=0x091600
DRBDADM_VERSION=9.22.0
----

NOTE: On the primary node, `node-1`, `drbdadm --version` will still show the

[[s-migrating_your_configuration_files]]
=== Migrating Your Configuration Files

DRBD 9.x is backward compatible with the 8.4 configuration files;
however, some
syntax has changed. See <<s-recent-changes-config>> for
a full list of changes. In the meantime you can port your old
configs fairly easily by using `drbdadm dump all` command. This
will output both a new global configuration followed by the
new resource configuration files. Take this output and make changes
accordingly.

[[s-upgrade-convert-v8-to-v9]]

=== Changing the Metadata

Now you need to convert the on-disk metadata to the new version. You can do this by using the
`drbdadm create-md` command and answering two questions.

If you want to change the number of nodes, you should already have increased
the size of the lower level device, so that there is enough space to store the
additional bitmaps; in that case, you would run the command below with an
additional argument `--max-peers=__<N>__`. When determining the number of
(possible) peers please take setups like the <<s-drbd-client>> into account.

----
# drbdadm create-md <resource>
You want me to create a v09 style flexible-size internal meta data block.
There appears to be a v08 flexible-size internal meta data block
already in place on <disk> at byte offset <offset>

Valid v08 meta-data found, convert to v09?
[need to type 'yes' to confirm] yes

md_offset <offsets...>
al_offset <offsets...>
bm_offset <offsets...>

Found some data

 ==> This might destroy existing data! <==

Do you want to proceed?
[need to type 'yes' to confirm] yes

Writing meta data...
New drbd meta data block successfully created.
success
----

Of course, you can pass `all` for the resource names, too. And if you feel
lucky, brave, or both you can avoid the questions by using the `--force` flag like this:

----
drbdadm -v --max-peers=<N>  -- --force create-md <resources>
----

IMPORTANT: The order of these arguments is important. Make sure you understand the potential
data loss implications of this command before you enter it.

[[s-upgrade-start-drbd-v8-to-v9]]
=== Starting DRBD Again

Now, the only thing left to do is to get the DRBD devices up and running again. You can do this by using the `drbdadm up all` command.

Next, depending on whether you are using a cluster manager or if you keep track of your
DRBD resources manually, there are two different ways to bring up your resources. If you are
using a cluster manager follow its documentation.

* Manually
+
----
node-2# systemctl start drbd@<resource>.target
----

ifndef::drbd-only[]
* Pacemaker
+
----
# crm node online node-2
----
endif::drbd-only[]

This should make DRBD connect to the other node, and the resynchronization
process will start.

When the two nodes are _UpToDate_ on all resources again, you can move your
applications to the already upgraded node (here `node-2`), and then follow the
same steps on the cluster node still running version 8.4.

