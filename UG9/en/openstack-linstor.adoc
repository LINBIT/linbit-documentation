[[ch-openstack-linstor]]
== LINSTOR volumes in Openstack

indexterm:[Openstack]indexterm:[Cinder]indexterm:[Nova]indexterm:[LINSTOR]
This chapter describes DRBD in Openstack for persistent, replicated, and
high-performance block storage with
https://github.com/LINBIT/openstack-cinder/tree/stein-linstor[LINSTOR Driver].


[[s-openstack-linstor-overview]]
=== Openstack Overview

Openstack consists of a wide range of individual services; the two that are
mostly relevant to DRBD are Cinder and Nova.  *Cinder* is the block storage
service, while *Nova* is the compute node service that's responsible for making
the volumes available for the VMs.

The LINSTOR driver for OpenStack manages DRBD/LINSTOR clusters and makes
them available within the OpenStack environment, especially within Nova
compute instances.
LINSTOR-backed Cinder volumes will seamlessly provide all the features of
DRBD/LINSTOR while allowing OpenStack to manage all their deployment and
management.  The driver will allow OpenStack to create and delete persistent
LINSTOR volumes as well as managing and deploying volume snapshots and raw
volume images.

Aside from using the kernel-native DRBD protocols for replication, the LINSTOR
driver also allows using iSCSI with LINSTOR cluster(s) to provide maximum
compatibility. For more information on these two options, please see
<<s-openstack-linstor-transport-protocol>>.


[[s-openstack-linstor-install]]
=== LINSTOR for Openstack Installation

An initial installation and configuration of DRBD and LINSTOR must be completed
prior to installing OpenStack driver.
Each LINSTOR node in a cluster should also have a Storage Pool defined as well.
Details about LINSTOR installation can be found <<s-linstor-init-cluster,
here>>.

[[s-here_s_a_synopsis_on_quickly_setting_up_a_linstor_cluster_on_ubuntu]]
==== Here's a synopsis on quickly setting up a LINSTOR cluster on Ubuntu:

[[s-install_drbd_and_linstor_on_cinder_node_as_a_linstor_controller_node]]
===== Install DRBD and LINSTOR on Cinder node as a LINSTOR Controller node:

----
# First, set up LINBIT repository per support contract

# Install DRBD and LINSTOR packages
sudo apt-get update
sudo apt install -y drbd-dkms lvm2
sudo apt install -y linstor-controller linstor-satellite linstor-client
sudo apt install -y drbdtop

# Start both LINSTOR Controller and Satellite Services
systemctl enable linstor-controller.service
systemctl start linstor-controller.service
systemctl enable linstor-satellite.service
systemctl start linstor-satellite.service

# For Diskless Controller, skip the following two 'sudo' commands

# For Diskful Controller, create backend storage for DRBD/LINSTOR by creating
# a Volume Group 'drbdpool' and specify appropriate volume location (/dev/vdb)
sudo vgcreate drbdpool /dev/vdb

# Create a Logical Volume 'thinpool' within 'drbdpool'
# Specify appropriate thin volume size (64G)
sudo lvcreate -L 64G -T drbdpool/thinpool
----

NOTE: OpenStack measures storage size in GiBs.

[[s-install_drbd_and_linstor_on_other_nodes_on_the_linstor_cluster]]
===== Install DRBD and LINSTOR on other node(s) on the LINSTOR cluster:

----
# First, set up LINBIT repository per support contract

# Install DRBD and LINSTOR packages
sudo apt-get update
sudo apt install -y drbd-dkms lvm2
sudo apt install -y linstor-satellite
sudo apt install -y drbdtop

# Start only the LINSTOR Satellite service
systemctl enable linstor-satellite.service
systemctl start linstor-satellite.service

# Create backend storage for DRBD/LINSTOR by creating a Volume Group 'drbdpool'
# Specify appropriate volume location (/dev/vdb)
sudo vgcreate drbdpool /dev/vdb

# Create a Logical Volume 'thinpool' within 'drbdpool'
# Specify appropriate thin volume size (64G)
sudo lvcreate -L 64G -T drbdpool/thinpool
----

[[s-lastly_from_the_cinder_node_create_linstor_satellite_nodes_and_storage_pools]]
===== Lastly, from the Cinder node, create LINSTOR Satellite Node(s) and Storage Pool(s)

----
# Create a LINSTOR cluster, including the Cinder node as one of the nodes
# For each node, specify node name, its IP address, volume type (diskless) and
# volume location (drbdpool/thinpool)

# Create the controller node as combined controller and satellite node
linstor node create cinder-node-name 192.168.1.100 --node-type Combined

# Create the satellite node(s)
linstor node create another-node-name 192.168.1.101
# repeat to add more satellite nodes in the LINSTOR cluster

# Create LINSTOR Storage Pool on each nodes
# For each node, specify node name, its IP address,
# storage pool name (DfltStorPool),
# volume type (diskless / lvmthin) and node type (Combined)

# Create diskless Controller node on the Cinder controller
linstor storage-pool create diskless cinder-node-name DfltStorPool

# Create diskful Satellite nodes
linstor storage-pool create lvmthin another-node-name DfltStorPool drbdpool/thinpool
# repeat to add a storage pool to each node in the LINSTOR cluster
----

[[s-install_the_linstor_driver_file]]
==== Install the LINSTOR driver file

The _linstor driver_ will be officially available starting OpenStack Stein
release. The latest release is located at
https://github.com/LINBIT/openstack-cinder/blob/stein-linstor/cinder/volume/drivers/linstordrv.py[LINBIT OpenStack Repo].
It is a single Python file called *linstordrv.py*.  Depending on your OpenStack
installation, its destination may vary.

Place the driver ( linstordrv.py ) in an appropriate location within your
OpenStack Cinder node.

For Devstack:

----
/opt/stack/cinder/cinder/volume/drivers/linstordrv.py
----

For Ubuntu:

----
/usr/lib/python2.7/dist-packages/cinder/volume/drivers/linstordrv.py
----

For RDO Packstack:

----
/usr/lib/python2.7/site-packages/cinder/volume/drivers/linstordrv.py
----


[[s-openstack-cinder-linstor-install]]
=== Cinder Configuration for LINSTOR

[[s-edit_cinder_configuration_file_cinder_conf_in__etc_cinder__as_follows]]
==== Edit Cinder configuration file *cinder.conf* in /etc/cinder/ as follows:

[[s-enable_linstor_driver_by_adding__linstor__to_enabled_backends]]
===== Enable LINSTOR driver by adding 'linstor' to enabled_backends

----
[DEFAULT]
...
enabled_backends=lvm, linstor
...
----

[[s-add_the_following_configuration_options_at_the_end_of_the_cinder_conf]]
===== Add the following configuration options at the end of the cinder.conf

----
[linstor]
volume_backend_name = linstor
volume_driver = cinder.volume.drivers.linstordrv.LinstorDrbdDriver
linstor_default_volume_group_name=drbdpool
linstor_default_uri=linstor://localhost
linstor_default_storage_pool_name=DfltStorPool
linstor_default_resource_size=1
linstor_volume_downsize_factor=4096
----

[[s-update_python_python_libraries_for_the_driver]]
==== Update Python python libraries for the driver

----
sudo pip install google --upgrade
sudo pip install protobuf --upgrade
sudo pip install eventlet --upgrade
----

[[s-create_a_new_backend_type_for_linstor]]
==== Create a new backend type for LINSTOR
Run these commands from the Cinder node once environment variables are
configured for OpenStack command line operation.

----
cinder type-create linstor
cinder type-key linstor set volume_backend_name=linstor
----

[[s-restart_the_cinder_services_to_finalize]]
==== Restart the Cinder services to finalize

For Devstack:

----
sudo systemctl restart devstack@c-vol.service
sudo systemctl restart devstack@c-api.service
sudo systemctl restart devstack@c-sch.service
----

For RDO Packstack:

----
sudo systemctl restart openstack-cinder-volume.service
sudo systemctl restart openstack-cinder-api.service
sudo systemctl restart openstack-cinder-scheduler.service
----

For full OpenStack:

----
sudo systemctl restart cinder-volume.service
sudo systemctl restart cinder-api.service
sudo systemctl restart cinder-scheduler.service
----

[[s-verify_proper_installation]]
==== Verify proper installation:
Once the Cinder service is restarted, a new Cinder volume with LINSTOR
backend may be created using the Horizon GUI or command line.  Use following
as a guide for creating a volume with the command line.

----
# Check to see if there are any recurring errors with the driver.
# Occasional 'ERROR' keyword associated with the database is normal.
# Use Ctrl-C to stop the log output to move on.
sudo systemctl -f -u devstack@c-* | grep error

# Create a LINSTOR test volume.  Once the volume is created, volume list
# command should show one new Cinder volume.  The 'linstor' command then
# should list actual resource nodes within the LINSTOR cluster backing that
# Cinder volume.
openstack volume create --type linstor --size 1 --availability-zone nova linstor-test-vol
openstack volume list
linstor resource list
----

[[s-openstack-linstor-addtl-conf]]
==== Additional Configuration

More to come


[[s-openstack-linstor-transport-protocol]]
=== Choosing the Transport Protocol

There are two main ways to run DRBD/LINSTOR with Cinder:

  * accessing storage via <<s-openstack-linstor-iscsi,iSCSI exports>>, and

  * using <<s-openstack-linstor-drbd,the DRBD transport protocol>> on the wire with
  LINSTOR.

These are not exclusive; you can define multiple backends, have some of them
use iSCSI, and others the DRBD protocol.


[[s-openstack-linstor-iscsi]]
==== iSCSI Transport

The default way to export Cinder volumes is via iSCSI. This brings the
advantage of maximum compatibility - iSCSI can be used with every hypervisor,
be it VMWare, Xen, HyperV, or KVM.

The drawback is that all data has to be sent to a Cinder node, to be processed
by an (userspace) iSCSI daemon; that means that the data needs to pass the
kernel/userspace border, and these transitions will cost some performance.


[[s-openstack-linstor-drbd]]
==== DRBD/LINSTOR Transport

The alternative is to get the data to the VMs by using DRBD as the transport
protocol. This means that DRBD 9footnote:[LINSTOR must be installed on Cinder
node. Please see the note at <<s-openstack-linstor-drbd-external-NOTE>>.]
needs to be installed on the Cinder node as well.

[NOTE]
Since OpenStack only functions in Linux, using DRBD/LINSTOR Transport restricts
deployment only on Linux hosts with KVM at the moment.

One advantage of that solution is that the storage access requests of the VMs
can be sent via the DRBD kernel module to the storage nodes, which can then
directly access the allocated LVs; this means no Kernel/Userspace transitions
on the data path, and consequently better performance. Combined with RDMA
capable hardware you should get about the same performance as with VMs
accessing a FC backend directly.

Another advantage is that you will be implicitly benefitting from the HA
background of DRBD: using multiple storage nodes, possibly available over
different network connections, means redundancy and avoiding a single point
of failure.


[[s-openstack-linstor-drbd-external-NOTE]]
[NOTE]
--
Default configuration options for Cinder driver assumes the Cinder node
to be a *Diskless* LINSTOR node.  If the node is a *Diskful* node, please
change the 'linstor_controller_diskless=True' to
'linstor_controller_diskless=False' and restart the Cinder services.
--


[[s-openstack-linstor-conf-transport-protocol]]
==== Configuring the Transport Protocol

In the LINSTOR section in `cinder.conf` you can define which transport protocol
to use.  The initial setup described at the beginning of this chapter is set
to use DRBD transport.  You can configure as necessary as shown below.
Then Horizonfootnote:[The OpenStack GUI] should offer these storage backends at
volume creation time.


	* To use iSCSI with LINSTOR:
+
----
    volume_driver=cinder.volume.drivers.drbdmanagedrv.DrbdManageIscsiDriver
----

	* To use DRBD Kernel Module with LINSTOR:
+
----
    volume_driver=cinder.volume.drivers.drbdmanagedrv.DrbdManageDrbdDriver
----

The old class name "DrbdManageDriver" is being kept for the time because of
compatibility reasons; it's just an alias to the iSCSI driver.


To summarize:

	* You'll need the LINSTOR Cinder driver 0.1.0 or later, and  LINSTOR 0.6.5
	or later.

	* The <<s-openstack-linstor-drbd,DRBD transport protocol>> should be
	preferred whenever possible; iSCSI won't offer any locality benefits.

	* Take care to not run out of disk space, especially with thin volumes.

// Keep the empty line before this comment, otherwise the next chaper is folded into this
