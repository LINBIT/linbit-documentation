[[ch-openstack-linstor]]
== DRBD volumes in Openstack

indexterm:[Openstack]indexterm:[Cinder]indexterm:[Nova]indexterm:[LINSTOR]
This chapter describes DRBD in Openstack for persistent, replicated, and
high-performance block storage with
https://github.com/LINBIT/openstack-cinder/tree/stein-linstor[LINSTOR Driver].


[[s-openstack-linstor-overview]]
=== Openstack Overview

Openstack consists of a wide range of individual services; the two that are
mostly relevant to DRBD are Cinder and Nova.  *Cinder* is the block storage
service, while *Nova* is the compute node service that's responsible for making
the volumes available for the VMs.

The LINSTOR driver for OpenStack manages DRBD/LINSTOR clusters and makes
them available within the OpenStack environment, especially within Nova
compute instances.
LINSTOR-backed Cinder volumes will seamlessly provide all the features of
DRBD/LINSTOR while allowing OpenStack to manage all their deployment and
management.  The driver will allow OpenStack to create and delete persistent
LINSTOR volumes as well as managing and deploying volume snapshots and raw
volume images.

Aside from using the kernel-native DRBD protocols for replication, the LINSTOR
driver also allows using iSCSI with LINSTOR cluster(s) to provide maximum
compatibility. For more information on these two options, please see
<<s-openstack-linstor-transport-protocol>>.


[[s-openstack-linstor-install]]
=== LINSTOR for Openstack Installation

An initial installation and configuration of DRBD and LINSTOR must be complete
prior to installing OpenStack driver.
Each LINSTOR node in a cluster should also have a Storage Pool defined as well.

==== Here's a synopsis on quickly setting up a LINSTOR cluster on Ubuntu:

===== Install DRBD and LINSTOR on Cinder node also acting as a LINSTOR node:

----
# Install packages
sudo add-apt-repository ppa:linbit/linbit-drbd9-stack
sudo apt update
sudo apt install drbd-dkms
sudo apt install linstor-controller linstor-satellite linstor-client
sudo apt install drbdtop

# Start LINSTOR Services
systemctl enable linstor-controller.service
systemctl start linstor-controller.service
systemctl enable linstor-satellite.service
systemctl start linstor-satellite.service

# Create backend storage for DRBD/LINSTOR by creating a Volume Group 'drbdpool'
# Specify appropriate volume location (/dev/vdb)
sudo vgcreate drbdpool /dev/vdb

# Create a Logical Volume 'thinpool' within 'drbdpool'
# Specify appropriate thin volume size (64G)
sudo lvcreate -L 64G -T drbdpool/thinpool
----

NOTE: OpenStack measures storage size in GiBs.

===== Install DRBD and LINSTOR on other node(s) on the same LINSTOR cluster:
----
sudo add-apt-repository ppa:linbit/linbit-drbd9-stack
sudo apt update
sudo apt install drbd-dkms
sudo apt install linstor-satellite linstor-client
sudo apt install drbdtop

systemctl enable linstor-satellite.service
systemctl start linstor-satellite.service

# Create backend storage for DRBD/LINSTOR by creating a Volume Group 'drbdpool'
# Specify appropriate volume location (/dev/vdb)
sudo vgcreate drbdpool /dev/vdb

# Create a Logical Volume 'thinpool' within 'drbdpool'
# Specify appropriate thin volume size (64G)
sudo lvcreate -L 64G -T drbdpool/thinpool
----

===== Lastly, from the Cinder node, create LINSTOR Node(s) and Storage Pool(s)
----
# Create a LINSTOR cluster, including the Cinder node as one of the nodes
# For each node, specify node name, its IP address, volume type (lvmthin) and
# volume location (drbdpool/thinpool)

linstor node create cinder-node-name 192.168.1.100 lvmthin drbdpool/thinpool
linstor node create another-linstor-node 192.168.1.101 lvmthin drbdpool/thinpool
# repeat to add more nodes in the LINSTOR cluster

# Create LINSTOR Storage Pool on each nodes
# For each node, specify node name, its IP address, storage pool name (DfltStorPool),
# volume type (lvmthin) and node type (Combined)

linstor storage-pool create cinder-node-name DfltStorPool lvmthn drbdpool/thinpool
linstor storage-pool create another-linstor-node DfltStorPool lvmthn drbdpool/thinpool
# repeat to add a storage pool to each node in the LINSTOR cluster
----

==== Install the LINSTOR driver file
The _linstor driver_ will be officially available starting OpenStack Stein
release. The latest release is located at
https://github.com/LINBIT/openstack-cinder/blob/stein-linstor/cinder/volume/drivers/linstordrv.py[LINBIT OpenStack Repo].
It is a single Python file called *linstordrv.py*.  Depending on your OpenStack
installation, its destination may vary.

Place the driver ( linstordrv.py ) in an appropriate location within your
OpenStack Cinder node.

For Devstack:
----
/opt/stack/cinder/cinder/volume/drivers/linstordrv.py
----

For Ubuntu:
----
/usr/lib/python2.7/dist-packages/cinder/volume/drivers/linstordrv.py
----

For RDO Packstack:
----
/usr/lib/python2.7/site-packages/cinder/volume/drivers/linstordrv.py
----


[[s-openstack-install]]
=== Cinder Configuration for LINSTOR

==== Edit Cinder configuration file *cinder.conf* in /etc/cinder/ as follows:

===== Enable LINSTOR driver by adding '*linstor*' to enabled_backends
----
[DEFAULT]
...
enabled_backends=lvm, linstor
...
----

===== Add the following configuration options at the end of the cinder.conf
----
[linstor]
volume_backend_name = linstor
volume_driver = cinder.volume.drivers.linstordrv.LinstorDrbdDriver
linstor_redundancy = 1
linstor_disk_options = {"c-min-rate": "4M"}
linstor_net_options = {"connect-int": "4", "allow-two-primaries": "yes", "ko-count": "30", "max-buffers": "20000", "ping-timeout": "100"}
linstor_resource_options = {"auto-promote-timeout": "300"}
linstor_default_volume_group_name=drbdpool
linstor_default_uri=linstor://localhost
linstor_default_storage_pool_name=DfltStorPool
linstor_default_resource_size=1
linstor_volume_downsize_factor=4096
----

==== Apply a manual patch to a dependency.
We need to manually patch a library dependency in order to suppress an error about
missing parameter. In /usr/local/lib/python2.7/dist-packages/eventlet/green/select.py,
starting line 77, change it to read as follows:
----
    try:
        for k, v in six.iteritems(ds):
            if v.get('read'):
                #listeners.append(hub.add(hub.READ, k, on_read, on_error, lambda x: None))
                listeners.append(hub.add(hub.READ, k, on_read, current.throw, lambda: None))
            if v.get('write'):
                #listeners.append(hub.add(hub.WRITE, k, on_write, on_error, lambda x: None))
                listeners.append(hub.add(hub.WRITE, k, on_write, current.throw, lambda: None))
----

==== Create a new backend type for LINSTOR
Run these commands from the Cinder node.
----
cinder type-create linstor
cinder type-key linstor set volume_backend_name=linstor
----

==== Restart the Cinder services to finalize

For Devstack:
----
sudo systemctl restart devstack@c-vol.service
sudo systemctl restart devstack@c-api.service
sudo systemctl restart devstack@c-sch.service
----

For RDO Packstack:
----
sudo systemctl restart openstack-cinder-volume.service
sudo systemctl restart openstack-cinder-api.service
sudo systemctl restart openstack-cinder-scheduler.service
----

For full OpenStack:
----
sudo systemctl restart cinder-volume.service
sudo systemctl restart cinder-api.service
sudo systemctl restart cinder-scheduler.service
----


[[s-openstack-linstor-addtl-conf]]
==== Additional Configuration

More to come


[[s-openstack-linstor-transport-protocol]]
=== Choosing the Transport Protocol

There are two main ways to run DRBD/LINSTOR with Cinder:

  * accessing storage via <<s-openstack-linstor-iscsi,iSCSI exports>>, and

  * using <<s-openstack-linstor-drbd,the DRBD transport protocol>> on the wire with
  LINSTOR.

These are not exclusive; you can define multiple backends, have some of them
use iSCSI, and others the DRBD protocol.


[[s-openstack-linstor-iscsi]]
==== iSCSI Transport

The default way to export Cinder volumes is via iSCSI. This brings the
advantage of maximum compatibility - iSCSI can be used with every hypervisor,
be it VMWare, Xen, HyperV, or KVM.

The drawback is that all data has to be sent to a Cinder node, to be processed
by an (userspace) iSCSI daemon; that means that the data needs to pass the
kernel/userspace border, and these transitions will cost some performance.


[[s-openstack-linstor-drbd]]
==== DRBD/LINSTOR Transport

The alternative is to get the data to the VMs by using DRBD as the transport
protocol. This means that DRBD 9footnote:[LINSTOR must be installed on Cinder
node. Please see the note at <<s-openstack-linstor-drbd-external-NOTE>>.]
needs to be installed on the Cinder node as well.

[NOTE]
Since OpenStack only functions in Linux, using DRBD/LINSTOR Transport restricts
deployment only on Linux hosts with KVM at the moment.

One advantage of that solution is that the storage access requests of the VMs
can be sent via the DRBD kernel module to the storage nodes, which can then
directly access the allocated LVs; this means no Kernel/Userspace transitions
on the data path, and consequently better performance. Combined with RDMA
capable hardware you should get about the same performance as with VMs
accessing a FC backend directly.

Another advantage is that you will be implicitly benefitting from the HA
background of DRBD: using multiple storage nodes, possibly available over
different network connections, means redundancy and avoiding a single point
of failure.


[[s-openstack-linstor-drbd-external-NOTE]]
[NOTE]
--
Currently, the Cinder node must also be one of the LINSTOR resource nodes.
Diskless option is being tested.
--


[[s-openstack-linstor-conf-transport-protocol]]
==== Configuring the Transport Protocol

In the LINSTOR section in `cinder.conf` you can define which transport protocol
to use.  The initial setup described at the beginning of this chapter is set
to use DRBD transport.  You can configure as necessary as shown below.
Then Horizonfootnote:[The OpenStack GUI] should offer these storage backends at
volume creation time.


	* To use iSCSI with LINSTOR:
+
----
    volume_driver=cinder.volume.drivers.drbdmanagedrv.DrbdManageIscsiDriver
----

	* To use DRBD Kernel Module with LINSTOR:
+
----
    volume_driver=cinder.volume.drivers.drbdmanagedrv.DrbdManageDrbdDriver
----

The old class name "DrbdManageDriver" is being kept for the time because of
compatibility reasons; it's just an alias to the iSCSI driver.


To summarize:

	* You'll need the LINSTOR Cinder driver 0.0.5 or later, and  LINSTOR 0.5.0
	or later.

	* The <<s-openstack-linstor-drbd,DRBD transport protocol>> should be
	preferred whenever possible; iSCSI won't offer any locality benefits.

	* Take care to not run out of disk space, especially with thin volumes.

// Keep the empty line before this comment, otherwise the next chaper is folded into this
