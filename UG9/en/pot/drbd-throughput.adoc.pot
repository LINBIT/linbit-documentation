# SOME DESCRIPTIVE TITLE
# Copyright (C) YEAR Free Software Foundation, Inc.
# This file is distributed under the same license as the PACKAGE package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PACKAGE VERSION\n"
"Report-Msgid-Bugs-To: documentation@linbit.com\n"
"POT-Creation-Date: 2023-10-31 19:39+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: en\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#. type: Title ===
#: UG9/en/drbd-latency.adoc:10 UG9/en/drbd-throughput.adoc:9
#, no-wrap
msgid "Hardware Considerations"
msgstr ""

#. type: Title ===
#: UG9/en/drbd-latency.adoc:107 UG9/en/drbd-throughput.adoc:78
#, no-wrap
msgid "Tuning Recommendations"
msgstr ""

#. type: Title ==
#: UG9/en/drbd-throughput.adoc:2
#, no-wrap
msgid "Optimizing DRBD Throughput"
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:7
msgid ""
"This chapter deals with optimizing DRBD throughput. It examines some "
"hardware considerations with regard to throughput optimization, and details "
"tuning recommendations for that purpose."
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:14
msgid ""
"DRBD throughput is affected by both the bandwidth of the underlying I/O "
"subsystem (disks, controllers, and corresponding caches), and the bandwidth "
"of the replication network."
msgstr ""

#. type: Block title
#: UG9/en/drbd-throughput.adoc:15
#, no-wrap
msgid "I/O subsystem throughput"
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:28
msgid ""
"indexterm:[throughput]I/O subsystem throughput is determined, largely, by "
"the number and type of storage units (disks, SSDs, other Flash storage [like "
"FusionIO], ...) that can be written to in parallel. A single, reasonably "
"recent, SCSI or SAS disk will typically allow streaming writes of roughly "
"40MiB/s to the single disk; an SSD will do 300MiB/s; one of the recent Flash "
"storages (NVMe) will be at 1GiB/s. When deployed in a striping "
"configuration, the I/O subsystem will parallelize writes across disks, "
"effectively multiplying a single disk's throughput by the number of stripes "
"in the configuration. Therefore, the same 40MiB/s disks will allow effective "
"throughput of 120MiB/s in a RAID-0 or RAID-1+0 configuration with three "
"stripes, or 200MiB/s with five stripes; with SSDs, NVMe, or both, you can "
"easily get to 1GiB/sec."
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:33
msgid ""
"A RAID-controller with RAM and a <<s-hardware-bbu,BBU>> can speed up short "
"spikes (by buffering them), and so too-short benchmark tests might show "
"speeds like 1GiB/s too; for sustained writes its buffers will just run full, "
"and then not be of much help, though."
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:39
msgid ""
"Disk _mirroring_ (RAID-1) in hardware typically has little, if any, effect "
"on throughput. Disk _striping with parity_ (RAID-5) does have an effect on "
"throughput, usually an adverse one when compared to striping; RAID-5 and "
"RAID-6 in software even more so."
msgstr ""

#. type: Block title
#: UG9/en/drbd-throughput.adoc:40
#, no-wrap
msgid "Network throughput"
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:50
msgid ""
"indexterm:[throughput]Network throughput is usually determined by the amount "
"of traffic present on the network, and on the throughput of any routing/"
"switching infrastructure present. These concerns are, however, largely "
"irrelevant in DRBD replication links which are normally dedicated, back-to-"
"back network connections. Therefore, network throughput may be improved "
"either by switching to a higher-throughput hardware (such as 10 Gigabit "
"Ethernet, or 56GiB InfiniBand), or by using link aggregation over several "
"network links, as one may do using the Linux indexterm:[bonding "
"driver]`bonding` network driver."
msgstr ""

#. type: Title ===
#: UG9/en/drbd-throughput.adoc:52
#, no-wrap
msgid "Estimating DRBD's Effects on Throughput"
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:56
msgid ""
"When estimating the throughput effects associated with DRBD, it is important "
"to consider the following natural limitations:"
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:58
msgid "DRBD throughput is limited by that of the raw I/O subsystem."
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:59
msgid "DRBD throughput is limited by the available network bandwidth."
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:64
msgid ""
"The _lower_ of these two establishes the theoretical throughput _maximum_ "
"available to DRBD. DRBD then reduces that baseline throughput maximum number "
"by DRBD's additional I/O activity, which can be expected to be less than "
"three percent of the baseline number."
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:71
msgid ""
"Consider the example of two cluster nodes containing I/O subsystems capable "
"of 600 MB/s throughput, with a Gigabit Ethernet link available between them. "
"Gigabit Ethernet can be expected to produce 110 MB/s throughput for TCP "
"connections, therefore the network connection would be the bottleneck in "
"this configuration and one would expect about 110 MB/s maximum DRBD "
"throughput."
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:75
msgid ""
"By contrast, if the I/O subsystem is capable of only 80 MB/s for sustained "
"writes, then it constitutes the bottleneck, and you should expect only about "
"77 MB/s maximum DRBD throughput."
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:88
msgid ""
"DRBD offers several configuration options which may have an effect on your "
"system's throughput. This section list some recommendations for tuning for "
"throughput. However, since throughput is largely hardware dependent, the "
"effects of tweaking the options described here may vary greatly from system "
"to system. It is important to understand that these recommendations should "
"not be interpreted as \"silver bullets\" which would magically remove any "
"and all throughput bottlenecks."
msgstr ""

#. type: Title ====
#: UG9/en/drbd-throughput.adoc:90
#, no-wrap
msgid "Setting `max-buffers` and `max-epoch-size`"
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:100
msgid ""
"These options affect write performance on the secondary nodes. `max-buffers` "
"is the maximum number of buffers DRBD allocates for writing data to disk "
"while `max-epoch-size` is the maximum number of write requests permitted "
"between two write barriers. `max-buffers` must be equal or bigger to `max-"
"epoch-size` to increase performance.  The default for both is 2048; setting "
"it to around 8000 should be fine for most reasonably high-performance "
"hardware RAID controllers."
msgstr ""

#. type: delimited block -
#: UG9/en/drbd-throughput.adoc:111
#, no-wrap
msgid ""
"resource <resource> {\n"
"  net {\n"
"    max-buffers    8000;\n"
"    max-epoch-size 8000;\n"
"    ...\n"
"  }\n"
"  ...\n"
"}\n"
msgstr ""

#. type: Title ====
#: UG9/en/drbd-throughput.adoc:114
#, no-wrap
msgid "Tuning the TCP Send Buffer Size"
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:123
msgid ""
"The TCP send buffer is a memory buffer for outgoing TCP traffic. By default, "
"it is set to a size of 128 KiB. For use in high-throughput networks (such as "
"dedicated Gigabit Ethernet or load-balanced bonded connections), it may make "
"sense to increase this to a size of 2MiB, or perhaps even more. Send buffer "
"sizes of more than 16MiB are generally not recommended (and are also "
"unlikely to produce any throughput improvement)."
msgstr ""

#. type: delimited block -
#: UG9/en/drbd-throughput.adoc:133
#, no-wrap
msgid ""
"resource <resource> {\n"
"  net {\n"
"    sndbuf-size 2M;\n"
"    ...\n"
"  }\n"
"  ...\n"
"}\n"
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:139
msgid ""
"DRBD also supports TCP send buffer auto-tuning. After enabling this feature, "
"DRBD will dynamically select an appropriate TCP send buffer size. TCP send "
"buffer auto tuning is enabled by simply setting the buffer size to zero:"
msgstr ""

#. type: delimited block -
#: UG9/en/drbd-throughput.adoc:149
#, no-wrap
msgid ""
"resource <resource> {\n"
"  net {\n"
"    sndbuf-size 0;\n"
"    ...\n"
"  }\n"
"  ...\n"
"}\n"
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:155
msgid ""
"Please note that your ``sysctl``'s settings `net.ipv4.tcp_rmem` and `net."
"ipv4.tcp_wmem` will still influence the behaviour; you should check these "
"settings, and perhaps set them similar to `131072 1048576 16777216` (minimum "
"128kiB, default 1MiB, max 16MiB)."
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:159
msgid ""
"`net.ipv4.tcp_mem` is a different beast, with a different unit - do not "
"touch, wrong values can easily push your machine into out-of-memory "
"situations!"
msgstr ""

#. type: Title ====
#: UG9/en/drbd-throughput.adoc:161
#, no-wrap
msgid "Tuning the Activity Log Size"
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:167
msgid ""
"If the application using DRBD is write intensive in the sense that it "
"frequently issues small writes scattered across the device, it is usually "
"advisable to use a fairly large activity log. Otherwise, frequent metadata "
"updates may be detrimental to write performance."
msgstr ""

#. type: delimited block -
#: UG9/en/drbd-throughput.adoc:177
#, no-wrap
msgid ""
"resource <resource> {\n"
"  disk {\n"
"    al-extents 6007;\n"
"    ...\n"
"  }\n"
"  ...\n"
"}\n"
msgstr ""

#. For tuning on striped RAID setups, please see the <<al-stripes,al-stripes>> and
#. <<al-stripe-size,al-stripe-size>> settings, too.
#. type: Title ====
#: UG9/en/drbd-throughput.adoc:185
#, no-wrap
msgid "Disabling Barriers and Disk Flushes"
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:189
msgid ""
"The recommendations outlined in this section should be applied _only_ to "
"systems with non-volatile (battery backed) controller caches."
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:194
msgid ""
"Systems equipped with battery backed write cache come with built-in means of "
"protecting data in the face of power failure. In that case, it is "
"permissible to disable some of DRBD's own safeguards created for the same "
"purpose. This may be beneficial in terms of throughput:"
msgstr ""

#. type: delimited block -
#: UG9/en/drbd-throughput.adoc:205
#, no-wrap
msgid ""
"resource <resource> {\n"
"  disk {\n"
"    disk-barrier no;\n"
"    disk-flushes no;\n"
"    ...\n"
"  }\n"
"  ...\n"
"}\n"
msgstr ""

#. type: Title ===
#: UG9/en/drbd-throughput.adoc:209
#, no-wrap
msgid "Achieving Better Read Performance Through Increased Redundancy"
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:213
msgid ""
"As detailed in the man page of `drbd.conf` under `read-balancing`, you can "
"increase your read performance by adding more copies of your data."
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:217
msgid ""
"As a ballpark figure: with a single node processing read requests, `fio` on "
"a __FusionIO__ card gave us 100k IOPS; after enabling `read-balancing`, the "
"performance jumped to 180k IOPS, i.e. +80%!"
msgstr ""

#. type: Plain text
#: UG9/en/drbd-throughput.adoc:220
msgid ""
"So, in case you're running a read-mostly workload (big databases with many "
"random reads), it might be worth a try to turn `read-balancing` on - and, "
"perhaps, add another copy for still more read IO throughput."
msgstr ""
