
[[s-administrative-tasks-setup]]
== Basic administrative tasks / Setup

LINSTOR is a configuration management system for storage on Linux systems.
It manages LVM logical volumes and/or ZFS ZVOLs on a cluster of nodes. It
leverages DRBD for replication between different nodes and to provide
block storage devices to users and applications. It manages snapshots,
encryption and caching of HDD backed data in SSDs via bcache.

// Troubleshooting for LINSTOR guide?
/////
This chapter outlines typical administrative tasks encountered during
day-to-day operations. It does not cover troubleshooting tasks, these
are covered in detail in <<ch-troubleshooting>>.
/////


[[s-concepts_and_terms]]
=== Concepts and Terms

A LINSTOR setup has exactly one active controller and multiple satellites.
The _linstor-controller_ contains the database that holds all configuration
information for the whole cluster. It makes all decisions that need to have a
view of the whole cluster. The controller is typically deployed as a HA service
using Pacemaker and DRBD as it is a crucial part of the system. Multiple controllers 
can be used for LINSTOR but as already mentioned only one can be active.

The _linstor-satellite_ runs on each node where LINSTOR consumes local
storage or provides storage to services. It is stateless; it receives
all the information it needs from the controller. It runs programs
like `lvcreate` and `drbdadm`. It acts like a node agent.

The _linstor-client_ is a command line utility that you use to issue
commands to the system and to investigate the status of the system.

[[s-broader_context]]
=== Broader Context

While LINSTOR might be used to make the management of DRBD more
convenient, it is often integrated with software stacks higher up.
Such integration exist already for Kubernetes, OpenStack, OpenNebula
and Proxmox. Chapters specific to deploying LINSTOR in these
environments are included in this guide.

The southbound drivers used by LINSTOR are LVM, thinLVM and ZFS
with support for Swordfish in progress.

[[s-packages]]
=== Packages

LINSTOR is packaged in both the .rpm and the .deb variants:

. _linstor-client_ contains the command line client program. It only depends
  on python which is usually already installed.
. _linstor-controller_ contains the _controller_ and _linstor-satellite_ the _satellite_.
  These packages also provide systemd unit files for the services. It depends on a
  Java runtime environment (JRE) version 1.8 (headless) or higher. This might
  pull in about 100MB of dependencies.

NOTE: If you have a support subscription to LINBIT you will have access to the standard repositories. 

[[s-installtion]]
=== Installation

IMPORTANT: If you want to use LINSTOR in containers skip this Topic and use the "Containers" section below for the installation.   



[[s-ubuntu_linux]]
==== Ubuntu Linux

First we're installing the newest version of DRBD and its tools on all nodes. Furthermore we can install the lvm tools as well. 

--------------------------------------------
# apt install -y drbd-dkms drbd-utils lvm2
--------------------------------------------

Depending on wether your node is a LINSTOR controller, satellite, or both (Combined) will determine what packages
are required on that node. For combined type nodes, we'll need both the controller and satellite LINSTOR package.

Combined node:

--------------------------------------------------------------------
# apt install linstor-controller linstor-satellite  linstor-client     
--------------------------------------------------------------------

That will make our remaining nodes our Satellites, so we'll need to install the following packages on them:

-------------------------------------------------
# apt install linstor-satellite  linstor-client 
-------------------------------------------------


[[s-suse_linux_enterprise_server]]
==== SUSE Linux Enterprise Server


SLES High Availability Extension (HAE) includes DRBD.

On SLES, DRBD is normally installed via the software installation component of YaST2. It comes bundled with the High Availabilty 
package selection.

As we download DRBD's newest module we can check if the LVM-tools are uptodate as well. User who prefer a command line install 
may simply issue the following command to get the newest DRBD and LVM version:

----------------------
# zypper install drbd lvm2
----------------------



Depending on wether your node is a LINSTOR controller, satellite, or both (Combined) will determine what packages
are required on that node. For combined type nodes, we'll need both the controller and satellite LINSTOR package.

Combined node:

------------------------------------------------------------------------
# zypper install linstor-controller linstor-satellite  linstor-client  
------------------------------------------------------------------------

That will make our remaining nodes our Satellites, so we'll need to install the following packages on them:

----------------------------------------------------
# zypper install linstor-satellite  linstor-client  
----------------------------------------------------


[[s-centos]]
==== CentOS

CentOS has had DRBD 8 since release 5. For DRBD 9 you'll need to look at EPEL and similar sources.
DRBD can be installed using `yum`. We can also check for the newest version of the LVM-tools as well.

IMPORTANT: You will need a correct repository enabled for this to work 

-----------------------------
# yum install drbd kmod-drbd lvm2
-----------------------------


Depending on wether your node is a LINSTOR controller, satellite, or both (Combined) will determine what packages
are required on that node. For combined type nodes, we'll need both the controller and satellite LINSTOR package.

Combined node:

-------------------------------------------------------------------
# yum install linstor-controller linstor-satellite  linstor-client 
-------------------------------------------------------------------

That will make our remaining nodes our Satellites, so we'll need to install the following packages on them:

------------------------------------------------
# yum install linstor-satellite  linstor-client 
------------------------------------------------


[[s-containers]]
=== Containers

LINSTOR is also available as containers. The base images are available
in LINBIT's container registry, `drbd.io`.

In order to access the images, you first have to login to the
registry (reach out to sales@linbit.com for credentials):

----------------------------
# docker login drbd.io
----------------------------

The containers available in this repo are:

* drbd.io/drbd9:rhel7
* drbd.io/drbd9:bionic
* drbd.io/linstor-csi
* drbd.io/linstor-controller
* drbd.io/linstor-satellite
* drbd.io/linstor-client

An up to date list of available images with versions can be retrieved by opening http://drbd.io in your
browser. Make sure to access the host via "http", as the registry's images themselves are served via "https".

To load the kernel module, needed only for LINSTOR satellites, you'll
need to run a `drbd9` container in privileged mode:

----------------------------
# docker run -it --rm --privileged -v /lib/modules:/lib/modules drbd.io/drbd9:rhel7
----------------------------

IMPORTANT: For now (i.e., pre DRBD 9 version "9.0.17"), you must use the containerized DRBD kernel module,
as opposed to loading a kernel module onto the host system. If you
intend to use the containers you should not install the DRBD kernel
module on your host systems. After version "9.0.17" is released, you can install the kernel module as usual on
the host system, but you need to make sure to load the module with the `usermode_helper=disabled` parameter
(e.g., `modprobe drbd usermode_helper=disabled`).

Then run the LINSTOR satellite container, also privileged, as a daemon:

----------------------------
# docker run -d --name=linstor-satellite --net=host --privileged drbd.io/linstor-satellite
----------------------------

NOTE: `net=host` is required for the containerized `drbd-utils` to be
able to communicate with the host-kernel via netlink.

To run the LINSTOR controller container as a daemon, mapping ports
`3376` and `3377` on the host to the container:

----------------------------
# docker run -d --name=linstor-controller -p 3376:3376 -p 3377:3377 drbd.io/linstor-controller
----------------------------

To interact with the containerized LINSTOR cluster, you can either use
a LINSTOR client installed on a system via packages, or via the
containerized LINSTOR client. To use the LINSTOR client container:

----------------------------
# docker run -it --rm -e LS_CONTROLLERS=<controller-host-IP-address> drbd.io/linstor-client node list
----------------------------

From this point you would use the LINSTOR client to initialize your
cluster and begin creating resources using the typical LINSTOR
patterns.

To stop and remove a daemonized container and image:

----------------------------
# docker stop linstor-controller
# docker rm linstor-controller
----------------------------

[[s-linstor-init-cluster]]
=== Initializing your cluster
We assume that the following steps are accomplished on *all* cluster nodes:

. The DRBD9 kernel module is installed and loaded
. `drbd-utils` are installed
. `LVM` tools are installed
. `linstor-controller` and/or `linstor-satellite` its dependencies are installed

Start and enable the linstor-controller service on the host where it has been installed:

----------------------------
# systemctl enable --now linstor-controller
----------------------------

If you are sure the linstor-controller service gets autimatically enabled on installation you can use the 
following command as well:

------------------------------------------
# systemctl start linstor-controller
------------------------------------------

// FIXME: Move this whole chapter to an appendix
/////
[[s-linstor-migrate-from-dm]]
=== Migrating resources from DRBDManage
The LINSTOR client contains a sub-command that can generate a migration script that adds existing DRBDManage
nodes and resources to a LINSTOR cluster. Migration can be done without downtime. If you do not plan to
migrate existing resources, continue with the next section.

The first thing to check is if the DRBDManage cluster is in a healthy state. If the output of `drbdmanage
assignments` looks good, you can export the existing cluster database via `drbdmanage export-ctrlvol >
ctrlvol.json`. You can then use that as input for the LINSTOR client. The client does *not* immediately
migrate your resources, it just generates a shell script. Therefore, you can run the migration assistant
multiple times and review/modify the generated shell script before actually executing it. Migration script
generation is started via `linstor dm-migrate ctrlvol.json dmmmigrate.sh`. The script will ask a few questions
and then generate the shell script. After carefully reading the script, you can then shutdown DRBDManage, and
rename the following files.  If you do not rename them, the lower-level drbd-utils pick up both kinds of resource
files, the ones from DRBDManage and the ones from LINSTOR.

Obviously, you need the linstor-controller service started on one node and the linstor-satellite service on all
nodes.

----------------------------
# drbdmanage shutdown -qc # on all nodes
# mv /etc/drbd.d/drbdctrl.res{,.dis} # on all nodes
# mv /etc/drbd.d/drbdmanage-resources.res{,.dis} # on all nodes
# bash dmmigrate.sh
----------------------------
/////


[[s-using_the_linstor_client]]
=== Using the LINSTOR client
Whenever you run the LINSTOR command line client, it needs to know where your
linstor-controller runs. If you do not specify it, it will try to reach a locally
running linstor-controller listening on IP `127.0.0.1` port `3376`. Therefore we 
will use the linstor-client on the same host as the linstor-controller.  

----------------------------
# linstor node list
----------------------------
should give you an empty list and not an error message.

You can use the `linstor` command on any other machine, but then you need
to tell the client how to find the linstor-controller. As shown, this can be
specified as a command line option, an environment variable or in a global
file:

----------------------------
# linstor --controllers=alice node list
# LS_CONTROLLERS=alice linstor node list
----------------------------

Alternatively you can create the `/etc/linstor/linstor-client.conf`
file and populate it like below.


-----
[global]
controllers=alice
-----

If you have multiple linstor-controllers configured you can simply
specify them all in a comma separated list. The linstor-client will
simply try them in the order listed.


NOTE: The linstor-client commands can also be used in a much faster
and convinient way by only writing the starting letters of the parameters
e.g.: `linstor node list` -> `linstor n l`

[[s-adding_nodes_to_your_cluster]]
=== Adding nodes to your cluster
The next step is to add nodes to your LINSTOR cluster. You need to
provide:

. A node name which *must* match the output of `uname -n`
. The IP address of the node.

----------------------------
# linstor node create bravo 10.43.70.3
----------------------------

When you use `linstor node list` you will see that the new node
is marked as offline. Now start and enable the linstor-satellite on that node
so that the service comes up on reboot as well: 
----------------------------
# systemctl enable --now  linstor-satellite
----------------------------

You can also use `systemctl start linstor-satellite`
if you are sure that the service is already enabled as default and comes up on
reboot.

About 10 seconds later you will see the status in `linstor node list`
becoming online. Of course the satellite process may be started before
the controller knows about the existence of the satellite node.

NOTE: In case the node which hosts your controller should also contribute
storage to the LINSTOR cluster, you have to add it as a node and start
the linstor-satellite as well.

[[s-storage_pools]]
=== Storage pools

_Storage pools_ identify storage in the context of LINSTOR.
To group storage pools from multiple nodes, simply use the same name
on each node.
For example, one valid approach is to give all SSDs one name and
all HDDs another.

On each host contributing storage, you need to create
either an LVM VG or a ZFS zPool. The VGs and zPools identified with one
LINSTOR storage pool name may have different VG or zPool names on the
hosts, but do yourself a favor and use the same VG or zPool name on all
nodes.

----------------------------
# vgcreate vg_ssd /dev/nvme0n1 /dev/nvme1n1 [...]
----------------------------

These then need to be registered with LINSTOR:

----------------------------
# linstor storage-pool create lvm alpha pool_ssd vg_ssd
# linstor storage-pool create lvm bravo pool_ssd vg_ssd
----------------------------

NOTE: The storage pool name and common metadata is referred to as a
_storage pool definition_.
The listed commands create a storage pool definition implicitly.
You can see that by using `linstor storage-pool-definition list`.
Creating storage pool definitions explicitly is possible but
not necessary.

To list your storage-pools you can use:

------
# linstor storage-pool list 
------

or

-----
# linstor sp l
-----

if you want the short version.

[[s-a_storage_pool_per_backend_device]]
==== A storage pool per backend device

In clusters where you have only one kind of storage and the capability
to hot-repair storage devices, you may choose a model where you create
one storage pool per physical backing device. The advantage of this
model is to confine failure domains to a single storage device.


[[s-linstor-set-config]]
=== Cluster configuration

[[s-available_storage_plugins]]
==== Available storage plugins

indexterm:[linstor, storage plugins]

LINSTOR has the following supported storage plugins as of writing:

  * Thick LVM

  * Thin LVM with a single thin pool

  * Thick ZFS

  * Thin ZFS

[[s-linstor-new-volume]]
=== Creating and deploying resources/volumes
In the following scenario we assume that the goal is to create a resource
'backups' with a size of '500 GB' that is replicated among three cluster nodes.

First, we create a new resource definition:

----------------------------
# linstor resource-definition create backups
----------------------------

Second, we create a new volume definition within that resource definition:

----------------------------
# linstor volume-definition create backups 500G
----------------------------

If you want to change the size of the volume-definition you can simply do that by:

-------
# linstor volume-definition set-size backups 0 100G
-------

The parameter `0` is the number of the volume in the resource `backups`. You have to provide this parameter
, because resources can have multiple volumes and they are identified by a so called volume-number. This number
can be found by listing the volume-defintions.

IMPORTANT: The size of a volume-definition can only be decreased if it has no resource. Despite
of that the size can be increased even with an deployed resource.

So far we have only created objects in LINSTOR's database, not a single LV was
created on the storage nodes. Now you have the choice of delegating the
task of placement to LINSTOR or doing it yourself.

[[s-manual_placement]]
==== Manual placement

With the `resource create` command you may assign a resource definition
to named nodes explicitly.

----------------------------
# linstor resource create alpha backups --storage-pool pool_hdd
# linstor resource create bravo backups --storage-pool pool_hdd
# linstor resource create charlie backups --storage-pool pool_hdd
----------------------------

[[s-autoplace-linstor]]
==== Autoplace

The value after autoplace tells LINSTOR how many replicas you want to have.
The storage-pool option should be obvious.
----------------------------
# linstor resource create backups --auto-place 3 --storage-pool pool_hdd
----------------------------
Maybe not so obvious is that you may omit the `--storage-pool` option, then
LINSTOR may select a storage pool on its own. The selection follows these rules:

  * Ignore all nodes and storage pools the current user has no access to
  * Ignore all diskless storage pools
  * Ignore all storage pools not having enough free space

From the remaining storage pools, LINSTOR currently chooses the one with the
most available free space.

NOTE: If everything went right the DRBD-resource has now been created by LINSTOR.
This can be checked by looking for the DRBD-blockdevice with the `lsblk`
command which should look like `drbd0000` or similiar.  


Now we should be able to mount the Blockdevice of our resource and start using
LINSTOR.


[[s-more-about-linstor]]
== Further LINSTOR tasks

[[s-drbd_clients]]
=== DRBD clients
By using the `--diskless` option instead of `--storage-pool` you can
have a permanently diskless DRBD device on a node. This means that
the resource will appear as Blockdevice and can be mounted to the
filesystem without an exisitng storage-device. The data of the
resource is accessed over the network on another nodes with the
same resource. 

----------------------------
# linstor resource create delta backups --diskless
----------------------------


[[s-linstor-drbd-conistency-group-multiple-volumes]]
=== LINSTOR - DRBD consistency group/multiple volumes

The so called consistency group is a feature from DRBD. It is mentioned in this user-guide, due to the
fact that one of LINSTOR's main functions is to manage storage-clusters with DRBD. Multiple volumes in 
one resource are a consistency group.

This means that changes on different volumes from one resource are getting replicated in
the same chronological order on the other Satellites.                       

Therefore you dont have to worry about the timing if you have interdependet data on different volumes in a 
resource. 
   
To deploy more than one volume in a LINSTOR-resource you just have to create two volume-definitions with the same name.

----
# linstor volume-definition create backups 500G
# linstor volume-definition create backups 100G
----


[[s-volumes_of_one_resource_to_different_storage_pools]]
=== Volumes of one resource to different Storage-Pools
This can be achieved by setting the `StorPoolName` property to the volume
definitions before the resource is deployed to the nodes:


----------------------------
# linstor resource-definition create backups
# linstor volume-definition create backups 500G
# linstor volume-definition create backups 100G
# linstor volume-definition set-property backups 0 StorPoolName pool_hdd
# linstor volume-definition set-property backups 1 StorPoolName pool_ssd
# linstor resource create alpha backups
# linstor resource create bravo backups
# linstor resource create charlie backups
----------------------------

NOTE: Since the `volume-definition create` command is used without the `--vlmnr` option
LINSTOR assigned the volume numbers starting at 0. In the following two
lines the 0 and 1 refer to these automatically assigned volume numbers.

Here the 'resource create' commands do not need a `--storage-pool` option.
In this case LINSTOR uses a 'fallback' storage pool. Finding that
storage pool, LINSTOR queries the properties of the following objects
in the following order:

  * Volume definition
  * Resource
  * Resource definition
  * Node

If none of those objects contain a `StorPoolName` property, the controller
falls back to a hardcoded 'DfltStorPool' string as a storage pool.

This also means that if you forgot to define a storage pool prior deploying a
resource, you will get an error message that LINSTOR could not find the
storage pool named 'DfltStorPool'.


[[s-nvme-linstor-without-drbd]]
=== NVMe-oF/NVMe-TCP and LINSTOR without DRBD

LINSTOR can be used without DRBD as well. Without DRBD it has the only abilty to create local
resources with LVM or ZFS. Because of that LINSTOR itself is not able to let resources 
on different nodes communicate. To change that NVMe-oF/NVMe-TCP is being used.

NVMe-oF/NVMe-TCP allows LINSTOR to connect diskless resources to a node with the same resource 
where the data is stored. This leads to the advantage that resources can be mounted 
without using local storage by accessing the data over the network. Remind in this
case LINSTOR is not using DRBD. Therefore this resource has no replication feature and the data is stored on one node. 

NOTE: NVMe-oF only works on RDMA-capable networks and NVMe-TCP on every network that can carry IP traffic. If you
      want to know more about NVMe-oF/NVMe-TCP visit https://www.linbit.com/en/nvme-linstor-swordfish/ for more information. 

To use NVMe-oF/NVMe-TCP with LINSTOR the package `nvme-cli` needs to be installed on every Node which acts as a Satellite
and will use NVMe-oF/NVMe-TCP for a resource:

IMPORTANT: If you are not using Ubuntu use the suitable command for installing packages on your OS - SLES: zypper - 
CentOS: yum

------
# apt install nvme-cli
------

To make a resource which uses NVMe-oF/NVMe-TCP an additional parameter has to be given as you create the resource-definiton: 

------
# linstor resource-definition create nvmedata  -l nvme,storage
------

NOTE: As default the -l (layer-stack) parameter is set to `drbd, storage` when DRBD is used. If you want to create LINSTOR resources
with neither NVMe nor DBRD you have to set the `-l` parameter to only `storage`. 

Create the volume-definition for our resource:

------
# linstor volume-definiton create nvmedata 500G
------

Before you create the resource on your nodes you have to know where the data will be stored
locally and which node accesses it over the network.

First we create the resource on the node where our data is stored:

------
# linstor resource create alpha nvmedata --storage-pool pool_ssd
------ 

On the nodes where the resource-data is accessed over the network, the resource has to be defined as diskless:

-----
# linstor resource create beta nvmedata -d
-----    

The `-d` parameter creates the resource on this node as diskless.


Now you can  mount the resource `nvmedata` on one of your nodes.

IMPORTANT: If your nodes have more than one NIC you should force the route between them for NVMe-of/NVME-TCP, otherwise 
multiple NIC's could cause troubles. 


[[s-managing_network_interface_cards]]
=== Managing Network Interface Cards

LINSTOR can deal with multiple network interface cards (NICs) in a machine,
they are called `netif` in LINSTOR speak.

NOTE: When a satellite node is created a first `netif` gets created implicitly
with the name `default`. Using the `--interface-name` option of the `node create`
command you can give it a different name.

Additional NICs are created like this:
----------------------------
# linstor node interface create alpha 100G_nic 192.168.43.221
# linstor node interface create alpha 10G_nic 192.168.43.231
----------------------------

NICs are identified by the IP address only, the name is arbitrary and is
*not* related to the interface name used by Linux. The NICs can be assigned
to storage pools so that whenever a resource is created in such a storage
pool, the DRBD traffic will be routed through the specified NIC.

----------------------------
# linstor storage-pool set-property alpha pool_hdd PrefNic 10G_nic
# linstor storage-pool set-property alpha pool_ssd PrefNic 100G_nic
----------------------------

FIXME describe how to route the controller +<->+ client communication through
a specific `netif`.

[[s-linstor-encrypted-volumes]]
=== Encrypted volumes
LINSTOR can handle transparent encryption of drbd volumes. dm-crypt is used to
encrypt the provided storage from the storage device.

Basic steps to use encryption:

1. Disable user security on the controller (this will be obsolete once authentication works)
2. Create a master passphrase
3. Create a volume definition with the `--encrypt` option
4. Don't forget to re-enter the master passphrase after a controller restart.

[[s-disable_user_security]]
==== Disable user security
Disabling the user security on the `Linstor` controller is a one time operation and is
afterwards persisted.

1. Stop the running linstor-controller via systemd: `systemctl stop linstor-controller`
2. Start a linstor-controller in debug mode: `/usr/share/linstor-server/bin/Controller -c /etc/linstor -d`
3. In the debug console enter: `setSecLvl secLvl(NO_SECURITY)`
4. Stop linstor-controller with the debug shutdown command: `shutdown`
5. Start the controller again with systemd: `systemctl start linstor-controller`


[[s-encrypt_commands]]
==== Encrypt commands
Below are details about the commands.

Before LINSTOR can encrypt any volume a master passphrase needs to be created.
This can be done with the linstor-client.

----
# linstor encryption create-passphrase
----

`crypt-create-passphrase` will wait for the user to input the initial master passphrase
(as all other crypt commands will with no arguments).

If you ever want to change the master passphrase this can be done with:

----
# linstor encryption modify-passphrase
----

To mark which volumes should be encrypted you have to add a flag while creating
a volume definition, the flag is is `--encrypt` e.g.:

----
# linstor volume-definition create crypt_rsc 1G --encrypt
----

To enter the master passphrase (after controller restart) use the following command:

----
# linstor encryption enter-passphrase
----

NOTE: Whenever the linstor-controller is restarted, the user has to send
the master passphrase to the controller, otherwise LINSTOR is unable to reopen or
create encrypted volumes.

[[s-linstor-status]]
=== Checking the state of your cluster
LINSTOR provides various commands to check the state of your cluster.
These commands start with a 'list-' prefix and provide various filtering and
sorting options. The '--groupby' option can be used to group and sort the
output in multiple dimensions.

----------------------------
# linstor node list
# linstor storage-pool list --groupby Size
----------------------------

[[s-linstor-snapshots]]
=== Managing snapshots
Snapshots are supported with thin LVM and ZFS storage pools.

[[s-creating_a_snapshot-linstor]]
==== Creating a snapshot
Assuming a resource definition named 'resource1' which has been placed on some
nodes, a snapshot can be created as follows:

----------------------------
# linstor snapshot create resource1 snap1
----------------------------

This will create snapshots on all nodes where the resource is present.
LINSTOR will ensure that consistent snapshots are taken even when the
resource is in active use.

[[s-restoring_a_snapshot-linstor]]
==== Restoring a snapshot
The following steps restore a snapshot to a new resource.
This is possible even when the original resource has been removed
from the nodes where the snapshots were taken.

First define the new resource with volumes matching those from the snapshot:

----------------------------
# linstor resource-definition create resource2
# linstor snapshot volume-definition restore --from-resource resource1 --from-snapshot snap1 --to-resource resource2
----------------------------

At this point, additional configuration can be applied if necessary.
Then, when ready, create resources based on the snapshots:

----------------------------
# linstor snapshot resource restore --from-resource resource1 --from-snapshot snap1 --to-resource resource2
----------------------------

This will place the new resource on all nodes where the snapshot is present.
The nodes on which to place the resource can also be selected explicitly;
see the help (`linstor snapshot resource restore -h`).

[[s-rolling_back_snapshot-linstor]]
==== Rolling back to a snapshot
LINSTOR can roll a resource back to a snapshot state.
The resource must not be in use.
That is, it may not be mounted on any nodes.
If the resource is in use, consider whether you can achieve your goal by
<<s-restoring_a_snapshot-linstor,restoring the snapshot>> instead.

Rollback is performed as follows:

----------------------------
# linstor snapshot rollback resource1 snap1
----------------------------

A resource can only be rolled back to the most recent snapshot.
To roll back to an older snapshot, first delete the intermediate snapshots.

[[s-removing_a_snapshot-linstor]]
==== Removing a snapshot
An existing snapshot can be removed as follows:

----------------------------
# linstor snapshot delete resource1 snap1
----------------------------

[[s-linstor-setupopts]]
=== Setting options for resources

DRBD options are set using LINSTOR commands.
Configuration in files such as `/etc/drbd.d/global_common.conf` that are not
managed by LINSTOR will be ignored.
The following commands show the usage and available options:

----------------------------
# linstor controller drbd-options -h
# linstor resource-definition drbd-options -h
# linstor volume-definition drbd-options -h
# linstor resource drbd-peer-options -h
----------------------------

For instance, it is easy to set the DRBD protocol for a resource named
`backups`:

----------------------------
# linstor resource-definition drbd-options --protocol C backups
----------------------------

[[s-linstor-toggle-disk]]
=== Adding and removing disks
LINSTOR can convert resources between diskless and having a disk.
This is achieved with the `resource toggle-disk` command,
which has syntax similar to `resource create`.

For instance, add a disk to the diskless resource `backups` on 'alpha':

----------------------------
# linstor resource toggle-disk alpha backups --storage-pool pool_ssd
----------------------------

Remove this disk again:

----------------------------
# linstor resource toggle-disk alpha backups --diskless
----------------------------

[[s-linstor-migrate-disk]]
==== Migrating disks
In order to move a resource between nodes without reducing redundancy at any point,
LINSTOR's disk migrate feature can be used.
First create a diskless resource on the target node,
and then add a disk using the `--migrate-from` option.
This will wait until the data has been synced to the new disk and then remove
the source disk.

For example, to migrate a resource `backups` from 'alpha' to 'bravo':

----------------------------
# linstor resource create bravo backups --diskless
# linstor resource toggle-disk bravo backups --storage-pool pool_ssd --migrate-from alpha
----------------------------

[[s-linstor-proxy]]
=== DRBD Proxy with LINSTOR

LINSTOR can be used to configure DRBD Proxy for long-distance replication.
DRBD Proxy must first be installed and licensed as described in
<<s-using-drbd-proxy>>.

LINSTOR expects DRBD Proxy to be running on the nodes which are involved in the
relevant connections. It does not currently support connections via DRBD Proxy
on a separate node.

Suppose our cluster consists of nodes 'alpha' and 'bravo' in a local network
and 'charlie' at a remote site, with a resource definition named `backups`
deployed to each of the nodes. Then DRBD Proxy can be enabled for the
connections to 'charlie' as follows:

----------------------------
# linstor drbd-proxy enable alpha charlie backups
# linstor drbd-proxy enable bravo charlie backups
----------------------------

The DRBD Proxy configuration can be tailored with commands such as:

----------------------------
# linstor drbd-proxy options backups --memlimit 100000000
# linstor drbd-proxy compression zlib backups --level 9
----------------------------

LINSTOR does not automatically optimize the DRBD configuration for
long-distance replication, so you will probably want to set some configuration
options such as the protocol:

----------------------------
# linstor resource-connection drbd-options alpha charlie backups --protocol A
# linstor resource-connection drbd-options bravo charlie backups --protocol A
----------------------------

Please contact LINBIT for assistance optimizing your configuration.

[[s-linstor-external-database]]
=== External database

It is possible to have LINSTOR working with an external database provider
like Postgresql or MariaDB.
To use an external database there are a few additional steps to configure.

1. The JDBC database driver for your database needs to be downloaded
   and installed to the LINSTOR library directory.
2. The `/etc/linstor/database.cfg` configuration file needs to be editied for your database setup.

[[s-postgresql]]
==== Postgresql

Postgresql JDBC driver can be downloaded here:

https://jdbc.postgresql.org/download.html

And afterwards copied to:
`/usr/share/linstor-server/lib/`

A sample Postgresql `database.cfg` looks like this:

------------------------------------------------------
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
  <comment>LinStor MariaDB configuration</comment>
  <entry key="user">linstor</entry>
  <entry key="password">linstor</entry>
  <entry key="connection-url">jdbc:postgresql://localhost/linstor</entry>
</properties>
------------------------------------------------------

[[s-mariadb_mysql]]
==== MariaDB/Mysql

MariaDB JDBC driver can be downloaded here:

https://downloads.mariadb.org/connector-java/

And afterwards copied to:
`/usr/share/linstor-server/lib/`

A sample MariaDB `database.cfg` looks like this:

------------------------------------------------------
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
  <comment>LinStor MariaDB configuration</comment>
  <entry key="user">linstor</entry>
  <entry key="password">linstor</entry>
  <entry key="connection-url">jdbc:mariadb://localhost/LINSTOR?createDatabaseIfNotExist=true</entry>
</properties>
------------------------------------------------------

NOTE: The LINSTOR schema/database is created as `LINSTOR` so make sure the mariadb connection string
refers to the `LINSTOR` schema, as in the example above.


[[s-linstor-rest-api]]
=== LINSTOR REST-API

To make LINSTOR's administrative tasks more accessable and also available for web-frontends a 
REST-API has been created. The REST-API is embedded in the linstor-controller and can be configured
by using:

--------- 
$ linstor controller set-property REST/bindAddress 127.0.0.1
---------

to set the address on which you want to reach the API and:

-------
$ linstor controller set-property REST/port 8080
-------

to set the port it should listen on.

If you want to use the REST-API the current documentation can be found on the following link:
https://app.swaggerhub.com/apis-docs/Linstor/Linstor/



[[s-linstor-getting-help]]
=== Getting help
WRITE MAN PAGE

A quick way to list available commands on the command line is to type
`linstor`.

Further information on subcommands (e.g., list-nodes) can be retrieved in
two ways:

----------------------------
# linstor node list -h
# linstor help node list
----------------------------

Using the 'help' subcommand is especially helpful when LINSTOR is executed
in interactive mode (`linstor interactive`).

One of the most helpful features of LINSTOR is its rich tab-completion,
which can be used to complete basically every object LINSTOR knows about
(e.g., node names, IP addresses, resource names, ...).
In the following examples, we show some possible completions, and their results:

----------------------------
# linstor node create alpha 1<tab> # completes the IP address if hostname can be resolved
# linstor resource create b<tab> c<tab> # linstor assign-resource backups charlie
----------------------------

If tab-completion does not work out of the box, please try to source the
appropriate file:

----------------------------
# source /etc/bash_completion.d/linstor # or
# source /usr/share/bash_completion/completions/linstor
----------------------------

For zsh shell users linstor-client can generate a zsh compilation file,
that has basic support for command and argument completion.

----------------------------
# linstor gen-zsh-completer > /usr/share/zsh/functions/Completion/Linux/_linstor
----------------------------
