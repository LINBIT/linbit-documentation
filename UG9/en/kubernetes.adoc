[[ch-kubernetes]]
== LINSTOR Volumes in Kubernetes

indexterm:[Kubernetes]This chapter describes the usage of LINSTOR in Kubernetes
as managed by the operator and with volumes provisioned using the
https://github.com/LINBIT/linstor-csi[LINSTOR CSI plugin].

[[s-kubernetes-overview]]
=== Kubernetes Overview

_Kubernetes_ is a container orchestrator. Kubernetes defines the behavior of
containers and related services via declarative specifications. In this guide,
we'll focus on using `kubectl` to manipulate `.yaml` files that define the
specifications of Kubernetes objects.

[[s-kubernetes-deploy]]
=== Deploying LINSTOR on Kubernetes

[[s-kubernetes-deploy-linstor-operator]]
==== Deploying with the LINSTOR Operator

LINBIT provides a LINSTOR operator to commercial support customers.
The operator eases deployment of LINSTOR on Kubernetes by installing DRBD,
managing Satellite and Controller pods, and other related functions.

The operator itself is installed using a Helm v3 chart as follows:

* Label the worker nodes with `linstor.linbit.com/linstor-node=true`
by running:
+
----
kubectl label node <NODE_NAME> linstor.linbit.com/linstor-node=true
----

* Create a kubernetes secret containing your my.linbit.com credentials:
+
----
kubectl create secret docker-registry drbdiocred --docker-server=drbd.io --docker-username=<YOUR_LOGIN> --docker-email=<YOUR_EMAIL> --docker-password=<YOUR_PASSWORD>
----
+
The name of this secret must match the one specified in the Helm values,
by default `drbdiocred`.

* Configure storage for the LINSTOR etcd instance. There are various options
for configuring the etcd instance for LINSTOR:
** Use an existing storage provisioner with a default `StorageClass`.
** <<s-kubernetes-etcd-hostpath-persistence,Use `hostPath` volumes>>.
** Disable persistence for basic testing. This can be done by adding `--set
etcd.persistentVolume.enabled=false` to the `helm install` command below.

* Read <<s-kubernetes-storage, the storage guide>> and configure a basic storage setup for LINSTOR:
** Create storage pools from available devices. Recommended for simple set ups. <<s-kubernetes-physical-device,Guide>>
** Create storage pools from existing LVM setup. <<s-kubernetes-storage-pool-configuration,Guide>>

* Read the <<s-kubernetes-securing-deployment,section on securing the deployment>> and configure as needed.

* Select the appropriate kernel module injector using `--set` with the `helm
install` command in the final step.

** Choose the injector according to the distribution you are using.
Select the latest version from one of `drbd9-rhel7`, `drbd9-rhel8`,...  from http://drbd.io/ as appropriate.
The drbd9-rhel8 image should also be used for RHCOS (OpenShift). For the SUSE CaaS Platform use the SLES injector
that matches the base system of the CaaS Platform you are using (e.g., `drbd9-sles15sp1`). For example:
+
----
operator.nodeSet.kernelModImage=drbd.io/drbd9-rhel8:v9.0.24
----

** Only inject modules that are already present on the host machine. If a module is not found, it will be skipped.
+
----
operator.nodeSet.drbdKernelModuleInjectionMode=DepsOnly
----

** Disable kernel module injection if you are installing DRBD by other means. Deprecated by `DepsOnly`
+
----
operator.nodeSet.drbdKernelModuleInjectionMode=None
----

* Finally create a Helm deployment named `linstor-op` that will set up
everything.
+
----
helm repo add linstor https://charts.linstor.io
helm install linstor-op linstor/linstor
----

[[s-kubernetes-etcd-hostpath-persistence]]
===== LINSTOR etcd `hostPath` persistence

You can use the `pv-hostpath` Helm templates to create `hostPath` persistent
volumes. Create as many PVs as needed to satisfy your configured etcd
`replicaCount` (default 3).

Create the `hostPath` persistent volumes, substituting cluster node
names accordingly in the `nodes=` option:

----
helm repo add linstor https://charts.linstor.io
helm install linstor-etcd linstor/pv-hostpath --set "nodes={<NODE0>,<NODE1>,<NODE2>}"
----

Persistence for etcd is enabled by default.

[[s-kubernetes-existing-database]]
===== Using an existing database

LINSTOR can connect to an existing PostgreSQL, MariaDB or etcd database. For
instance, for a PostgreSQL instance with the following configuration:

----
POSTGRES_DB: postgresdb
POSTGRES_USER: postgresadmin
POSTGRES_PASSWORD: admin123
----

The Helm chart can be configured to use this database instead of deploying an
etcd cluster by adding the following to the Helm install command:

----
--set etcd.enabled=false --set "operator.controllerSet.dbConnectionURL=jdbc:postgresql://postgres/postgresdb?user=postgresadmin&password=admin123"
----

[[s-kubernetes-storage]]
==== Configuring storage

The LINSTOR operator can automate some basic storage set up.

[[s-kubernetes-physical-device]]
===== Preparing physical devices

The LINSTOR operator can automatically configure devices on satellites. Eligible for automatic configuration are block devices that:

* do not contain partition information
* have more than 1 GiB

Any such device will be prepared according to the value of `operator.nodeSet.automaticStorageType`. Devices are added to a storage pool based on the device name (i.e. all `/dev/nvme1` devices will be part of the pool `nvme1`)

The possible values for `operator.nodeSet.automaticStorageType` are:

* `None` no automatic set up (default)
* `LVM` create a LVM (thick) storage pool
* `LVMTHIN` create a LVM thin storage pool
* `ZFS` create a ZFS based storage pool

[[s-kubernetes-storage-pool-configuration]]
===== Configuring storage pools

The operator installed by helm can be used to create storage pools. Creation is under control of the
LinstorNodeSet resource:

[source,yaml]
----
$ kubectl get LinstorNodeSet <nodeset> -o yaml
kind: LinstorNodeSet
metadata:
..
spec:
  ..
  storagePools:
    lvmPools:
    - name: lvm-thick
      volumeGroup: drbdpool
    lvmThinPools:
    - name: lvm-thin
      thinVolume: thinpool
      volumeGroup: drbdpool
----

There are two ways to configure storage pools

[[s-kubernetes-storage-pool-configuration-at-install-time]]
====== At install time

At install time, by setting the value of `operator.nodeSet.storagePools` when running helm install.

First create a file with the storage configuration like:

[source,yaml]
----
operator:
  nodeSet:
    storagePools:
      lvmPools:
      - name: lvm-thick
        volumeGroup: drbdpool
      ..
----

This file can be passed to the helm installation like this:

----
helm install linstor-op linstor/linstor -f <file> ..
----

[[s-kubernetes-storage-pool-configuration-after-install]]
====== After install

On a cluster with the operator already configured (i.e. after `helm install`),
you can edit the nodeset configuration like this:

----
$ kubectl edit LinstorNodeSet <nodeset>
----

The storage pool configuration can be updated like in the example above.

[[s-kubernetes-securing-deployment]]
==== Securing deployment
This section describes the different options for enabling security features available when
using this operator. The following guides assume the operator is installed <<s-kubernetes-deploy-linstor-operator,using Helm>>

===== Secure communication with an existing etcd instance

Secure communication to an `etcd` instance can be enabled by providing a CA certificate to the operator in form of a
kubernetes secret. The secret has to contain the key `ca.pem` with the PEM encoded CA certificate as value.

The secret can then be passed to the controller by passing the following argument to `helm install`

----
--set operator.controllerSet.dbCertSecret=<secret name>
----

====== Authentication with `etcd` using certificates

If you want to use TLS certificates to authenticate with an `etcd` database, you need to set the following option on
helm install:

----
--set operator.controllerSet.dbUseClientCert=true
----

If this option is active, the secret specified in the above section must contain two additional keys:
* `client.cert` PEM formatted certificate presented to `etcd` for authentication
* `client.key` private key **in PKCS8 format**, matching the above client certificate.
Keys can be converted into PKCS8 format using `openssl`:

----
openssl pkcs8 -topk8 -nocrypt -in client-key.pem -out client-key.pkcs8
----

===== Configuring secure communication between LINSTOR components

The default communication between LINSTOR components is not secured by TLS. If this is needed for your setup,
follow these steps:

* Create private keys in the java keystore format, one for the controller, one for all nodes:
----
keytool -keyalg rsa -keysize 2048 -genkey -keystore node-keys.jks -storepass linstor -alias node -dname "CN=XX, OU=node, O=Example, L=XX, ST=XX, C=X"
keytool -keyalg rsa -keysize 2048 -genkey -keystore control-keys.jks -storepass linstor -alias control -dname "CN=XX, OU=control, O=Example, L=XX, ST=XX, C=XX"
----
* Create a trust store with the public keys that each component needs to trust:
* Controller needs to trust the nodes
* Nodes need to trust the controller
+
----
keytool -importkeystore -srcstorepass linstor -deststorepass linstor -srckeystore control-keys.jks -destkeystore node-trust.jks
keytool -importkeystore -srcstorepass linstor -deststorepass linstor -srckeystore node-keys.jks -destkeystore control-trust.jks
----

* Create kubernetes secrets that can be passed to the controller and node pods
+
----
kubectl create secret generic control-secret --from-file=keystore.jks=control-keys.jks --from-file=certificates.jks=control-trust.jks
kubectl create secret generic node-secret --from-file=keystore.jks=node-keys.jks --from-file=certificates.jks=node-trust.jks
----
* Pass the names of the created secrets to `helm install`
+
----
--set operator.nodeSet.sslSecret=node-secret --set operator.controllerSet.sslSecret=control-secret
----

IMPORTANT: It is currently **NOT** possible to change the keystore password. LINSTOR expects the passwords to be
`linstor`. This is a current limitation of LINSTOR.

===== Configuring secure communications for the LINSTOR API

Various components need to talk to the LINSTOR controller via its REST interface. This interface can be
secured via HTTPS, which automatically includes authentication. For HTTPS+authentication to work, each component
needs access to:

* A private key
* A certificate based on the key
* A trusted certificate, used to verify that other components are trustworthy

The next sections will guide you through creating all required components.

====== Creating the private keys

Private keys can be created using java's keytool

----
keytool -keyalg rsa -keysize 2048 -genkey -keystore controller.pkcs12 -storetype pkcs12 -storepass linstor -ext san=dns:linstor-op-cs.default.svc -dname "CN=XX, OU=controller, O=Example, L=XX, ST=XX, C=X" -validity 5000
keytool -keyalg rsa -keysize 2048 -genkey -keystore client.pkcs12 -storetype pkcs12 -storepass linstor -dname "CN=XX, OU=client, O=Example, L=XX, ST=XX, C=XX" -validity 5000
----

The clients need private keys and certificate in a different format, so we need to convert it

----
openssl pkcs12 -in client.pkcs12 -passin pass:linstor -out client.cert -clcerts -nokeys
openssl pkcs12 -in client.pkcs12 -passin pass:linstor -out client.key -nocerts -nodes
----

NOTE: The alias specified for the controller key (i.e. `-ext san=dns:linstor-op-cs.default.svc`) has to exactly match the
service name created by the operator. When using `helm`, this is always of the form `<release-name>-cs.<release-namespace>.svc`.

IMPORTANT: It is currently NOT possible to change the keystore password. LINSTOR expects the passwords to be linstor. This is a current limitation of LINSTOR

====== Create the trusted certificates

For the controller to trust the clients, we can use the following command to create a truststore, importing the client certificate

----
keytool -importkeystore -srcstorepass linstor -srckeystore client.pkcs12 -deststorepass linstor -deststoretype pkcs12 -destkeystore controller-trust.pkcs12
----

For the client, we have to convert the controller certificate into a different format

----
openssl pkcs12 -in controller.pkcs12 -passin pass:linstor -out ca.pem -clcerts -nokeys
----

====== Create Kubernetes secrets

Now you can create secrets for the controller and for clients:

----
kubectl create secret generic http-controller --from-file=keystore.jks=controller.pkcs12 --from-file=truststore.jks=controller-trust.pkcs12
kubectl create secret generic http-client --from-file=ca.pem=ca.pem --from-file=client.cert=client.cert --from-file=client.key=client.key
----

The names of the secrets can be passed to `helm install` to configure all clients to use https.

----
--set linstorHttpsControllerSecret=http-controller  --set linstorHttpsClientSecret=http-client
----

===== Automatically set the passphrase for encrypted volumes

Linstor can be used to create encrypted volumes using LUKS. The passphrase used when creating these volumes can
be set via a secret:

----
kubectl create secret generic linstor-pass --from-literal=MASTER_PASSPHRASE=<password>
----

On install, add the following arguments to the helm command:

----
--set operator.controllerSet.luksSecret=linstor-pass
----

[[s-kubernetes-helm-terminate]]
===== Terminating Helm deployment

To protect the storage infrastructure of the cluster from accidentally deleting vital components, it is necessary to perform some manual steps before deleting a Helm deployment.

1. Delete all volume claims managed by LINSTOR components. You can use the following command to get a list of volume claims managed by LINSTOR. After checking that none of the listed volumes still hold needed data, you can delete them using the generated kubectl delete command.
+
----
$ kubectl get pvc --all-namespaces -o=jsonpath='{range .items[?(@.metadata.annotations.volume\.beta\.kubernetes\.io/storage-provisioner=="linstor.csi.linbit.com")]}kubectl delete pvc --namespace {.metadata.namespace} {.metadata.name}{"\n"}{end}'
kubectl delete pvc --namespace default data-mysql-0
kubectl delete pvc --namespace default data-mysql-1
kubectl delete pvc --namespace default data-mysql-2
----
+
WARNING: These volumes, once deleted, cannot be recovered.

2. Delete the LINSTOR controller and satellite resources.
+
Deployment of LINSTOR satellite and controller is controlled by the linstornodeset and linstorcontrollerset resources. You can delete the resources associated with your deployment using kubectl
+
----
kubectl delete linstorcontrollerset <helm-deploy-name>-cs
kubectl delete linstornodeset <helm-deploy-name>-ns
----
+
After a short wait, the controller and satellite pods should terminate. If they continue to run, you can check the above resources for errors (they are only removed after all associated pods terminate)

3. Delete the Helm deployment.
+
If you removed all PVCs and all LINSTOR pods have terminated, you can uninstall the helm deployment
+
----
helm uninstall linstor-op
----
+
NOTE: Due to the Helm's current policy, the Custom Resource Definitions named linstorcontrollerset and linstornodeset will not be deleted by the command.
More information regarding Helm's current position on CRD's can be found
https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#method-1-let-helm-do-it-for-you[here].

[[s-kubernetes-deploy-piraeus-operator]]
==== Deploying with the Piraeus Operator

The community supported edition of the LINSTOR deployment in Kubernetes is
called Piraeus. The Piraeus project provides
https://github.com/piraeusdatastore/piraeus-operator[an operator] for
deployment.

[[s-kubernetes-linstor-interacting]]
=== Interacting with LINSTOR in Kubernetes

The Controller pod includes a LINSTOR Client, making it easy to interact directly with LINSTOR.
For instance:

----
kubectl exec linstor-op-cs-controller-0 -- linstor storage-pool list
----

This should only be necessary for investigating problems and accessing advanced functionality.
Regular operation such as creating volumes should be achieved via the
<<s-kubernetes-basic-configuration-and-deployment,Kubernetes integration>>.

[[s-kubernetes-linstor-csi-plugin-deployment]]
=== LINSTOR CSI Plugin Deployment

The operator Helm chart deploys the LINSTOR CSI plugin for you so if you used
that, you can skip this section.

If you are integrating LINSTOR using a different method, you will need to install the LINSTOR CSI plugin.
Instructions for deploying the CSI plugin can be found on the
https://github.com/LINBIT/linstor-csi[project's github]. This will result in a
linstor-csi-controller _StatefulSet_ and a linstor-csi-node _DaemonSet_ running in the
kube-system namespace.

----
NAME                       READY   STATUS    RESTARTS   AGE     IP              NODE
linstor-csi-controller-0   5/5     Running   0          3h10m   191.168.1.200   kubelet-a
linstor-csi-node-4fcnn     2/2     Running   0          3h10m   192.168.1.202   kubelet-c
linstor-csi-node-f2dr7     2/2     Running   0          3h10m   192.168.1.203   kubelet-d
linstor-csi-node-j66bc     2/2     Running   0          3h10m   192.168.1.201   kubelet-b
linstor-csi-node-qb7fw     2/2     Running   0          3h10m   192.168.1.200   kubelet-a
linstor-csi-node-zr75z     2/2     Running   0          3h10m   192.168.1.204   kubelet-e
----

[[s-kubernetes-basic-configuration-and-deployment]]
=== Basic Configuration and Deployment

Once all linstor-csi __Pod__s are up and running, we can provision volumes
using the usual Kubernetes workflows.

Configuring the behavior and properties of LINSTOR volumes deployed via Kubernetes
is accomplished via the use of __StorageClass__es.

IMPORTANT: the "resourceGroup" parameter is mandatory. Usually you want it to be unique and the same as the storage class name.

Here below is the simplest practical _StorageClass_ that can be used to deploy volumes:

.linstor-basic-sc.yaml
[source,yaml]
----
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  # The name used to identify this StorageClass.
  name: linstor-basic-storage-class
  # The name used to match this StorageClass with a provisioner.
  # linstor.csi.linbit.com is the name that the LINSTOR CSI plugin uses to identify itself
provisioner: linstor.csi.linbit.com
parameters:
  # LINSTOR will provision volumes from the drbdpool storage pool configured
  # On the satellite nodes in the LINSTOR cluster specified in the plugin's deployment
  storagePool: "drbdpool"
  resourceGroup: "linstor-basic-storage-class"
----

DRBD options can be set as well in the parameters section. Valid keys are defined in the
https://app.swaggerhub.com/apis-docs/Linstor/Linstor[LINSTOR REST-API]
(e.g., `DrbdOptions/Net/allow-two-primaries: "yes"`).

We can create the _StorageClass_ with the following command:

----
kubectl create -f linstor-basic-sc.yaml
----

Now that our _StorageClass_ is created, we can now create a _PersistentVolumeClaim_
which can be used to provision volumes known both to Kubernetes and LINSTOR:

.my-first-linstor-volume-pvc.yaml
[source,yaml]
----
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-first-linstor-volume
  annotations:
    # This line matches the PersistentVolumeClaim with our StorageClass
    # and therefore our provisioner.
    volume.beta.kubernetes.io/storage-class: linstor-basic-storage-class
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
----

We can create the _PersistentVolumeClaim_ with the following command:

----
kubectl create -f my-first-linstor-volume-pvc.yaml
----

This will create a _PersistentVolumeClaim_ known to Kubernetes, which will have
a _PersistentVolume_ bound to it, additionally LINSTOR will now create this
volume according to the configuration defined in the `linstor-basic-storage-class`
_StorageClass_. The LINSTOR volume's name will be a UUID prefixed with `csi-`
This volume can be observed with the usual `linstor resource list`. Once that
volume is created, we can attach it to a pod. The following _Pod_ spec will spawn
a Fedora container with our volume attached that busy waits so it is not
unscheduled before we can interact with it:

.my-first-linstor-volume-pod.yaml
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: fedora
  namespace: default
spec:
  containers:
  - name: fedora
    image: fedora
    command: [/bin/bash]
    args: ["-c", "while true; do sleep 10; done"]
    volumeMounts:
    - name: my-first-linstor-volume
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: my-first-linstor-volume
    persistentVolumeClaim:
      claimName: "my-first-linstor-volume"
----

We can create the _Pod_ with the following command:

----
kubectl create -f my-first-linstor-volume-pod.yaml
----

Running `kubectl describe pod fedora` can be used to confirm that _Pod_
scheduling and volume attachment succeeded.

To remove a volume, please ensure that no pod is using it and then delete the
_PersistentVolumeClaim_ via `kubectl`. For example, to remove the volume that we
just made, run the following two commands, noting that the _Pod_ must be
unscheduled before the _PersistentVolumeClaim_ will be removed:

----
kubectl delete pod fedora # unschedule the pod.

kubectl get pod -w # wait for pod to be unscheduled

kubectl delete pvc my-first-linstor-volume # remove the PersistentVolumeClaim, the PersistentVolume, and the LINSTOR Volume.
----

[[s-kubernetes-snapshots]]
=== Snapshots

Creating <<s-linstor-snapshots, snapshots>> and creating new volumes from
snapshots is done via the use of __VolumeSnapshot__s, __VolumeSnapshotClass__es,
and __PVC__s.

[[s-kubernetes-add-snaphot-support]]
==== Adding snapshot support

LINSTOR supports the volume snapshot feature, which is currently in beta. To use it, you need to install a cluster wide
snapshot controller. This is done either by the cluster provider, or you can use the LINSTOR chart.

By default, the LINSTOR chart will install its own snapshot controller. This can lead to conflict in some cases:

* the cluster already has a snapshot controller
* the cluster does not meet the minimal version requirements (>= 1.17)

In such a case, installation of the snapshot controller can be disabled:

----
--set csi-snapshotter.enabled=false
----

[[s-kubernetes-use-snapshot]]
==== Using volume snapshots
Then we can create our _VolumeSnapshotClass_:

.my-first-linstor-snapshot-class.yaml
[source,yaml]
----
apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshotClass
metadata:
  name: my-first-linstor-snapshot-class
driver: linstor.csi.linbit.com
deletionPolicy: Delete
----

Create the _VolumeSnapshotClass_ with `kubectl`:

----
kubectl create -f my-first-linstor-snapshot-class.yaml
----

Now we will create a volume snapshot for the volume that we created above. This
is done with a _VolumeSnapshot_:

.my-first-linstor-snapshot.yaml
[source,yaml]
----
apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshot
metadata:
  name: my-first-linstor-snapshot
spec:
  volumeSnapshotClassName: my-first-linstor-snapshot-class
  source:
    persistentVolumeClaimName: my-first-linstor-volume
----

Create the _VolumeSnapshot_ with `kubectl`:

----
kubectl create -f my-first-linstor-snapshot.yaml
----

You can check that the snapshot creation was successful

----
kubectl describe volumesnapshots.snapshot.storage.k8s.io my-first-linstor-snapshot
...
Spec:
  Source:
    Persistent Volume Claim Name:  my-first-linstor-snapshot
  Volume Snapshot Class Name:      my-first-linstor-snapshot-class
Status:
  Bound Volume Snapshot Content Name:  snapcontent-b6072ab7-6ddf-482b-a4e3-693088136d2c
  Creation Time:                       2020-06-04T13:02:28Z
  Ready To Use:                        true
  Restore Size:                        500Mi
----

Finally, we'll create a new volume from the snapshot with a _PVC_.

.my-first-linstor-volume-from-snapshot.yaml
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-first-linstor-volume-from-snapshot
spec:
  storageClassName: linstor-basic-storage-class
  dataSource:
    name: my-first-linstor-snapshot
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
----

Create the _PVC_ with `kubectl`:

----
kubectl create -f my-first-linstor-volume-from-snapshot.yaml
----


[[s-kubernetes-volume-accessibility]]
=== Volume Accessibility
// This only covers DRBD volumes, section might change if linked docs are updated.
LINSTOR volumes are typically accessible both locally and
<<s-drbd_clients,over the network>>.

By default, the CSI plugin will attach volumes directly if the _Pod_ happens
to be scheduled on a _kubelet_ where its underlying storage is present. However,
_Pod_ scheduling does not currently take volume locality into account. The
<<s-kubernetes-replicasonsame,replicasOnSame>> parameter can be used to restrict
where the underlying storage may be provisioned, if locally attached volumes
are desired.

See <<s-kubernetes-localstoragepolicy,localStoragePolicy>> to see how this
default behavior can be modified.

[[s-kubernetes-stork]]
=== Volume Locality Optimization using Stork

Stork is a scheduler extender plugin for Kubernetes which allows a storage
driver to give the Kubernetes scheduler hints about where to place a new pod
so that it is optimally located for storage performance. You can learn more
about the project on its https://portworx.com/stork-storage-orchestration-kubernetes/[GitHub page].

We are currently working with the maintainers behind Stork to have a LINSTOR
driver shipped with it by default. In the meantime, you can use a custom-built
Stork container by LINBIT which includes a LINSTOR driver,
https://hub.docker.com/repository/docker/linbit/stork[available on Docker Hub]

[[s-kubernetes-using-stork]]
==== Using Stork

By default, the operator will install the components required for Stork, and register a new scheduler called `stork`
with Kubernetes. This new scheduler can be used to place pods near to their volumes.

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  schedulerName: stork <1>
  containers:
  - name: busybox
    image: busybox
    command: ["tail", "-f", "/dev/null"]
    volumeMounts:
    - name: my-first-linstor-volume
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: my-first-linstor-volume
    persistentVolumeClaim:
      claimName: "test-volume"
----

<1> Add the name of the scheduler to your pod.

Deployment of the scheduler can be disabled using

----
--set stork.enabled=false
----

[[s-kubernetes-advanced-configuration]]
=== Advanced Configuration

In general, all configuration for LINSTOR volumes in Kubernetes should be done
via the _StorageClass_ parameters, as seen with the _storagePool_ in the basic
example above. We'll give all the available options an in-depth treatment here.

[[s-kubernetes-nodelist]]
==== nodeList

`nodeList` is a list of nodes for volumes to be assigned to. This will assign
the volume to each node and it will be replicated among all of them. This
can also be used to select a single node by hostname, but it's more flexible to use
<<s-kubernetes-replicasonsame,replicasOnSame>> to select a single node.

IMPORTANT: If you use this option, you must not use <<s-kubernetes-autoplace,autoPlace>>.

TIP: This option determines on which LINSTOR nodes the underlying storage for volumes
will be provisioned and is orthogonal from which _kubelets_ these volumes will be
accessible.

Example: `nodeList: "node-a node-b node-c"`

Example: `nodeList: "node-a"`

[[s-kubernetes-autoplace]]
==== autoPlace

`autoPlace` is an integer that determines the amount of replicas a volume of
this _StorageClass_ will have.  For instance, `autoPlace: 3` will produce
volumes with three-way replication. If neither `autoPlace` nor `nodeList` are
set, volumes will be <<s-autoplace-linstor,automatically placed>> on one node.

IMPORTANT: If you use this option, you must not use <<s-kubernetes-nodelist,nodeList>>.

TIP: This option (and all options which affect autoplacement behavior) modifies the
number of LINSTOR nodes on which the underlying storage for volumes will be
provisioned and is orthogonal to which _kubelets_ those volumes will be accessible
from.

Example: `autoPlace: 2`

Default: `autoPlace: 1`


[[s-kubernetes-replicasonsame]]
==== replicasOnSame

// These should link to the linstor documentation about node properties, but those
// do not exist at the time of this commit.
`replicasOnSame` is a list of key=value pairs used as required autoplacement selection
labels when <<s-kubernetes-autoplace,autoplace>> is used to determine where to
provision storage. These labels correspond to LINSTOR node aux props. Please note both
the key and value names are user-defined and arbitrary. Let's explore this behavior
with examples assuming a LINSTOR cluster such that `node-a` is configured with the
following aux props `zone=z1` and `role=backups`, while `node-b` is configured with
only `zone=z1`.

If we configure a _StorageClass_ with `autoPlace: "1"` and `replicasOnSame: "zone=z1 role=backups"`,
then all volumes created from that _StorageClass_ will be provisioned on `node-a`,
since that is the only node with all of the correct key=value pairs in the LINSTOR
cluster. This is the most flexible way to select a single node for provisioning.

If we configure a _StorageClass_ with `autoPlace: "1"` and `replicasOnSame: "zone=z1"`,
then volumes will be provisioned on either `node-a` or `node-b` as they both have
the `zone=z1` aux prop.

If we configure a _StorageClass_ with `autoPlace: "2"` and `replicasOnSame: "zone=z1 role=backups"`,
then provisioning will fail, as there are not two or more nodes that have
the appropriate aux props.

If we configure a _StorageClass_ with `autoPlace: "2"` and `replicasOnSame: "zone=z1"`,
then volumes will be provisioned on both `node-a` and `node-b` as they both have
the `zone=z1` aux prop.

Example: `replicasOnSame: "zone=z1 role=backups"`

[[s-kubernetes-replicasondifferent]]
==== replicasOnDifferent

`replicasOnDifferent` is a list of key=value pairs to avoid as autoplacement
selection. It is the inverse of <<s-kubernetes-replicasonsame,replicasOnSame>>.

Example: `replicasOnDifferent: "no-csi-volumes=true"`

[[s-kubernetes-localstoragepolicy]]
==== localStoragePolicy

`localStoragePolicy` determines, via volume topology, which LINSTOR
__Satellite__s volumes should be assigned and from where Kubernetes will
access volumes. The behavior of each option is explained below in detail.

IMPORTANT: If you specify a <<s-kubernetes-nodelist,nodeList>>, volumes will
be created on those nodes, irrespective of the `localStoragePolicy`; however,
the accessibility reporting will still be as described.

IMPORTANT: You must set `volumeBindingMode: WaitForFirstConsumer` in the
_StorageClass_ and the LINSTOR __Satellite__s running on the __kubelet__s must
be able to support the diskfull placement of volumes as they are configured in
that _StorageClass_ for <<s-kubernetes-localstoragepolicy-required,required>>
or <<s-kubernetes-localstoragepolicy-preferred,preferred>> to work properly.

TIP: Use `topologyKey: "linbit.com/hostname"` rather than `topologyKey:
"kubernetes.io/hostname"` if you are setting `affinity` in your _Pod_ or
_StatefulSet_ specs.

Example: `localStoragePolicy: required`

[[s-kubernetes-localstoragepolicy-ignore]]
===== ignore (default)

When `localStoragePolicy` is set to `ignore`, regular autoplacement
occurs based on <<s-kubernetes-autoplace,autoplace>>,
<<s-kubernetes-replicasonsame,replicasOnSame>>, and
<<s-kubernetes-replicasonsame,replicasOnDifferent>>. Volume location will not
affect _Pod_ scheduling in Kubernetes and the volumes will be accessed over
the network if they're not local to the _kubelet_ where the _Pod_ was scheduled.

[[s-kubernetes-localstoragepolicy-required]]
===== required

When `localStoragePolicy` is set to `required`, Kubernetes will report a list
of places that it wants to schedule a _Pod_ in order of preference. The plugin
will attempt to provision the volume(s) according to that preference. The
number of volumes to be provisioned in total is based off of
<<s-kubernetes-autoplace,autoplace>>.

If all preferences have been attempted, but no volumes where successfully
assigned volume creation will fail.

In case of multiple replicas when all preferences have been attempted, and at
least one has succeeded, but there are still replicas remaining to be
provisioned, <<s-kubernetes-autoplace,autoplace>> behavior will apply for the
remaining volumes.

With this option set, Kubernetes will consider volumes that are not locally
present on a _kubelet_ to be unaccessible from that _kubelet_.

[[s-kubernetes-localstoragepolicy-preferred]]
===== preferred

When `localStoragePolicy` is set to `preferred`, volume placement behavior
will be the same as when it's set to
<<s-kubernetes-localstoragepolicy-required,required>> with the exception that
volume creation will not fail if no preference was able to be satisfied.
Volume accessibility will be the same as when set to
<<s-kubernetes-localstoragepolicy-ignore,ignore>>.

[[s-kubernetes-storagepool]]
==== storagePool

`storagePool` is the name of the LINSTOR <<s-storage_pools,storage pool>> that
will be used to provide storage to the newly-created volumes.

CAUTION: Only nodes configured with this same _storage pool_ with be considered
for <<s-kubernetes-autoplace,autoplacement>>. Likewise, for _StorageClasses_ using
<<s-kubernetes-nodelist,nodeList>> all nodes specified in that list must have this
_storage pool_ configured on them.

Example: `storagePool: my-storage-pool`

[[s-kubernetes-disklessstoragepool]]
==== disklessStoragePool

// This should link to the linstor section talking about diskless storage pools
// when that gets written.
`disklessStoragePool` is an optional parameter that only effects LINSTOR volumes
assigned disklessly to _kubelets_ i.e., as clients. If you have a custom
_diskless storage pool_ defined in LINSTOR, you'll specify that here.

Example: `disklessStoragePool: my-custom-diskless-pool`

[[s-kubernetes-encryption]]
==== encryption

`encryption` is an optional parameter that determines whether to encrypt
volumes. LINSTOR must be <<s-linstor-encrypted-Volumes,configured for encryption>>
for this to work properly.

Example: `encryption: "true"`

[[s-kubernetes-filesystem]]
==== filesystem

`filesystem` is an option parameter to specify the filesystem for non raw block
volumes. Currently supported options are `xfs` and `ext4`.

Example: `filesystem: "xfs"`

Default: `filesystem: "ext4"`

[[s-kubernetes-fsops]]
==== fsOpts
`fsOpts` is an optional parameter that passes options to the volume's
filesystem at creation time.

IMPORTANT: Please note these values are specific to your chosen
<<s-kubernetes-filesystem, filesystem>>.

Example: `fsOpts: "-b 2048"`

[[s-kubernetes-mountops]]
==== mountOpts
`mountOpts` is an optional parameter that passes options to the volume's
filesystem at mount time.

Example: `mountOpts: "sync,noatime"`
