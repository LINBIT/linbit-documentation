[[ch-kubernetes]]
== LINSTOR Volumes in Kubernetes

indexterm:[Kubernetes]This chapter describes the usage of LINSTOR in Kubernetes
as managed by the operator and with volumes provisioned using the
https://github.com/LINBIT/linstor-csi[LINSTOR CSI plugin].

This Chapter goes into great detail regarding all the install time
options and various configurations possible with LINSTOR and
Kubernetes. For those more interested in a "quick-start" for testing,
or those looking for some examples for reference. We have some
complete <<Helm Install Examples>> of a few common uses near the end
of the chapter.

[[s-kubernetes-overview]]
=== Kubernetes Overview

_Kubernetes_ is a container orchestrator. Kubernetes defines the behavior of
containers and related services via declarative specifications. In this guide,
we'll focus on using `kubectl` to manipulate `.yaml` files that define the
specifications of Kubernetes objects.

[[s-kubernetes-deploy]]
=== Deploying LINSTOR on Kubernetes

[[s-kubernetes-deploy-linstor-operator]]
==== Deploying with the LINSTOR Operator

LINBIT provides a LINSTOR operator to commercial support customers.
The operator eases deployment of LINSTOR on Kubernetes by installing DRBD,
managing Satellite and Controller pods, and other related functions.

The operator itself is installed using a Helm v3 chart as follows:

* Create a kubernetes secret containing your my.linbit.com credentials:
+
----
kubectl create secret docker-registry drbdiocred --docker-server=drbd.io --docker-username=<YOUR_LOGIN> --docker-email=<YOUR_EMAIL> --docker-password=<YOUR_PASSWORD>
----
+
The name of this secret must match the one specified in the Helm values,
by default `drbdiocred`.

* Configure storage for the LINSTOR etcd instance. There are various options
for configuring the etcd instance for LINSTOR:
** Use an existing storage provisioner with a default `StorageClass`.
** <<s-kubernetes-etcd-hostpath-persistence,Use `hostPath` volumes>>.
** Disable persistence for basic testing. This can be done by adding `--set
etcd.persistentVolume.enabled=false` to the `helm install` command below.

* Read <<s-kubernetes-storage, the storage guide>> and configure a basic storage setup for LINSTOR

* Read the <<s-kubernetes-securing-deployment,section on securing the deployment>> and configure as needed.

* Select the appropriate kernel module injector using `--set` with the `helm
install` command in the final step.

** Choose the injector according to the distribution you are using.
Select the latest version from one of `drbd9-rhel7`, `drbd9-rhel8`,...  from http://drbd.io/ as appropriate.
The drbd9-rhel8 image should also be used for RHCOS (OpenShift). For the SUSE CaaS Platform use the SLES injector
that matches the base system of the CaaS Platform you are using (e.g., `drbd9-sles15sp1`). For example:
+
----
operator.satelliteSet.kernelModuleInjectionImage=drbd.io/drbd9-rhel8:v9.0.24
----

** Only inject modules that are already present on the host machine. If a module is not found, it will be skipped.
+
----
operator.satelliteSet.kernelModuleInjectionMode=DepsOnly
----

** Disable kernel module injection if you are installing DRBD by other means. Deprecated by `DepsOnly`
+
----
operator.satelliteSet.kernelModuleInjectionMode=None
----

* Finally create a Helm deployment named `linstor-op` that will set up
everything.
+
----
helm repo add linstor https://charts.linstor.io
helm install linstor-op linstor/linstor
----
Further deployment customization is discussed in the <<s-kubernetes-advanced-deployments,advanced deployment section>>

[[s-kubernetes-etcd-hostpath-persistence]]
===== LINSTOR etcd `hostPath` persistence

You can use the `pv-hostpath` Helm templates to create `hostPath` persistent
volumes. Create as many PVs as needed to satisfy your configured etcd
`replicas` (default 1).

Create the `hostPath` persistent volumes, substituting cluster node
names accordingly in the `nodes=` option:

----
helm repo add linstor https://charts.linstor.io
helm install linstor-etcd linstor/pv-hostpath --set "nodes={<NODE0>,<NODE1>,<NODE2>}"
----

Persistence for etcd is enabled by default.

[[s-kubernetes-existing-database]]
===== Using an existing database

LINSTOR can connect to an existing PostgreSQL, MariaDB or etcd database. For
instance, for a PostgreSQL instance with the following configuration:

----
POSTGRES_DB: postgresdb
POSTGRES_USER: postgresadmin
POSTGRES_PASSWORD: admin123
----

The Helm chart can be configured to use this database instead of deploying an
etcd cluster by adding the following to the Helm install command:

----
--set etcd.enabled=false --set "operator.controller.dbConnectionURL=jdbc:postgresql://postgres/postgresdb?user=postgresadmin&password=admin123"
----

[[s-kubernetes-storage]]
==== Configuring storage

The LINSTOR operator can automate some basic storage set up for LINSTOR.

===== Configuring storage pool creation

The LINSTOR operator can be used to create LINSTOR storage pools. Creation is under control of the
LinstorSatelliteSet resource:

[source]
----
$ kubectl get LinstorSatelliteSet.linstor.linbit.com linstor-op-ns -o yaml
kind: LinstorSatelliteSet
metadata:
..
spec:
  ..
  storagePools:
    lvmPools:
    - name: lvm-thick
      volumeGroup: drbdpool
    lvmThinPools:
    - name: lvm-thin
      thinVolume: thinpool
      volumeGroup: ""
    zfsPools:
    - name: my-linstor-zpool
      zPool: for-linstor
      thin: true
----

====== At install time

At install time, by setting the value of `operator.satelliteSet.storagePools` when running helm install.

First create a file with the storage configuration like:

[source,yaml]
----
operator:
  satelliteSet:
    storagePools:
      lvmPools:
      - name: lvm-thick
        volumeGroup: drbdpool
----

This file can be passed to the helm installation like this:

[source]
----
helm install -f <file> linstor linstor/linstor
----

====== After install

On a cluster with the operator already configured (i.e. after `helm install`),
you can edit the LinstorSatelliteSet configuration like this:

[source]
----
$ kubectl edit LinstorSatelliteSet.linstor.linbit.com <satellitesetname>
----

The storage pool configuration can be updated like in the example above.

===== Preparing physical devices

By default, LINSTOR expects the referenced VolumeGroups, ThinPools and so on to be present. You can use the
`devicePaths: []` option to let LINSTOR automatically prepare devices for the pool. Eligible for automatic configuration
are block devices that:

* Are a root device (no partition)
* do not contain partition information
* have more than 1 GiB

To enable automatic configuration of devices, set the `devicePaths` key on `storagePools` entries:

[source,yaml]
----
  storagePools:
    lvmPools:
    - name: lvm-thick
      volumeGroup: drbdpool
      devicePaths:
      - /dev/vdb
    lvmThinPools:
    - name: lvm-thin
      thinVolume: thinpool
      volumeGroup: linstor_thinpool
      devicePaths:
      - /dev/vdc
      - /dev/vdd
----

Currently, this method supports creation of LVM and LVMTHIN storage pools.

===== `lvmPools` configuration
* `name` name of the LINSTOR storage pool.Required
* `volumeGroup` name of the VG to create.Required
* `devicePaths` devices to configure for this pool.Must be empty and >= 1GiB to be recognized.Optional
* `raidLevel` LVM raid level.Optional
* `vdo` Enable [VDO] (requires VDO tools in the satellite).Optional
* `vdoLogicalSizeKib` Size of the created VG (expected to be bigger than the backing devices by using VDO).Optional
* `vdoSlabSizeKib` Slab size for VDO. Optional

[VDO]: https://www.redhat.com/en/blog/look-vdo-new-linux-compression-layer

===== `lvmThinPools` configuration
* `name` name of the LINSTOR storage pool. Required
* `volumeGroup` VG to use for the thin pool. If you want to use `devicePaths`, you must set this to `""`.
This is required because LINSTOR does not allow configuration of the VG name when preparing devices.
* `thinVolume` name of the thinpool. Required
* `devicePaths` devices to configure for this pool. Must be empty and >= 1GiB to be recognized. Optional
* `raidLevel` LVM raid level. Optional

NOTE: The volume group created by LINSTOR for LVMTHIN pools will always follow the scheme "linstor_$THINPOOL".

===== `zfsPools` configuration
* `name` name of the LINSTOR storage pool. Required
* `zPool` name of the zpool to use. Must already be present on all machines. Required
* `thin` `true` to use thin provisioning, `false` otherwise. Required

===== Using `automaticStorageType` (DEPRECATED)

_ALL_ eligible devices will be prepared according to the value of `operator.satelliteSet.automaticStorageType`, unless
they are already prepared using the `storagePools` section. Devices are added to a storage pool based on the device
name (i.e. all `/dev/nvme1` devices will be part of the pool `autopool-nvme1`)

The possible values for `operator.satelliteSet.automaticStorageType`:

* `None` no automatic set up (default)
* `LVM` create a LVM (thick) storage pool
* `LVMTHIN` create a LVM thin storage pool
* `ZFS` create a ZFS based storage pool (**UNTESTED**)

[[s-kubernetes-securing-deployment]]
==== Securing deployment
This section describes the different options for enabling security features available when
using this operator. The following guides assume the operator is installed <<s-kubernetes-deploy-linstor-operator,using Helm>>

===== Secure communication with an existing etcd instance

Secure communication to an `etcd` instance can be enabled by providing a CA certificate to the operator in form of a
kubernetes secret. The secret has to contain the key `ca.pem` with the PEM encoded CA certificate as value.

The secret can then be passed to the controller by passing the following argument to `helm install`

----
--set operator.controller.dbCertSecret=<secret name>
----

====== Authentication with `etcd` using certificates

If you want to use TLS certificates to authenticate with an `etcd` database, you need to set the following option on
helm install:

----
--set operator.controller.dbUseClientCert=true
----

If this option is active, the secret specified in the above section must contain two additional keys:
* `client.cert` PEM formatted certificate presented to `etcd` for authentication
* `client.key` private key **in PKCS8 format**, matching the above client certificate.
Keys can be converted into PKCS8 format using `openssl`:

----
openssl pkcs8 -topk8 -nocrypt -in client-key.pem -out client-key.pkcs8
----

===== Configuring secure communication between LINSTOR components

The default communication between LINSTOR components is not secured by TLS. If this is needed for your setup,
follow these steps:

* Create private keys in the java keystore format, one for the controller, one for all satellites:
----
keytool -keyalg rsa -keysize 2048 -genkey -keystore satellite-keys.jks -storepass linstor -alias satellite -dname "CN=XX, OU=satellite, O=Example, L=XX, ST=XX, C=X"
keytool -keyalg rsa -keysize 2048 -genkey -keystore control-keys.jks -storepass linstor -alias control -dname "CN=XX, OU=control, O=Example, L=XX, ST=XX, C=XX"
----
* Create a trust store with the public keys that each component needs to trust:
* Controller needs to trust the satellites
* Nodes need to trust the controller
+
----
keytool -importkeystore -srcstorepass linstor -deststorepass linstor -srckeystore control-keys.jks -destkeystore satellite-trust.jks
keytool -importkeystore -srcstorepass linstor -deststorepass linstor -srckeystore satellite-keys.jks -destkeystore control-trust.jks
----

* Create kubernetes secrets that can be passed to the controller and satellite pods
+
----
kubectl create secret generic control-secret --from-file=keystore.jks=control-keys.jks --from-file=certificates.jks=control-trust.jks
kubectl create secret generic satellite-secret --from-file=keystore.jks=satellite-keys.jks --from-file=certificates.jks=satellite-trust.jks
----
* Pass the names of the created secrets to `helm install`
+
----
--set operator.satelliteSet.sslSecret=satellite-secret --set operator.controller.sslSecret=control-secret
----

IMPORTANT: It is currently **NOT** possible to change the keystore password. LINSTOR expects the passwords to be
`linstor`. This is a current limitation of LINSTOR.

===== Configuring secure communications for the LINSTOR API

Various components need to talk to the LINSTOR controller via its REST interface. This interface can be
secured via HTTPS, which automatically includes authentication. For HTTPS+authentication to work, each component
needs access to:

* A private key
* A certificate based on the key
* A trusted certificate, used to verify that other components are trustworthy

The next sections will guide you through creating all required components.

====== Creating the private keys

Private keys can be created using java's keytool

----
keytool -keyalg rsa -keysize 2048 -genkey -keystore controller.pkcs12 -storetype pkcs12 -storepass linstor -ext san=dns:linstor-op-cs.default.svc -dname "CN=XX, OU=controller, O=Example, L=XX, ST=XX, C=X" -validity 5000
keytool -keyalg rsa -keysize 2048 -genkey -keystore client.pkcs12 -storetype pkcs12 -storepass linstor -dname "CN=XX, OU=client, O=Example, L=XX, ST=XX, C=XX" -validity 5000
----

The clients need private keys and certificate in a different format, so we need to convert it

----
openssl pkcs12 -in client.pkcs12 -passin pass:linstor -out client.cert -clcerts -nokeys
openssl pkcs12 -in client.pkcs12 -passin pass:linstor -out client.key -nocerts -nodes
----

NOTE: The alias specified for the controller key (i.e. `-ext san=dns:linstor-op-cs.default.svc`) has to exactly match the
service name created by the operator. When using `helm`, this is always of the form `<release-name>-cs.<release-namespace>.svc`.

IMPORTANT: It is currently NOT possible to change the keystore password. LINSTOR expects the passwords to be linstor. This is a current limitation of LINSTOR

====== Create the trusted certificates

For the controller to trust the clients, we can use the following command to create a truststore, importing the client certificate

----
keytool -importkeystore -srcstorepass linstor -srckeystore client.pkcs12 -deststorepass linstor -deststoretype pkcs12 -destkeystore controller-trust.pkcs12
----

For the client, we have to convert the controller certificate into a different format

----
openssl pkcs12 -in controller.pkcs12 -passin pass:linstor -out ca.pem -clcerts -nokeys
----

====== Create Kubernetes secrets

Now you can create secrets for the controller and for clients:

----
kubectl create secret generic http-controller --from-file=keystore.jks=controller.pkcs12 --from-file=truststore.jks=controller-trust.pkcs12
kubectl create secret generic http-client --from-file=ca.pem=ca.pem --from-file=client.cert=client.cert --from-file=client.key=client.key
----

The names of the secrets can be passed to `helm install` to configure all clients to use https.

----
--set linstorHttpsControllerSecret=http-controller  --set linstorHttpsClientSecret=http-client
----

===== Automatically set the passphrase for encrypted volumes

Linstor can be used to create encrypted volumes using LUKS. The passphrase used when creating these volumes can
be set via a secret:

----
kubectl create secret generic linstor-pass --from-literal=MASTER_PASSPHRASE=<password>
----

On install, add the following arguments to the helm command:

----
--set operator.controller.luksSecret=linstor-pass
----

===== Helm Install Examples

All the below examples use the following `sp-values.yaml` file. Feel
free to adjust this for your uses and environment. See <<Configuring storage pool creation>>
for further details.

-----
operator:
  satelliteSet:
    storagePools:
      lvmThinPools:
      - name: lvm-thin
        thinVolume: thinpool
        volumeGroup: ""
        devicePaths:
        - /dev/sdb
-----

Default install. Please note this does not setup any persistence for
the backing etcd keyvalue store.

WARNING: This is not suggested for any use
outside of testing.

-----
kubectl create secret docker-registry drbdiocred --docker-server=drbd.io --docker-username=<YOUR_LOGIN> --docker-password=<YOUR_PASSWORD>
helm repo add linstor https://charts.linstor.io
helm install linstor-op linstor/linstor
-----

Install with LINSTOR storage-pools defined at install via
`sp-values.yaml`, persistent hostPath volumes, 3 etcd replicas, and by
compiling the DRBD kernel modules for the host kernels.

This should be adequate for most basic deployments. Please note that
this deployment is not using the pre-compiled DRBD kernel modules just
to make this command more portable. Using the pre-compiled binaries
will make for a much faster install and deployment. Using the
`Compile` option would not be suggested for use in a large Kubernetes clusters.

-----
kubectl create secret docker-registry drbdiocred --docker-server=drbd.io --docker-username=<YOUR_LOGIN> --docker-password=<YOUR_PASSWORD>
helm repo add linstor https://charts.linstor.io
helm install linstor-etcd linstor/pv-hostpath --set "nodes={<NODE0>,<NODE1>,<NODE2>}"
helm install -f sp-values.yaml linstor-op linstor/linstor --set etcd.replicas=3 --set operator.satelliteSet.kernelModuleInjectionMode=Compile
-----

Install with LINSTOR storage-pools defined at install via
`sp-values.yaml`, use an already created Postgres DB (preferably
clustered), instead of etcd, and use already compiled kernel modules for
DRBD. Additionally, we'll disable the Stork scheduler in this example.

The Postgres database in this particular example is reachable via a
service endpoint named `postgres`. Postgres itself is configured with
`POSTGRES_DB=postgresdb`, `POSTGRES_USER=postgresadmin`, and
`POSTGRES_PASSWORD=admin123` 

-----
kubectl create secret docker-registry drbdiocred --docker-server=drbd.io --docker-username=<YOUR_LOGIN> --docker-email=<YOUR_EMAIL> --docker-password=<YOUR_PASSWORD>
helm repo add linstor https://charts.linstor.io
helm install -f sp-values.yaml linstor-op linstor/linstor --set etcd.enabled=false --set "operator.controller.dbConnectionURL=jdbc:postgresql://postgres/postgresdb?user=postgresadmin&password=admin123" --set stork.enabled=false
-----

[[s-kubernetes-helm-terminate]]
===== Terminating Helm deployment

To protect the storage infrastructure of the cluster from accidentally deleting vital components, it is necessary to perform some manual steps before deleting a Helm deployment.

1. Delete all volume claims managed by LINSTOR components. You can use the following command to get a list of volume claims managed by LINSTOR. After checking that none of the listed volumes still hold needed data, you can delete them using the generated kubectl delete command.
+
----
$ kubectl get pvc --all-namespaces -o=jsonpath='{range .items[?(@.metadata.annotations.volume\.beta\.kubernetes\.io/storage-provisioner=="linstor.csi.linbit.com")]}kubectl delete pvc --namespace {.metadata.namespace} {.metadata.name}{"\n"}{end}'
kubectl delete pvc --namespace default data-mysql-0
kubectl delete pvc --namespace default data-mysql-1
kubectl delete pvc --namespace default data-mysql-2
----
+
WARNING: These volumes, once deleted, cannot be recovered.

2. Delete the LINSTOR controller and satellite resources.
+
Deployment of LINSTOR satellite and controller is controlled by the LinstorSatelliteSet and LinstorController resources. You can delete the resources associated with your deployment using kubectl
+
----
kubectl delete linstorcontroller <helm-deploy-name>-cs
kubectl delete linstorsatelliteset <helm-deploy-name>-ns
----
+
After a short wait, the controller and satellite pods should terminate. If they continue to run, you can check the above resources for errors (they are only removed after all associated pods terminate)

3. Delete the Helm deployment.
+
If you removed all PVCs and all LINSTOR pods have terminated, you can uninstall the helm deployment
+
----
helm uninstall linstor-op
----
+
NOTE: Due to the Helm's current policy, the Custom Resource Definitions named LinstorController and LinstorSatelliteSet will not be deleted by the command.
More information regarding Helm's current position on CRD's can be found
https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#method-1-let-helm-do-it-for-you[here].

[[s-kubernetes-advanced-deployments]]
==== Advanced deployment options

The helm charts provide a set of further customization options for advanced use cases.

[source,yaml]
----
global:
  imagePullPolicy: IfNotPresent # empty pull policy means k8s default is used ("always" if tag == ":latest", "ifnotpresent" else) <1>
  setSecurityContext: true # Force non-privileged containers to run as non-root users
# Dependency charts
etcd:
  persistentVolume:
    enabled: true
    storage: 1Gi
  replicas: 1 # How many instances of etcd will be added to the initial cluster. <2>
  resources: {} # resource requirements for etcd containers <3>
  image:
    repository: gcr.io/etcd-development/etcd
    tag: v3.4.9
csi-snapshotter:
  enabled: true # <- enable to add k8s snapshotting CRDs and controller. Needed for CSI snapshotting
  image: k8s.gcr.io/sig-storage/snapshot-controller:v3.0.2
  replicas: 1 <2>
  resources: {} # resource requirements for the cluster snapshot controller. <3>
stork:
  enabled: true
  storkImage: docker.io/openstorage/stork:2.5.0
  schedulerImage: k8s.gcr.io/kube-scheduler-amd64
  schedulerTag: ""
  replicas: 1 <2>
  storkResources: {} # resources requirements for the stork plugin containers <3>
  schedulerResources: {} # resource requirements for the kube-scheduler containers <3>
  podsecuritycontext: {}
csi:
  enabled: true
  pluginImage: "drbd.io/linstor-csi:v0.11.0"
  csiAttacherImage: k8s.gcr.io/sig-storage/csi-attacher:v3.0.2
  csiLivenessProbeImage: k8s.gcr.io/sig-storage/livenessprobe:v2.1.0
  csiNodeDriverRegistrarImage: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1
  csiProvisionerImage: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4
  csiSnapshotterImage: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2
  csiResizerImage: k8s.gcr.io/sig-storage/csi-resizer:v1.0.1
  controllerReplicas: 1 <2>
  nodeAffinity: {} <4>
  nodeTolerations: [] <4>
  controllerAffinity: {} <4>
  controllerTolerations: [] <4>
  enableTopology: false
  resources: {} <3>
priorityClassName: ""
drbdRepoCred: drbdiocred
linstorHttpsControllerSecret: "" # <- name of secret containing linstor server certificates+key.
linstorHttpsClientSecret: "" # <- name of secret containing linstor client certificates+key.
controllerEndpoint: "" # <- override to the generated controller endpoint. use if controller is not deployed via operator
psp:
  privilegedRole: ""
  unprivilegedRole: ""
operator:
  replicas: 1 # <- number of replicas for the operator deployment <2>
  image: "drbd.io/linstor-operator:v1.3.1"
  affinity: {} <4>
  tolerations: [] <4>
  resources: {} <3>
  podsecuritycontext: {}
  controller:
    enabled: true
    controllerImage: "drbd.io/linstor-controller:v1.11.1"
    luksSecret: ""
    dbCertSecret: ""
    dbUseClientCert: false
    sslSecret: ""
    affinity: {} <4>
    tolerations: <4>
      - key: node-role.kubernetes.io/master
        operator: "Exists"
        effect: "NoSchedule"
    resources: {} <3>
    replicas: 1 <2>
  satelliteSet:
    enabled: true
    satelliteImage: "drbd.io/linstor-satellite:v1.11.1"
    storagePools: {}
    sslSecret: ""
    automaticStorageType: None
    affinity: {} <4>
    tolerations: [] <4>
    resources: {} <3>
    kernelModuleInjectionImage: "drbd.io/drbd9-rhel7:v9.0.27"
    kernelModuleInjectionMode: ShippedModules
    kernelModuleInjectionResources: {} <3>
haController:
  enabled: true
  image: drbd.io/linstor-k8s-ha-controller:v0.1.3
  affinity: {} <4>
  tolerations: [] <4>
  resources: {} <3>
  replicas: 1 <2>
----
<1> Sets the pull policy for all images.
<2> Controls the number of replicas for each component.
<3> Set container resource requests and limits. See https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/[the kubernetes docs].
Most containers need a minimal amount of resources, except for:
* `etcd.resources` See the https://etcd.io/docs/v3.4.0/op-guide/hardware/[etcd docs]
* `operator.controller.resources` Around `700MiB` memory is required
* `operater.satelliteSet.resources` Around `700MiB` memory is required
* `operator.satelliteSet.kernelModuleInjectionResources` If kernel modules are compiled, 1GiB of memory is required.
<4> Affinity and toleration determine where pods are scheduled on the cluster. See the
https://kubernetes.io/docs/concepts/scheduling-eviction/[kubernetes docs on affinity and toleration].
This may be especially important for the `operator.satelliteSet` and `csi.node*` values. To schedule a pod using
a LINSTOR persistent volume, the node requires a running LINSTOR satellite and LINSTOR CSI pod.

[[s-kubernetes-ha-deployment]]
===== High Availability Deployment
To create a High Availability deployment of all components, take a look at the https://github.com/piraeusdatastore/piraeus-operator/blob/b00fd34/doc/scheduling.md[upstream guide]
The default values are chosen so that scaling the components to multiple replicas ensures that the replicas are placed on different nodes. This ensures
that a single node failures will not interrupt the service.

[[s-kubernetes-deploy-external-controller]]
==== Deploying with an external LINSTOR controller

The operator can configure the satellites and CSI plugin to use an existing LINSTOR setup. This can be useful in cases
where the storage infrastructure is separate from the Kubernetes cluster. Volumes can be provisioned in diskless mode
on the Kubernetes nodes while the storage nodes will provide the backing disk storage.

To skip the creation of a LINSTOR Controller deployment and configure the other components to use your existing LINSTOR
Controller, use the following options when running `helm install`:

* `operator.controller.enabled=false` This disables creation of the `LinstorController` resource
* `operator.etcd.enabled=false` Since no LINSTOR Controller will run on Kubernetes, no database is required.
* `controllerEndpoint=<url-of-linstor-controller>` The HTTP endpoint of the existing LINSTOR Controller. For example: `http://linstor.storage.cluster:3370/`

After all pods are ready, you should see the Kubernetes cluster nodes as satellites in your LINSTOR setup.

IMPORTANT: Your kubernetes nodes must be reachable using their IP by the controller and storage nodes.

Create a storage class referencing an existing storage pool on your storage nodes.

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: linstor-on-k8s
provisioner: linstor.csi.linbit.com
parameters:
  autoPlace: "3"
  storagePool: existing-storage-pool
  resourceGroup: linstor-on-k8s
----

You can provision new volumes by creating PVCs using your storage class. The volumes will first be placed only on nodes
with the given storage pool, i.e. your storage infrastructure. Once you want to use the volume in a pod, LINSTOR CSI
will create a diskless resource on the Kubernetes node and attach over the network to the diskfull resource.

[[s-kubernetes-deploy-piraeus-operator]]
==== Deploying with the Piraeus Operator

The community supported edition of the LINSTOR deployment in Kubernetes is
called Piraeus. The Piraeus project provides
https://github.com/piraeusdatastore/piraeus-operator[an operator] for
deployment.

[[s-kubernetes-linstor-interacting]]
=== Interacting with LINSTOR in Kubernetes

The Controller pod includes a LINSTOR Client, making it easy to interact directly with LINSTOR.
For instance:

----
kubectl exec linstor-op-cs-controller-<deployment-info> -- linstor storage-pool list
----

This should only be necessary for investigating problems and accessing advanced functionality.
Regular operation such as creating volumes should be achieved via the
<<s-kubernetes-basic-configuration-and-deployment,Kubernetes integration>>.

[[s-kubernetes-linstor-csi-plugin-deployment]]
=== LINSTOR CSI Plugin Deployment

The operator Helm chart deploys the LINSTOR CSI plugin for you so if you used
that, you can skip this section.

If you are integrating LINSTOR using a different method, you will need to install the LINSTOR CSI plugin.
Instructions for deploying the CSI plugin can be found on the
https://github.com/LINBIT/linstor-csi[project's github]. This will result in a
linstor-csi-controller _Deployment_ and a linstor-csi-node _DaemonSet_ running in the
kube-system namespace.

----
NAME                           READY   STATUS    RESTARTS   AGE     IP              NODE
linstor-csi-controller-ab789   5/5     Running   0          3h10m   191.168.1.200   kubelet-a
linstor-csi-node-4fcnn         2/2     Running   0          3h10m   192.168.1.202   kubelet-c
linstor-csi-node-f2dr7         2/2     Running   0          3h10m   192.168.1.203   kubelet-d
linstor-csi-node-j66bc         2/2     Running   0          3h10m   192.168.1.201   kubelet-b
linstor-csi-node-qb7fw         2/2     Running   0          3h10m   192.168.1.200   kubelet-a
linstor-csi-node-zr75z         2/2     Running   0          3h10m   192.168.1.204   kubelet-e
----

[[s-kubernetes-basic-configuration-and-deployment]]
=== Basic Configuration and Deployment

Once all linstor-csi __Pod__s are up and running, we can provision volumes
using the usual Kubernetes workflows.

Configuring the behavior and properties of LINSTOR volumes deployed via Kubernetes
is accomplished via the use of __StorageClass__es.

IMPORTANT: the "resourceGroup" parameter is mandatory. Usually you want it to be unique and the same as the storage class name.

Here below is the simplest practical _StorageClass_ that can be used to deploy volumes:

.linstor-basic-sc.yaml
[source,yaml]
----
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  # The name used to identify this StorageClass.
  name: linstor-basic-storage-class
  # The name used to match this StorageClass with a provisioner.
  # linstor.csi.linbit.com is the name that the LINSTOR CSI plugin uses to identify itself
provisioner: linstor.csi.linbit.com
parameters:
  # LINSTOR will provision volumes from the drbdpool storage pool configured
  # On the satellite nodes in the LINSTOR cluster specified in the plugin's deployment
  storagePool: "drbdpool"
  resourceGroup: "linstor-basic-storage-class"
  # Setting a fstype is required for "fsGroup" permissions to work correctly.
  # Currently supported: xfs/ext4
  csi.storage.k8s.io/fstype: xfs
----

DRBD options can be set as well in the parameters section. Valid keys are defined in the
https://app.swaggerhub.com/apis-docs/Linstor/Linstor[LINSTOR REST-API]
(e.g., `DrbdOptions/Net/allow-two-primaries: "yes"`).

We can create the _StorageClass_ with the following command:

----
kubectl create -f linstor-basic-sc.yaml
----

Now that our _StorageClass_ is created, we can now create a _PersistentVolumeClaim_
which can be used to provision volumes known both to Kubernetes and LINSTOR:

.my-first-linstor-volume-pvc.yaml
[source,yaml]
----
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-first-linstor-volume
spec:
  storageClassName: linstor-basic-storage-class
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
----

We can create the _PersistentVolumeClaim_ with the following command:

----
kubectl create -f my-first-linstor-volume-pvc.yaml
----

This will create a _PersistentVolumeClaim_ known to Kubernetes, which will have
a _PersistentVolume_ bound to it, additionally LINSTOR will now create this
volume according to the configuration defined in the `linstor-basic-storage-class`
_StorageClass_. The LINSTOR volume's name will be a UUID prefixed with `csi-`
This volume can be observed with the usual `linstor resource list`. Once that
volume is created, we can attach it to a pod. The following _Pod_ spec will spawn
a Fedora container with our volume attached that busy waits so it is not
unscheduled before we can interact with it:

.my-first-linstor-volume-pod.yaml
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: fedora
  namespace: default
spec:
  containers:
  - name: fedora
    image: fedora
    command: [/bin/bash]
    args: ["-c", "while true; do sleep 10; done"]
    volumeMounts:
    - name: my-first-linstor-volume
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: my-first-linstor-volume
    persistentVolumeClaim:
      claimName: "my-first-linstor-volume"
----

We can create the _Pod_ with the following command:

----
kubectl create -f my-first-linstor-volume-pod.yaml
----

Running `kubectl describe pod fedora` can be used to confirm that _Pod_
scheduling and volume attachment succeeded.

To remove a volume, please ensure that no pod is using it and then delete the
_PersistentVolumeClaim_ via `kubectl`. For example, to remove the volume that we
just made, run the following two commands, noting that the _Pod_ must be
unscheduled before the _PersistentVolumeClaim_ will be removed:

----
kubectl delete pod fedora # unschedule the pod.

kubectl get pod -w # wait for pod to be unscheduled

kubectl delete pvc my-first-linstor-volume # remove the PersistentVolumeClaim, the PersistentVolume, and the LINSTOR Volume.
----

[[s-kubernetes-sc-parameters]]
==== Available parameters in a StorageClass

The following storage class contains all currently available parameters to configure the provisioned storage

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: full-example
provisioner: linstor.csi.linbit.com
parameters:
  # CSI related parameters
  csi.storage.k8s.io/fstype: xfs
  # LINSTOR parameters
  autoPlace: "2"
  placementCount: "2"
  resourceGroup: "full-example"
  storagePool: "my-storage-pool"
  disklessStoragePool: "DfltDisklessStorPool"
  layerList: "drbd,storage"
  placementPolicy: "AutoPlace"
  allowRemoteVolumeAccess: "true"
  encryption: "true"
  nodeList: "diskful-a,diskful-b"
  clientList: "diskless-a,diskless-b"
  replicasOnSame: "zone=a"
  replicasOnDifferent: "rack"
  disklessOnRemaining: "false"
  doNotPlaceWithRegex: "tainted.*"
  fsOpts: "nodiscard"
  mountOpts: "noatime"
  postMountXfsOpts: "extsize 2m"
  # DRBD parameters
  DrbdOptions/*: <x>
----

==== csi.storage.k8s.io/fstype

Sets the file system type to create for `volumeMode: FileSystem` PVCs. Currently supported are:

* `ext4` (default)
* `xfs`

[[s-kubernetes-autoplace]]
==== autoPlace

`autoPlace` is an integer that determines the amount of replicas a volume of
this _StorageClass_ will have.  For instance, `autoPlace: "3"` will produce
volumes with three-way replication. If neither `autoPlace` nor `nodeList` are
set, volumes will be <<s-autoplace-linstor,automatically placed>> on one node.

IMPORTANT: If you use this option, you must not use <<s-kubernetes-nodelist,nodeList>>.

IMPORTANT: You have to use quotes, otherwise Kubernetes will complain about a malformed _StorageClass_.

TIP: This option (and all options which affect autoplacement behavior) modifies the
number of LINSTOR nodes on which the underlying storage for volumes will be
provisioned and is orthogonal to which _kubelets_ those volumes will be accessible
from.

==== placementCount

`placementCount` is an alias for <<s-kubernetes-autoplace,`autoPlace`>>

==== resourceGroup

The <<s-linstor-resource-groups, LINSTOR Resource Group (RG)>> to associate with this StorageClass. If not set,
a new RG will be created for each new PVC.

[[s-kubernetes-storagepool]]
==== storagePool

`storagePool` is the name of the LINSTOR <<s-storage_pools,storage pool>> that
will be used to provide storage to the newly-created volumes.

CAUTION: Only nodes configured with this same _storage pool_ with be considered
for <<s-kubernetes-autoplace,autoplacement>>. Likewise, for _StorageClasses_ using
<<s-kubernetes-nodelist,nodeList>> all nodes specified in that list must have this
_storage pool_ configured on them.

[[s-kubernetes-disklessstoragepool]]
==== disklessStoragePool

`disklessStoragePool` is an optional parameter that only effects LINSTOR volumes
assigned disklessly to _kubelets_ i.e., as clients. If you have a custom
_diskless storage pool_ defined in LINSTOR, you'll specify that here.

==== layerList

A comma-seperated list of layers to use for the created volumes. The available layers and their order are described
towards the end of <<s-linstor-without-drbd, this section>>. Defaults to `drbd,storage`

[[s-kubernetes-placementpolicy]]
==== placementPolicy

Select from one of the available volume schedulers:

* `AutoPlace`, the default: Use LINSTOR autoplace, influenced by <<s-kubernetes-replicasonsame>> and <<s-kubernetes-replicasondifferent>>
* `FollowTopology`: Use CSI Topology information to place at least one volume in each "preferred" zone. Only useable if CSI Topology is enabled.
* `Manual`: Use only the nodes listed in `nodeList` and `clientList`.
* `Balanced`: **EXPERIMENTAL** Place volumes across failure domains, using the least used storage pool on each selected node.

==== allowRemoteVolumeAccess

Disable remote access to volumes. This implies that volumes can only be accessed from the initial set of nodes selected
on creation. CSI Topology processing is required to place pods on the correct nodes.

[[s-kubernetes-encryption]]
==== encryption

`encryption` is an optional parameter that determines whether to encrypt
volumes. LINSTOR must be <<s-linstor-encrypted-volumes,configured for encryption>>
for this to work properly.

[[s-kubernetes-nodelist]]
==== nodeList

`nodeList` is a list of nodes for volumes to be assigned to. This will assign
the volume to each node and it will be replicated among all of them. This
can also be used to select a single node by hostname, but it's more flexible to use
<<s-kubernetes-replicasonsame,replicasOnSame>> to select a single node.

IMPORTANT: If you use this option, you must not use <<s-kubernetes-autoplace,autoPlace>>.

TIP: This option determines on which LINSTOR nodes the underlying storage for volumes
will be provisioned and is orthogonal from which _kubelets_ these volumes will be
accessible.

==== clientList

`clientList` is a list of nodes for diskless volumes to be assigned to. Use in conjunction with <<s-kubernetes-nodelist>>.

[[s-kubernetes-replicasonsame]]
==== replicasOnSame

// These should link to the linstor documentation about node properties, but those
// do not exist at the time of this commit.
`replicasOnSame` is a list of `key` or `key=value` items used as autoplacement selection
labels when <<s-kubernetes-autoplace,autoplace>> is used to determine where to
provision storage. These labels correspond to LINSTOR node properties.

IMPORTANT: LINSTOR node properties  are different from kubernetes node labels. You can see the properties of a node by
running `linstor node list-properties <nodename>`. You can also set additional properties ("auxiliary properties"): `linstor node set-property <nodename> --aux <key> <value>`.

Let's explore this behavior with examples assuming a LINSTOR cluster such that `node-a` is configured with the
following auxiliary property `zone=z1` and `role=backups`, while `node-b` is configured with
only `zone=z1`.

If we configure a _StorageClass_ with `autoPlace: "1"` and `replicasOnSame: "zone=z1 role=backups"`,
then all volumes created from that _StorageClass_ will be provisioned on `node-a`,
since that is the only node with all of the correct key=value pairs in the LINSTOR
cluster. This is the most flexible way to select a single node for provisioning.

IMPORTANT: This guide assumes LINSTOR CSI version 0.10.0 or newer. All properties referenced in `replicasOnSame`
and `replicasOnDifferent` are interpreted as auxiliary properties. If you are using an older version of LINSTOR CSI, you
need to add the `Aux/` prefix to all property names. So `replicasOnSame: "zone=z1"` would be `replicasOnSame: "Aux/zone=z1"`
Using `Aux/` manually will continue to work on newer LINSTOR CSI versions.

If we configure a _StorageClass_ with `autoPlace: "1"` and `replicasOnSame: "zone=z1"`,
then volumes will be provisioned on either `node-a` or `node-b` as they both have
the `zone=z1` aux prop.

If we configure a _StorageClass_ with `autoPlace: "2"` and `replicasOnSame: "zone=z1 role=backups"`,
then provisioning will fail, as there are not two or more nodes that have
the appropriate auxiliary properties.

If we configure a _StorageClass_ with `autoPlace: "2"` and `replicasOnSame: "zone=z1"`,
then volumes will be provisioned on both `node-a` and `node-b` as they both have
the `zone=z1` aux prop.

You can also use a property key without providing a value to ensure all replicas are placed on nodes with the same property value,
with caring about the particular value. Assuming there are 4 nodes, `node-a1` and `node-a2` are configured with `zone=a`. `node-b1` and `node-b2`
are configured with `zone=b`. Using `autoPlace: "2"` and `replicasOnSame: "zone"` will place on either `node-a1` and `node-a2` OR on `node-b1` and `node-b2`.

[[s-kubernetes-replicasondifferent]]
==== replicasOnDifferent

`replicasOnDifferent` takes a list of properties to consider, same as <<s-kubernetes-replicasonsame,replicasOnSame>>.
There are two modes of using `replicasOnDifferent`:

* Preventing volume placement on specific nodes:
+
If a value is given for the property, the nodes which have that property-value pair assigned will be considered last.
+
Example: `replicasOnDifferent: "no-csi-volumes=true"` will place no volume on any node with property
`no-csi-volumes=true` unless there are not enough other nodes to fulfill the `autoPlace` setting.

* Distribute volumes across nodes with different values for the same key:
+
If no property value is given, LINSTOR will place the volumes across nodes with different values for that property if
possible.
+
Example: Assuming there are 4 nodes, `node-a1` and `node-a2` are configured with `zone=a`. `node-b1` and `node-b2`
are configured with `zone=b`. Using a _StorageClass_ with `autoPlace: "2"` and `replicasOnDifferent: "zone"`,
LINSTOR will create one replica on either `node-a1` or `node-a2` _and_ one replica on either `node-b1` or `node-b2`.

==== disklessOnRemaining

Create a diskless resource on _all_ nodes that were not assigned a diskful resource.

==== doNotPlaceWithRegex

Do not place the resource on a node which has a resource with a name matching the regex.

[[s-kubernetes-fsops]]
==== fsOpts
`fsOpts` is an optional parameter that passes options to the volume's
filesystem at creation time.

IMPORTANT: Please note these values are specific to your chosen
<<s-kubernetes-filesystem, filesystem>>.

[[s-kubernetes-mountops]]
==== mountOpts
`mountOpts` is an optional parameter that passes options to the volume's
filesystem at mount time.

==== postMountXfsOpts

Extra arguments to pass to `xfs_io`, which gets called before right before first use of the volume.

====  DrbdOptions/*: <x>

Advanced DRBD options to pass to LINSTOR. For example, to change the replication protocol, use
`DrbdOptions/Net/protocol: "A"`.

The full list of options is available https://app.swaggerhub.com/apis-docs/Linstor/Linstor/1.5.0#/developers/resourceDefinitionModify[here]

[[s-kubernetes-snapshots]]
=== Snapshots

Creating <<s-linstor-snapshots, snapshots>> and creating new volumes from
snapshots is done via the use of __VolumeSnapshot__s, __VolumeSnapshotClass__es,
and __PVC__s.

[[s-kubernetes-add-snaphot-support]]
==== Adding snapshot support

LINSTOR supports the volume snapshot feature, which is currently in beta. To use it, you need to install a cluster wide
snapshot controller. This is done either by the cluster provider, or you can use the LINSTOR chart.

By default, the LINSTOR chart will install its own snapshot controller. This can lead to conflict in some cases:

* the cluster already has a snapshot controller
* the cluster does not meet the minimal version requirements (>= 1.17)

In such a case, installation of the snapshot controller can be disabled:

----
--set csi-snapshotter.enabled=false
----

[[s-kubernetes-use-snapshot]]
==== Using volume snapshots
Then we can create our _VolumeSnapshotClass_:

.my-first-linstor-snapshot-class.yaml
[source,yaml]
----
apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshotClass
metadata:
  name: my-first-linstor-snapshot-class
driver: linstor.csi.linbit.com
deletionPolicy: Delete
----

Create the _VolumeSnapshotClass_ with `kubectl`:

----
kubectl create -f my-first-linstor-snapshot-class.yaml
----

Now we will create a volume snapshot for the volume that we created above. This
is done with a _VolumeSnapshot_:

.my-first-linstor-snapshot.yaml
[source,yaml]
----
apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshot
metadata:
  name: my-first-linstor-snapshot
spec:
  volumeSnapshotClassName: my-first-linstor-snapshot-class
  source:
    persistentVolumeClaimName: my-first-linstor-volume
----

Create the _VolumeSnapshot_ with `kubectl`:

----
kubectl create -f my-first-linstor-snapshot.yaml
----

You can check that the snapshot creation was successful

----
kubectl describe volumesnapshots.snapshot.storage.k8s.io my-first-linstor-snapshot
...
Spec:
  Source:
    Persistent Volume Claim Name:  my-first-linstor-snapshot
  Volume Snapshot Class Name:      my-first-linstor-snapshot-class
Status:
  Bound Volume Snapshot Content Name:  snapcontent-b6072ab7-6ddf-482b-a4e3-693088136d2c
  Creation Time:                       2020-06-04T13:02:28Z
  Ready To Use:                        true
  Restore Size:                        500Mi
----

Finally, we'll create a new volume from the snapshot with a _PVC_.

.my-first-linstor-volume-from-snapshot.yaml
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-first-linstor-volume-from-snapshot
spec:
  storageClassName: linstor-basic-storage-class
  dataSource:
    name: my-first-linstor-snapshot
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
----

Create the _PVC_ with `kubectl`:

----
kubectl create -f my-first-linstor-volume-from-snapshot.yaml
----


[[s-kubernetes-volume-accessibility]]
=== Volume Accessibility
// This only covers DRBD volumes, section might change if linked docs are updated.
LINSTOR volumes are typically accessible both locally and
<<s-drbd_clients,over the network>>.

By default, the CSI plugin will attach volumes directly if the _Pod_ happens
to be scheduled on a _kubelet_ where its underlying storage is present. However,
_Pod_ scheduling does not currently take volume locality into account. The
<<s-kubernetes-replicasonsame,replicasOnSame>> parameter can be used to restrict
where the underlying storage may be provisioned, if locally attached volumes
are desired.

See <<s-kubernetes-placementpolicy>> to see how this
default behavior can be modified.

[[s-kubernetes-stork]]
=== Volume Locality Optimization using Stork

Stork is a scheduler extender plugin for Kubernetes which allows a storage
driver to give the Kubernetes scheduler hints about where to place a new pod
so that it is optimally located for storage performance. You can learn more
about the project on its https://portworx.com/stork-storage-orchestration-kubernetes/[GitHub page].

The next Stork release will include the LINSTOR driver by default.
In the meantime, you can use a custom-built Stork container by LINBIT which includes a LINSTOR driver,
https://hub.docker.com/repository/docker/linbit/stork[available on Docker Hub]

[[s-kubernetes-using-stork]]
==== Using Stork

By default, the operator will install the components required for Stork, and register a new scheduler called `stork`
with Kubernetes. This new scheduler can be used to place pods near to their volumes.

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  schedulerName: stork <1>
  containers:
  - name: busybox
    image: busybox
    command: ["tail", "-f", "/dev/null"]
    volumeMounts:
    - name: my-first-linstor-volume
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: my-first-linstor-volume
    persistentVolumeClaim:
      claimName: "test-volume"
----

<1> Add the name of the scheduler to your pod.

Deployment of the scheduler can be disabled using

----
--set stork.enabled=false
----

[[s-kubernetes-ha-controller]]
=== Fast workload fail over using the High Availability Controller

The LINSTOR High Availability Controller (HA Controller) will speed up the fail over process for stateful workloads using LINSTOR for storage.
It is deployed by default, and can be scaled to multiple replicas:

[source]
----
$ kubectl get pods -l app.kubernetes.io/name=linstor-ha-controller
NAME                                    READY   STATUS    RESTARTS   AGE
linstor-ha-controller-f496c5f77-fr76m   1/1     Running   0          89s
linstor-ha-controller-f496c5f77-jnqtc   1/1     Running   0          89s
linstor-ha-controller-f496c5f77-zcrqg   1/1     Running   0          89s
----

In the event of node failures, Kubernetes is very conservative in rescheduling stateful workloads. This means it can
take more than 15 minutes for Pods to be moved from unreachable nodes. With the information available to DRBD and
LINSTOR, this process can be sped up significantly.

The HA Controller enables fast fail over for:

* Pods using DRBD backed PersistentVolumes. The DRBD resources must make use of the quorum functionality
  LINSTOR will configure this automatically for volumes with 2 or more replicas in clusters with at least 3 nodes.
* The workload does not use any external resources in a way that could lead to a conflicting state if two instances
  try to use the external resource at the same time. While DRBD can ensure that only one instance can have write access
  to the storage, it cannot provide the same guarantee for external resources.
* The Pod is marked with the `linstor.csi.linbit.com/on-storage-lost: remove` label.

==== Example

The following StatefulSet uses the HA Controller to manage fail over of a pod.

[source,yaml]
----
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: my-stateful-app
spec:
  serviceName: my-stateful-app
  selector:
    matchLabels:
      app.kubernetes.io/name: my-stateful-app
  template:
    metadata:
      labels:
        app.kubernetes.io/name: my-stateful-app
        linstor.csi.linbit.com/on-storage-lost: remove <1>
    ...
----

<1> The label is applied to Pod template, **not** the StatefulSet. The label was applied correctly, if your Pod appears
in the output of `kubectl get pods -l linstor.csi.linbit.com/on-storage-lost=remove`.

Deploy the set and wait for the pod to start

[source]
----
$ kubectl get pod -o wide
NAME                                        READY   STATUS              RESTARTS   AGE     IP                NODE                    NOMINATED NODE   READINESS GATES
my-stateful-app-0                           1/1     Running             0          5m      172.31.0.1        node01.ha.cluster       <none>           <none>
----

Then one of the nodes becomes unreachable. Shortly after, Kubernetes will mark the node as `NotReady`

[source]
----
$ kubectl get nodes
NAME                    STATUS     ROLES     AGE    VERSION
master01.ha.cluster     Ready      master    12d    v1.19.4
master02.ha.cluster     Ready      master    12d    v1.19.4
master03.ha.cluster     Ready      master    12d    v1.19.4
node01.ha.cluster       NotReady   compute   12d    v1.19.4
node02.ha.cluster       Ready      compute   12d    v1.19.4
node03.ha.cluster       Ready      compute   12d    v1.19.4
----

After about 45 seconds, the Pod will be removed by the HA Controller and re-created by the StatefulSet

----
$ kubectl get pod -o wide
NAME                                        READY   STATUS              RESTARTS   AGE     IP                NODE                    NOMINATED NODE   READINESS GATES
my-stateful-app-0                           0/1     ContainerCreating   0          3s      172.31.0.1        node02.ha.cluster       <none>           <none>
$ kubectl get events --sort-by=.metadata.creationTimestamp -w
...
0s          Warning   ForceDeleted              pod/my-stateful-app-0                                                                   pod deleted because a used volume is marked as failing
0s          Warning   ForceDetached             volumeattachment/csi-d2b994ff19d526ace7059a2d8dea45146552ed078d00ed843ac8a8433c1b5f6f   volume detached because it is marked as failing
...
----
