[[ch-kubernetes]]
== LINSTOR Volumes in Kubernetes

indexterm:[Kubernetes]This chapter describes the usage of LINSTOR in Kubernetes
as managed by the operator and with volumes provisioned using the
https://github.com/LINBIT/linstor-csi[LINSTOR CSI plugin].

[[s-kubernetes-overview]]
=== Kubernetes Overview

_Kubernetes_ is a container orchestrator. Kubernetes defines the behavior of
containers and related services via declarative specifications. In this guide,
we'll focus on using `kubectl` to manipulate `.yaml` files that define the
specifications of Kubernetes objects.

[[s-kubernetes-deploy]]
=== Deploying LINSTOR on Kubernetes

[[s-kubernetes-deploy-linstor-operator]]
==== Deploying with the LINSTOR Operator

LINBIT provides a LINSTOR operator to commercial support customers.
The operator eases deployment of LINSTOR on Kubernetes by installing DRBD,
managing Satellite and Controller pods, and other related functions.

The operator itself is installed using a Helm v3 chart as follows:

* Create a kubernetes secret containing your my.linbit.com credentials:
+
----
kubectl create secret docker-registry drbdiocred --docker-server=drbd.io --docker-username=<YOUR_LOGIN> --docker-email=<YOUR_EMAIL> --docker-password=<YOUR_PASSWORD>
----
+
The name of this secret must match the one specified in the Helm values,
by default `drbdiocred`.

* Configure storage for the LINSTOR etcd instance. There are various options
for configuring the etcd instance for LINSTOR:
** Use an existing storage provisioner with a default `StorageClass`.
** <<s-kubernetes-etcd-hostpath-persistence,Use `hostPath` volumes>>.
** Disable persistence for basic testing. This can be done by adding `--set
etcd.persistentVolume.enabled=false` to the `helm install` command below.

* Read <<s-kubernetes-storage, the storage guide>> and configure a basic storage setup for LINSTOR

* Read the <<s-kubernetes-securing-deployment,section on securing the deployment>> and configure as needed.

* Select the appropriate kernel module injector using `--set` with the `helm
install` command in the final step.

** Choose the injector according to the distribution you are using.
Select the latest version from one of `drbd9-rhel7`, `drbd9-rhel8`,...  from http://drbd.io/ as appropriate.
The drbd9-rhel8 image should also be used for RHCOS (OpenShift). For the SUSE CaaS Platform use the SLES injector
that matches the base system of the CaaS Platform you are using (e.g., `drbd9-sles15sp1`). For example:
+
----
operator.satelliteSet.kernelModuleInjectionImage=drbd.io/drbd9-rhel8:v9.0.24
----

** Only inject modules that are already present on the host machine. If a module is not found, it will be skipped.
+
----
operator.satelliteSet.kernelModuleInjectionMode=DepsOnly
----

** Disable kernel module injection if you are installing DRBD by other means. Deprecated by `DepsOnly`
+
----
operator.satelliteSet.kernelModuleInjectionMode=None
----

* Finally create a Helm deployment named `linstor-op` that will set up
everything.
+
----
helm repo add linstor https://charts.linstor.io
helm install linstor-op linstor/linstor
----
Further deployment customization is discussed in the <<s-kubernetes-advanced-deployments,advanced deployment section>>

[[s-kubernetes-etcd-hostpath-persistence]]
===== LINSTOR etcd `hostPath` persistence

You can use the `pv-hostpath` Helm templates to create `hostPath` persistent
volumes. Create as many PVs as needed to satisfy your configured etcd
`replicas` (default 1).

Create the `hostPath` persistent volumes, substituting cluster node
names accordingly in the `nodes=` option:

----
helm repo add linstor https://charts.linstor.io
helm install linstor-etcd linstor/pv-hostpath --set "nodes={<NODE0>,<NODE1>,<NODE2>}"
----

Persistence for etcd is enabled by default.

[[s-kubernetes-existing-database]]
===== Using an existing database

LINSTOR can connect to an existing PostgreSQL, MariaDB or etcd database. For
instance, for a PostgreSQL instance with the following configuration:

----
POSTGRES_DB: postgresdb
POSTGRES_USER: postgresadmin
POSTGRES_PASSWORD: admin123
----

The Helm chart can be configured to use this database instead of deploying an
etcd cluster by adding the following to the Helm install command:

----
--set etcd.enabled=false --set "operator.controller.dbConnectionURL=jdbc:postgresql://postgres/postgresdb?user=postgresadmin&password=admin123"
----

[[s-kubernetes-storage]]
==== Configuring storage

The LINSTOR operator can automate some basic storage set up for LINSTOR.

===== Configuring storage pool creation

The LINSTOR operator can be used to create LINSTOR storage pools. Creation is under control of the
LinstorSatelliteSet resource:

[source]
----
$ kubectl get LinstorSatelliteSet.linstor.linbit.com linstor-op-ns -o yaml
kind: LinstorSatelliteSet
metadata:
..
spec:
  ..
  storagePools:
    lvmPools:
    - name: lvm-thick
      volumeGroup: drbdpool
    lvmThinPools:
    - name: lvm-thin
      thinVolume: thinpool
      volumeGroup: drbdpool
    zfsPools:
    - name: my-linstor-zpool
      zPool: for-linstor
      thin: true
----

====== At install time

At install time, by setting the value of `operator.satelliteSet.storagePools` when running helm install.

First create a file with the storage configuration like:

[source,yaml]
----
operator:
  satelliteSet:
    storagePools:
      lvmPools:
      - name: lvm-thick
        volumeGroup: drbdpool
----

This file can be passed to the helm installation like this:

[source]
----
helm install -f <file> linstor/linstor
----

====== After install

On a cluster with the operator already configured (i.e. after `helm install`),
you can edit the LinstorSatelliteSet configuration like this:

[source]
----
$ kubectl edit LinstorSatelliteSet.linstor.linbit.com <satellitesetname>
----

The storage pool configuration can be updated like in the example above.

===== Preparing physical devices

By default, LINSTOR expects the referenced VolumeGroups, ThinPools and so on to be present. You can use the
`devicePaths: []` option to let LINSTOR automatically prepare devices for the pool. Eligible for automatic configuration
are block devices that:

* Are a root device (no partition)
* do not contain partition information
* have more than 1 GiB

To enable automatic configuration of devices, set the `devicePaths` key on `storagePools` entries:

[source,yaml]
----
  storagePools:
    lvmPools:
    - name: lvm-thick
      volumeGroup: drbdpool
      devicePaths:
      - /dev/vdb
    lvmThinPools:
    - name: lvm-thin
      thinVolume: thinpool
      volumeGroup: linstor_thinpool
      devicePaths:
      - /dev/vdc
      - /dev/vdd
----

Currently, this method supports creation of LVM and LVMTHIN storage pools.

===== `lvmPools` configuration
* `name` name of the LINSTOR storage pool.Required
* `volumeGroup` name of the VG to create.Required
* `devicePaths` devices to configure for this pool.Must be empty and >= 1GiB to be recognized.Optional
* `raidLevel` LVM raid level.Optional
* `vdo` Enable [VDO] (requires VDO tools in the satellite).Optional
* `vdoLogicalSizeKib` Size of the created VG (expected to be bigger than the backing devices by using VDO).Optional
* `vdoSlabSizeKib` Slab size for VDO. Optional

[VDO]: https://www.redhat.com/en/blog/look-vdo-new-linux-compression-layer

===== `lvmThinPools` configuration
* `name` name of the LINSTOR storage pool. Required
* `volumeGroup` VG to use for the thin pool. If you want to use `devicePaths`, you must set this to `""`.
This is required because LINSTOR does not allow configuration of the VG name when preparing devices.
* `thinVolume` name of the thinpool. Required
* `devicePaths` devices to configure for this pool. Must be empty and >= 1GiB to be recognized. Optional
* `raidLevel` LVM raid level. Optional

NOTE: The volume group created by LINSTOR for LVMTHIN pools will always follow the scheme "linstor_$THINPOOL".

===== `zfsPools` configuration
* `name` name of the LINSTOR storage pool. Required
* `zPool` name of the zpool to use. Must already be present on all machines. Required
* `thin` `true` to use thin provisioning, `false` otherwise. Required

===== Using `automaticStorageType` (DEPRECATED)

_ALL_ eligible devices will be prepared according to the value of `operator.satelliteSet.automaticStorageType`, unless
they are already prepared using the `storagePools` section. Devices are added to a storage pool based on the device
name (i.e. all `/dev/nvme1` devices will be part of the pool `autopool-nvme1`)

The possible values for `operator.satelliteSet.automaticStorageType`:

* `None` no automatic set up (default)
* `LVM` create a LVM (thick) storage pool
* `LVMTHIN` create a LVM thin storage pool
* `ZFS` create a ZFS based storage pool (**UNTESTED**)

[[s-kubernetes-securing-deployment]]
==== Securing deployment
This section describes the different options for enabling security features available when
using this operator. The following guides assume the operator is installed <<s-kubernetes-deploy-linstor-operator,using Helm>>

===== Secure communication with an existing etcd instance

Secure communication to an `etcd` instance can be enabled by providing a CA certificate to the operator in form of a
kubernetes secret. The secret has to contain the key `ca.pem` with the PEM encoded CA certificate as value.

The secret can then be passed to the controller by passing the following argument to `helm install`

----
--set operator.controller.dbCertSecret=<secret name>
----

====== Authentication with `etcd` using certificates

If you want to use TLS certificates to authenticate with an `etcd` database, you need to set the following option on
helm install:

----
--set operator.controller.dbUseClientCert=true
----

If this option is active, the secret specified in the above section must contain two additional keys:
* `client.cert` PEM formatted certificate presented to `etcd` for authentication
* `client.key` private key **in PKCS8 format**, matching the above client certificate.
Keys can be converted into PKCS8 format using `openssl`:

----
openssl pkcs8 -topk8 -nocrypt -in client-key.pem -out client-key.pkcs8
----

===== Configuring secure communication between LINSTOR components

The default communication between LINSTOR components is not secured by TLS. If this is needed for your setup,
follow these steps:

* Create private keys in the java keystore format, one for the controller, one for all satellites:
----
keytool -keyalg rsa -keysize 2048 -genkey -keystore satellite-keys.jks -storepass linstor -alias satellite -dname "CN=XX, OU=satellite, O=Example, L=XX, ST=XX, C=X"
keytool -keyalg rsa -keysize 2048 -genkey -keystore control-keys.jks -storepass linstor -alias control -dname "CN=XX, OU=control, O=Example, L=XX, ST=XX, C=XX"
----
* Create a trust store with the public keys that each component needs to trust:
* Controller needs to trust the satellites
* Nodes need to trust the controller
+
----
keytool -importkeystore -srcstorepass linstor -deststorepass linstor -srckeystore control-keys.jks -destkeystore satellite-trust.jks
keytool -importkeystore -srcstorepass linstor -deststorepass linstor -srckeystore satellite-keys.jks -destkeystore control-trust.jks
----

* Create kubernetes secrets that can be passed to the controller and satellite pods
+
----
kubectl create secret generic control-secret --from-file=keystore.jks=control-keys.jks --from-file=certificates.jks=control-trust.jks
kubectl create secret generic satellite-secret --from-file=keystore.jks=satellite-keys.jks --from-file=certificates.jks=satellite-trust.jks
----
* Pass the names of the created secrets to `helm install`
+
----
--set operator.satelliteSet.sslSecret=satellite-secret --set operator.controller.sslSecret=control-secret
----

IMPORTANT: It is currently **NOT** possible to change the keystore password. LINSTOR expects the passwords to be
`linstor`. This is a current limitation of LINSTOR.

===== Configuring secure communications for the LINSTOR API

Various components need to talk to the LINSTOR controller via its REST interface. This interface can be
secured via HTTPS, which automatically includes authentication. For HTTPS+authentication to work, each component
needs access to:

* A private key
* A certificate based on the key
* A trusted certificate, used to verify that other components are trustworthy

The next sections will guide you through creating all required components.

====== Creating the private keys

Private keys can be created using java's keytool

----
keytool -keyalg rsa -keysize 2048 -genkey -keystore controller.pkcs12 -storetype pkcs12 -storepass linstor -ext san=dns:linstor-op-cs.default.svc -dname "CN=XX, OU=controller, O=Example, L=XX, ST=XX, C=X" -validity 5000
keytool -keyalg rsa -keysize 2048 -genkey -keystore client.pkcs12 -storetype pkcs12 -storepass linstor -dname "CN=XX, OU=client, O=Example, L=XX, ST=XX, C=XX" -validity 5000
----

The clients need private keys and certificate in a different format, so we need to convert it

----
openssl pkcs12 -in client.pkcs12 -passin pass:linstor -out client.cert -clcerts -nokeys
openssl pkcs12 -in client.pkcs12 -passin pass:linstor -out client.key -nocerts -nodes
----

NOTE: The alias specified for the controller key (i.e. `-ext san=dns:linstor-op-cs.default.svc`) has to exactly match the
service name created by the operator. When using `helm`, this is always of the form `<release-name>-cs.<release-namespace>.svc`.

IMPORTANT: It is currently NOT possible to change the keystore password. LINSTOR expects the passwords to be linstor. This is a current limitation of LINSTOR

====== Create the trusted certificates

For the controller to trust the clients, we can use the following command to create a truststore, importing the client certificate

----
keytool -importkeystore -srcstorepass linstor -srckeystore client.pkcs12 -deststorepass linstor -deststoretype pkcs12 -destkeystore controller-trust.pkcs12
----

For the client, we have to convert the controller certificate into a different format

----
openssl pkcs12 -in controller.pkcs12 -passin pass:linstor -out ca.pem -clcerts -nokeys
----

====== Create Kubernetes secrets

Now you can create secrets for the controller and for clients:

----
kubectl create secret generic http-controller --from-file=keystore.jks=controller.pkcs12 --from-file=truststore.jks=controller-trust.pkcs12
kubectl create secret generic http-client --from-file=ca.pem=ca.pem --from-file=client.cert=client.cert --from-file=client.key=client.key
----

The names of the secrets can be passed to `helm install` to configure all clients to use https.

----
--set linstorHttpsControllerSecret=http-controller  --set linstorHttpsClientSecret=http-client
----

===== Automatically set the passphrase for encrypted volumes

Linstor can be used to create encrypted volumes using LUKS. The passphrase used when creating these volumes can
be set via a secret:

----
kubectl create secret generic linstor-pass --from-literal=MASTER_PASSPHRASE=<password>
----

On install, add the following arguments to the helm command:

----
--set operator.controller.luksSecret=linstor-pass
----

[[s-kubernetes-helm-terminate]]
===== Terminating Helm deployment

To protect the storage infrastructure of the cluster from accidentally deleting vital components, it is necessary to perform some manual steps before deleting a Helm deployment.

1. Delete all volume claims managed by LINSTOR components. You can use the following command to get a list of volume claims managed by LINSTOR. After checking that none of the listed volumes still hold needed data, you can delete them using the generated kubectl delete command.
+
----
$ kubectl get pvc --all-namespaces -o=jsonpath='{range .items[?(@.metadata.annotations.volume\.beta\.kubernetes\.io/storage-provisioner=="linstor.csi.linbit.com")]}kubectl delete pvc --namespace {.metadata.namespace} {.metadata.name}{"\n"}{end}'
kubectl delete pvc --namespace default data-mysql-0
kubectl delete pvc --namespace default data-mysql-1
kubectl delete pvc --namespace default data-mysql-2
----
+
WARNING: These volumes, once deleted, cannot be recovered.

2. Delete the LINSTOR controller and satellite resources.
+
Deployment of LINSTOR satellite and controller is controlled by the LinstorSatelliteSet and LinstorController resources. You can delete the resources associated with your deployment using kubectl
+
----
kubectl delete linstorcontroller <helm-deploy-name>-cs
kubectl delete linstorsatelliteset <helm-deploy-name>-ns
----
+
After a short wait, the controller and satellite pods should terminate. If they continue to run, you can check the above resources for errors (they are only removed after all associated pods terminate)

3. Delete the Helm deployment.
+
If you removed all PVCs and all LINSTOR pods have terminated, you can uninstall the helm deployment
+
----
helm uninstall linstor-op
----
+
NOTE: Due to the Helm's current policy, the Custom Resource Definitions named LinstorController and LinstorSatelliteSet will not be deleted by the command.
More information regarding Helm's current position on CRD's can be found
https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#method-1-let-helm-do-it-for-you[here].

[[s-kubernetes-upgrades]]
==== Upgrades from older deployments

If you are already running a previous version (before v1.0.0) of `linstor-operator`, you
need to follow the https://github.com/piraeusdatastore/piraeus-operator/blob/master/UPGRADE.md#upgrade-to-v1.0[upstream guide].
We tried to stay compatible and lower the burden of upgrading as much as possible, but some
manual steps may still be required.

[[s-kubernetes-advanced-deployments]]
==== Advanced deployment options

The helm charts provide a set of further customization options for advanced use cases.

[source,yaml]
----
global:
  imagePullPolicy: IfNotPresent # empty pull policy means k8s default is used ("always" if tag == ":latest", "ifnotpresent" else) <1>
# Dependency charts
etcd:
  persistentVolume:
    enabled: true
    storage: 1Gi
  replicas: 1 # How many instances of etcd will be added to the initial cluster. <2>
  resources: {} # resource requirements for etcd containers <3>
  image:
    repository: gcr.io/etcd-development/etcd
    tag: v3.4.9
csi-snapshotter:
  enabled: true # <- enable to add k8s snapshotting CRDs and controller. Needed for CSI snapshotting
  image: quay.io/k8scsi/snapshot-controller:v2.1.0
  replicas: 1 <2>
  resources: {} # resource requirements for the cluster snapshot controller. <3>
stork:
  enabled: true
  storkImage: docker.io/linbit/stork:latest
  schedulerImage: gcr.io/google_containers/kube-scheduler-amd64
  replicas: 1 <2>
  storkResources: {} # resources requirements for the stork plugin containers <3>
  schedulerResources: {} # resource requirements for the kube-scheduler containers <3>
csi:
  enabled: true
  pluginImage: "drbd.io/linstor-csi:v0.9.1"
  csiAttacherImage: quay.io/k8scsi/csi-attacher:v2.2.0
  csiNodeDriverRegistrarImage: quay.io/k8scsi/csi-node-driver-registrar:v1.3.0
  csiProvisionerImage: quay.io/k8scsi/csi-provisioner:v1.6.0
  csiSnapshotterImage: quay.io/k8scsi/csi-snapshotter:v2.1.0
  csiResizerImage: quay.io/k8scsi/csi-resizer:v0.5.0
  controllerReplicas: 1 <2>
  nodeAffinity: {} <4>
  nodeTolerations: [] <4>
  controllerAffinity: {} <4>
  controllerTolerations: [] <4>
  resources: {} <3>
priorityClassName: ""
drbdRepoCred: drbdiocred # <- Specify the kubernetes secret name here
linstorHttpsControllerSecret: "" # <- name of secret containing linstor server certificates+key.
linstorHttpsClientSecret: "" # <- name of secret containing linstor client certificates+key.
controllerEndpoint: "" # <- override to the generated controller endpoint. use if controller is not deployed via operator
operator:
  replicas: 1 # <- number of replicas for the operator deployment <2>
  image: "drbd.io/linstor-operator:v1.0.0-rc1"
  resources: {} <3>
  controller:
    controllerImage: "drbd.io/linstor-controller:v1.7.3"
    luksSecret: ""
    dbCertSecret: ""
    dbUseClientCert: false
    sslSecret: ""
    affinity: {} <4>
    tolerations: [] <4>
    resources: {} <3>
  satelliteSet:
    satelliteImage: "drbd.io/linstor-satellite:v1.7.3"
    storagePools: null
    sslSecret: ""
    automaticStorageType: None
    affinity: {} <4>
    tolerations: [] <4>
    resources: {} <3>
    kernelModuleInjectionImage: "drbd.io/drbd9-rhel7:v9.0.24"
    kernelModuleInjectionMode: ShippedModules
    kernelModuleInjectionResources: {} <3>
----
<1> Sets the pull policy for all images.
<2> Controls the number of replicas for each component.
<3> Set container resource requests and limits. See https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/[the kubernetes docs].
Most containers need a minimal amount of resources, except for:
* `etcd.resources` See the https://etcd.io/docs/v3.4.0/op-guide/hardware/[etcd docs]
* `operator.controller.resources` Around `700MiB` memory is required
* `operater.satelliteSet.resources` Around `700MiB` memory is required
* `operator.satelliteSet.kernelModuleInjectionResources` If kernel modules are compiled, 1GiB of memory is required.
<4> Affinity and toleration determine where pods are scheduled on the cluster. See the
https://kubernetes.io/docs/concepts/scheduling-eviction/[kubernetes docs on affinity and toleration].
This may be especially important for the `operator.satelliteSet` and `csi.node*` values. To schedule a pod using
a LINSTOR persistent volume, the node requires a running LINSTOR satellite and LINSTOR CSI pod.


[[s-kubernetes-deploy-external-controller]]
==== Deploying with an external LINSTOR controller

The operator can configure the satellites and CSI plugin to use an existing LINSTOR setup. This can be useful in cases
where the storage infrastructure is separate from the Kubernetes cluster. Volumes can be provisioned in diskless mode
on the Kubernetes nodes while the storage nodes will provide the backing disk storage.

To skip the creation of a LINSTOR Controller deployment and configure the other components to use your existing LINSTOR
Controller, use the following options when running `helm install`:

* `operator.controller.enabled=false` This disables creation of the `LinstorController` resource
* `operator.etcd.enabled=false` Since no LINSTOR Controller will run on Kubernetes, no database is required.
* `controllerEndpoint=<url-of-linstor-controller>` The HTTP endpoint of the existing LINSTOR Controller. For example: `http://linstor.storage.cluster:3370/`

After all pods are ready, you should see the Kubernetes cluster nodes as satellites in your LINSTOR setup.

IMPORTANT: Your kubernetes nodes must be reachable using their IP by the controller and storage nodes.

Create a storage class referencing an existing storage pool on your storage nodes.

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: linstor-on-k8s
provisioner: linstor.csi.linbit.com
parameters:
  autoPlace: "3"
  storagePool: existing-storage-pool
  resourceGroup: linstor-on-k8s
----

You can provision new volumes by creating PVCs using your storage class. The volumes will first be placed only on nodes
with the given storage pool, i.e. your storage infrastructure. Once you want to use the volume in a pod, LINSTOR CSI
will create a diskless resource on the Kubernetes node and attach over the network to the diskfull resource.

[[s-kubernetes-deploy-piraeus-operator]]
==== Deploying with the Piraeus Operator

The community supported edition of the LINSTOR deployment in Kubernetes is
called Piraeus. The Piraeus project provides
https://github.com/piraeusdatastore/piraeus-operator[an operator] for
deployment.

[[s-kubernetes-linstor-interacting]]
=== Interacting with LINSTOR in Kubernetes

The Controller pod includes a LINSTOR Client, making it easy to interact directly with LINSTOR.
For instance:

----
kubectl exec linstor-op-cs-controller-<deployment-info> -- linstor storage-pool list
----

This should only be necessary for investigating problems and accessing advanced functionality.
Regular operation such as creating volumes should be achieved via the
<<s-kubernetes-basic-configuration-and-deployment,Kubernetes integration>>.

[[s-kubernetes-linstor-csi-plugin-deployment]]
=== LINSTOR CSI Plugin Deployment

The operator Helm chart deploys the LINSTOR CSI plugin for you so if you used
that, you can skip this section.

If you are integrating LINSTOR using a different method, you will need to install the LINSTOR CSI plugin.
Instructions for deploying the CSI plugin can be found on the
https://github.com/LINBIT/linstor-csi[project's github]. This will result in a
linstor-csi-controller _Deployment_ and a linstor-csi-node _DaemonSet_ running in the
kube-system namespace.

----
NAME                           READY   STATUS    RESTARTS   AGE     IP              NODE
linstor-csi-controller-ab789   5/5     Running   0          3h10m   191.168.1.200   kubelet-a
linstor-csi-node-4fcnn         2/2     Running   0          3h10m   192.168.1.202   kubelet-c
linstor-csi-node-f2dr7         2/2     Running   0          3h10m   192.168.1.203   kubelet-d
linstor-csi-node-j66bc         2/2     Running   0          3h10m   192.168.1.201   kubelet-b
linstor-csi-node-qb7fw         2/2     Running   0          3h10m   192.168.1.200   kubelet-a
linstor-csi-node-zr75z         2/2     Running   0          3h10m   192.168.1.204   kubelet-e
----

[[s-kubernetes-basic-configuration-and-deployment]]
=== Basic Configuration and Deployment

Once all linstor-csi __Pod__s are up and running, we can provision volumes
using the usual Kubernetes workflows.

Configuring the behavior and properties of LINSTOR volumes deployed via Kubernetes
is accomplished via the use of __StorageClass__es.

IMPORTANT: the "resourceGroup" parameter is mandatory. Usually you want it to be unique and the same as the storage class name.

Here below is the simplest practical _StorageClass_ that can be used to deploy volumes:

.linstor-basic-sc.yaml
[source,yaml]
----
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  # The name used to identify this StorageClass.
  name: linstor-basic-storage-class
  # The name used to match this StorageClass with a provisioner.
  # linstor.csi.linbit.com is the name that the LINSTOR CSI plugin uses to identify itself
provisioner: linstor.csi.linbit.com
parameters:
  # LINSTOR will provision volumes from the drbdpool storage pool configured
  # On the satellite nodes in the LINSTOR cluster specified in the plugin's deployment
  storagePool: "drbdpool"
  resourceGroup: "linstor-basic-storage-class"
  # Setting a fstype is required for "fsGroup" permissions to work correctly.
  # Currently supported: xfs/ext4
  csi.storage.k8s.io/fstype: xfs
----

DRBD options can be set as well in the parameters section. Valid keys are defined in the
https://app.swaggerhub.com/apis-docs/Linstor/Linstor[LINSTOR REST-API]
(e.g., `DrbdOptions/Net/allow-two-primaries: "yes"`).

We can create the _StorageClass_ with the following command:

----
kubectl create -f linstor-basic-sc.yaml
----

Now that our _StorageClass_ is created, we can now create a _PersistentVolumeClaim_
which can be used to provision volumes known both to Kubernetes and LINSTOR:

.my-first-linstor-volume-pvc.yaml
[source,yaml]
----
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-first-linstor-volume
spec:
  storageClassName: linstor-basic-storage-class
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
----

We can create the _PersistentVolumeClaim_ with the following command:

----
kubectl create -f my-first-linstor-volume-pvc.yaml
----

This will create a _PersistentVolumeClaim_ known to Kubernetes, which will have
a _PersistentVolume_ bound to it, additionally LINSTOR will now create this
volume according to the configuration defined in the `linstor-basic-storage-class`
_StorageClass_. The LINSTOR volume's name will be a UUID prefixed with `csi-`
This volume can be observed with the usual `linstor resource list`. Once that
volume is created, we can attach it to a pod. The following _Pod_ spec will spawn
a Fedora container with our volume attached that busy waits so it is not
unscheduled before we can interact with it:

.my-first-linstor-volume-pod.yaml
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: fedora
  namespace: default
spec:
  containers:
  - name: fedora
    image: fedora
    command: [/bin/bash]
    args: ["-c", "while true; do sleep 10; done"]
    volumeMounts:
    - name: my-first-linstor-volume
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: my-first-linstor-volume
    persistentVolumeClaim:
      claimName: "my-first-linstor-volume"
----

We can create the _Pod_ with the following command:

----
kubectl create -f my-first-linstor-volume-pod.yaml
----

Running `kubectl describe pod fedora` can be used to confirm that _Pod_
scheduling and volume attachment succeeded.

To remove a volume, please ensure that no pod is using it and then delete the
_PersistentVolumeClaim_ via `kubectl`. For example, to remove the volume that we
just made, run the following two commands, noting that the _Pod_ must be
unscheduled before the _PersistentVolumeClaim_ will be removed:

----
kubectl delete pod fedora # unschedule the pod.

kubectl get pod -w # wait for pod to be unscheduled

kubectl delete pvc my-first-linstor-volume # remove the PersistentVolumeClaim, the PersistentVolume, and the LINSTOR Volume.
----

[[s-kubernetes-snapshots]]
=== Snapshots

Creating <<s-linstor-snapshots, snapshots>> and creating new volumes from
snapshots is done via the use of __VolumeSnapshot__s, __VolumeSnapshotClass__es,
and __PVC__s.

[[s-kubernetes-add-snaphot-support]]
==== Adding snapshot support

LINSTOR supports the volume snapshot feature, which is currently in beta. To use it, you need to install a cluster wide
snapshot controller. This is done either by the cluster provider, or you can use the LINSTOR chart.

By default, the LINSTOR chart will install its own snapshot controller. This can lead to conflict in some cases:

* the cluster already has a snapshot controller
* the cluster does not meet the minimal version requirements (>= 1.17)

In such a case, installation of the snapshot controller can be disabled:

----
--set csi-snapshotter.enabled=false
----

[[s-kubernetes-use-snapshot]]
==== Using volume snapshots
Then we can create our _VolumeSnapshotClass_:

.my-first-linstor-snapshot-class.yaml
[source,yaml]
----
apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshotClass
metadata:
  name: my-first-linstor-snapshot-class
driver: linstor.csi.linbit.com
deletionPolicy: Delete
----

Create the _VolumeSnapshotClass_ with `kubectl`:

----
kubectl create -f my-first-linstor-snapshot-class.yaml
----

Now we will create a volume snapshot for the volume that we created above. This
is done with a _VolumeSnapshot_:

.my-first-linstor-snapshot.yaml
[source,yaml]
----
apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshot
metadata:
  name: my-first-linstor-snapshot
spec:
  volumeSnapshotClassName: my-first-linstor-snapshot-class
  source:
    persistentVolumeClaimName: my-first-linstor-volume
----

Create the _VolumeSnapshot_ with `kubectl`:

----
kubectl create -f my-first-linstor-snapshot.yaml
----

You can check that the snapshot creation was successful

----
kubectl describe volumesnapshots.snapshot.storage.k8s.io my-first-linstor-snapshot
...
Spec:
  Source:
    Persistent Volume Claim Name:  my-first-linstor-snapshot
  Volume Snapshot Class Name:      my-first-linstor-snapshot-class
Status:
  Bound Volume Snapshot Content Name:  snapcontent-b6072ab7-6ddf-482b-a4e3-693088136d2c
  Creation Time:                       2020-06-04T13:02:28Z
  Ready To Use:                        true
  Restore Size:                        500Mi
----

Finally, we'll create a new volume from the snapshot with a _PVC_.

.my-first-linstor-volume-from-snapshot.yaml
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-first-linstor-volume-from-snapshot
spec:
  storageClassName: linstor-basic-storage-class
  dataSource:
    name: my-first-linstor-snapshot
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
----

Create the _PVC_ with `kubectl`:

----
kubectl create -f my-first-linstor-volume-from-snapshot.yaml
----


[[s-kubernetes-volume-accessibility]]
=== Volume Accessibility
// This only covers DRBD volumes, section might change if linked docs are updated.
LINSTOR volumes are typically accessible both locally and
<<s-drbd_clients,over the network>>.

By default, the CSI plugin will attach volumes directly if the _Pod_ happens
to be scheduled on a _kubelet_ where its underlying storage is present. However,
_Pod_ scheduling does not currently take volume locality into account. The
<<s-kubernetes-replicasonsame,replicasOnSame>> parameter can be used to restrict
where the underlying storage may be provisioned, if locally attached volumes
are desired.

See <<s-kubernetes-localstoragepolicy,localStoragePolicy>> to see how this
default behavior can be modified.

[[s-kubernetes-stork]]
=== Volume Locality Optimization using Stork

Stork is a scheduler extender plugin for Kubernetes which allows a storage
driver to give the Kubernetes scheduler hints about where to place a new pod
so that it is optimally located for storage performance. You can learn more
about the project on its https://portworx.com/stork-storage-orchestration-kubernetes/[GitHub page].

The next Stork release will include the LINSTOR driver by default.
In the meantime, you can use a custom-built Stork container by LINBIT which includes a LINSTOR driver,
https://hub.docker.com/repository/docker/linbit/stork[available on Docker Hub]

[[s-kubernetes-using-stork]]
==== Using Stork

By default, the operator will install the components required for Stork, and register a new scheduler called `stork`
with Kubernetes. This new scheduler can be used to place pods near to their volumes.

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  schedulerName: stork <1>
  containers:
  - name: busybox
    image: busybox
    command: ["tail", "-f", "/dev/null"]
    volumeMounts:
    - name: my-first-linstor-volume
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: my-first-linstor-volume
    persistentVolumeClaim:
      claimName: "test-volume"
----

<1> Add the name of the scheduler to your pod.

Deployment of the scheduler can be disabled using

----
--set stork.enabled=false
----

[[s-kubernetes-advanced-configuration]]
=== Advanced Configuration

In general, all configuration for LINSTOR volumes in Kubernetes should be done
via the _StorageClass_ parameters, as seen with the _storagePool_ in the basic
example above. We'll give all the available options an in-depth treatment here.

[[s-kubernetes-nodelist]]
==== nodeList

`nodeList` is a list of nodes for volumes to be assigned to. This will assign
the volume to each node and it will be replicated among all of them. This
can also be used to select a single node by hostname, but it's more flexible to use
<<s-kubernetes-replicasonsame,replicasOnSame>> to select a single node.

IMPORTANT: If you use this option, you must not use <<s-kubernetes-autoplace,autoPlace>>.

TIP: This option determines on which LINSTOR nodes the underlying storage for volumes
will be provisioned and is orthogonal from which _kubelets_ these volumes will be
accessible.

Example: `nodeList: "node-a node-b node-c"`

Example: `nodeList: "node-a"`

[[s-kubernetes-autoplace]]
==== autoPlace

`autoPlace` is an integer that determines the amount of replicas a volume of
this _StorageClass_ will have.  For instance, `autoPlace: "3"` will produce
volumes with three-way replication. If neither `autoPlace` nor `nodeList` are
set, volumes will be <<s-autoplace-linstor,automatically placed>> on one node.

IMPORTANT: If you use this option, you must not use <<s-kubernetes-nodelist,nodeList>>.

IMPORTANT: You have to use quotes, otherwise Kubernetes will complain about a malformed _StorageClass_.

TIP: This option (and all options which affect autoplacement behavior) modifies the
number of LINSTOR nodes on which the underlying storage for volumes will be
provisioned and is orthogonal to which _kubelets_ those volumes will be accessible
from.

Example: `autoPlace: "2"`

Default: `autoPlace: "1"`

[[s-kubernetes-replicasonsame]]
==== replicasOnSame

// These should link to the linstor documentation about node properties, but those
// do not exist at the time of this commit.
`replicasOnSame` is a list of `key` or `key=value` items used as autoplacement selection
labels when <<s-kubernetes-autoplace,autoplace>> is used to determine where to
provision storage. These labels correspond to LINSTOR node properties.

IMPORTANT: LINSTOR node properties  are different from kubernetes node labels. You can see the properties of a node by
running `linstor node list-properties <nodename>`. You can also set additional properties ("auxiliary properties"): `linstor node set-property <nodename> --aux <key> <value>`.

Let's explore this behavior with examples assuming a LINSTOR cluster such that `node-a` is configured with the
following auxiliary property `zone=z1` and `role=backups`, while `node-b` is configured with
only `zone=z1`.

If we configure a _StorageClass_ with `autoPlace: "1"` and `replicasOnSame: "zone=z1 role=backups"`,
then all volumes created from that _StorageClass_ will be provisioned on `node-a`,
since that is the only node with all of the correct key=value pairs in the LINSTOR
cluster. This is the most flexible way to select a single node for provisioning.

IMPORTANT: This guide assumes LINSTOR CSI version 0.10.0 or newer. All properties referenced in `replicasOnSame`
and `replicasOnDifferent` are interpreted as auxiliary properties. If you are using an older version of LINSTOR CSI, you
need to add the `Aux/` prefix to all property names. So `replicasOnSame: "zone=z1"` would be `replicasOnSame: "Aux/zone=z1"`
Using `Aux/` manually will continue to work on newer LINSTOR CSI versions.

If we configure a _StorageClass_ with `autoPlace: "1"` and `replicasOnSame: "zone=z1"`,
then volumes will be provisioned on either `node-a` or `node-b` as they both have
the `zone=z1` aux prop.

If we configure a _StorageClass_ with `autoPlace: "2"` and `replicasOnSame: "zone=z1 role=backups"`,
then provisioning will fail, as there are not two or more nodes that have
the appropriate auxiliary properties.

If we configure a _StorageClass_ with `autoPlace: "2"` and `replicasOnSame: "zone=z1"`,
then volumes will be provisioned on both `node-a` and `node-b` as they both have
the `zone=z1` aux prop.

You can also use a property key without providing a value to ensure all replicas are placed on nodes with the same property value,
with caring about the particular value. Assuming there are 4 nodes, `node-a1` and `node-a2` are configured with `zone=a`. `node-b1` and `node-b2`
are configured with `zone=b`. Using `autoPlace: "2"` and `replicasOnSame: "zone"` will place on either `node-a1` and `node-a2` OR on `node-b1` and `node-b2`.

[[s-kubernetes-replicasondifferent]]
==== replicasOnDifferent

`replicasOnDifferent` takes a list of properties to consider, same as <<s-kubernetes-replicasonsame,replicasOnSame>>.
There are two modes of using `replicasOnDifferent`:

* Preventing volume placement on specific nodes:
+
If a value is given for the property, the nodes which have that property-value pair assigned will be considered last.
+
Example: `replicasOnDifferent: "no-csi-volumes=true"` will place no volume on any node with property
`no-csi-volumes=true` unless there are not enough other nodes to fulfill the `autoPlace` setting.

* Distribute volumes across nodes with different values for the same key:
+
If no property value is given, LINSTOR will place the volumes across nodes with different values for that property if
possible.
+
Example: Assuming there are 4 nodes, `node-a1` and `node-a2` are configured with `zone=a`. `node-b1` and `node-b2`
are configured with `zone=b`. Using a _StorageClass_ with `autoPlace: "2"` and `replicasOnDifferent: "zone"`,
LINSTOR will create one replica on either `node-a1` or `node-a2` _and_ one replica on either `node-b1` or `node-b2`.

[[s-kubernetes-localstoragepolicy]]
==== localStoragePolicy

`localStoragePolicy` determines, via volume topology, which LINSTOR
__Satellite__s volumes should be assigned and from where Kubernetes will
access volumes. The behavior of each option is explained below in detail.

IMPORTANT: If you specify a <<s-kubernetes-nodelist,nodeList>>, volumes will
be created on those nodes, irrespective of the `localStoragePolicy`; however,
the accessibility reporting will still be as described.

IMPORTANT: You must set `volumeBindingMode: WaitForFirstConsumer` in the
_StorageClass_ and the LINSTOR __Satellite__s running on the __kubelet__s must
be able to support the diskfull placement of volumes as they are configured in
that _StorageClass_ for <<s-kubernetes-localstoragepolicy-required,required>>
or <<s-kubernetes-localstoragepolicy-preferred,preferred>> to work properly.

TIP: Use `topologyKey: "linbit.com/hostname"` rather than `topologyKey:
"kubernetes.io/hostname"` if you are setting `affinity` in your _Pod_ or
_StatefulSet_ specs.

Example: `localStoragePolicy: required`

[[s-kubernetes-localstoragepolicy-ignore]]
===== ignore (default)

When `localStoragePolicy` is set to `ignore`, regular autoplacement
occurs based on <<s-kubernetes-autoplace,autoplace>>,
<<s-kubernetes-replicasonsame,replicasOnSame>>, and
<<s-kubernetes-replicasonsame,replicasOnDifferent>>. Volume location will not
affect _Pod_ scheduling in Kubernetes and the volumes will be accessed over
the network if they're not local to the _kubelet_ where the _Pod_ was scheduled.

[[s-kubernetes-localstoragepolicy-required]]
===== required

When `localStoragePolicy` is set to `required`, Kubernetes will report a list
of places that it wants to schedule a _Pod_ in order of preference. The plugin
will attempt to provision the volume(s) according to that preference. The
number of volumes to be provisioned in total is based off of
<<s-kubernetes-autoplace,autoplace>>.

If all preferences have been attempted, but no volumes where successfully
assigned volume creation will fail.

In case of multiple replicas when all preferences have been attempted, and at
least one has succeeded, but there are still replicas remaining to be
provisioned, <<s-kubernetes-autoplace,autoplace>> behavior will apply for the
remaining volumes.

With this option set, Kubernetes will consider volumes that are not locally
present on a _kubelet_ to be unaccessible from that _kubelet_.

[[s-kubernetes-localstoragepolicy-preferred]]
===== preferred

When `localStoragePolicy` is set to `preferred`, volume placement behavior
will be the same as when it's set to
<<s-kubernetes-localstoragepolicy-required,required>> with the exception that
volume creation will not fail if no preference was able to be satisfied.
Volume accessibility will be the same as when set to
<<s-kubernetes-localstoragepolicy-ignore,ignore>>.

[[s-kubernetes-storagepool]]
==== storagePool

`storagePool` is the name of the LINSTOR <<s-storage_pools,storage pool>> that
will be used to provide storage to the newly-created volumes.

CAUTION: Only nodes configured with this same _storage pool_ with be considered
for <<s-kubernetes-autoplace,autoplacement>>. Likewise, for _StorageClasses_ using
<<s-kubernetes-nodelist,nodeList>> all nodes specified in that list must have this
_storage pool_ configured on them.

Example: `storagePool: my-storage-pool`

[[s-kubernetes-disklessstoragepool]]
==== disklessStoragePool

// This should link to the linstor section talking about diskless storage pools
// when that gets written.
`disklessStoragePool` is an optional parameter that only effects LINSTOR volumes
assigned disklessly to _kubelets_ i.e., as clients. If you have a custom
_diskless storage pool_ defined in LINSTOR, you'll specify that here.

Example: `disklessStoragePool: my-custom-diskless-pool`

[[s-kubernetes-encryption]]
==== encryption

`encryption` is an optional parameter that determines whether to encrypt
volumes. LINSTOR must be <<s-linstor-encrypted-Volumes,configured for encryption>>
for this to work properly.

Example: `encryption: "true"`

[[s-kubernetes-filesystem]]
==== filesystem

`filesystem` is an option parameter to specify the filesystem for non raw block
volumes. Currently supported options are `xfs` and `ext4`.

Example: `filesystem: "xfs"`

Default: `filesystem: "ext4"`

[[s-kubernetes-fsops]]
==== fsOpts
`fsOpts` is an optional parameter that passes options to the volume's
filesystem at creation time.

IMPORTANT: Please note these values are specific to your chosen
<<s-kubernetes-filesystem, filesystem>>.

Example: `fsOpts: "-b 2048"`

[[s-kubernetes-mountops]]
==== mountOpts
`mountOpts` is an optional parameter that passes options to the volume's
filesystem at mount time.

Example: `mountOpts: "sync,noatime"`
