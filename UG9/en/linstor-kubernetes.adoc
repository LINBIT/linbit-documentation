[[ch-kubernetes]]
== LINSTOR Volumes in Kubernetes

indexterm:[Kubernetes]This chapter describes the usage of LINSTOR(R) in Kubernetes (K8s)
as managed by the operator and with volumes provisioned using the
https://github.com/LINBIT/linstor-csi[LINSTOR CSI plugin].

This chapter goes into great detail regarding all the install time
options and various configurations possible with LINSTOR and
Kubernetes. The chapter begins with some explanatory remarks and then moves onto deployment instructions. After that, there are instructions for getting started with LINSTOR to configure storage within a Kubernetes deployment. Following that, more advanced topics and configurations, such as snapshots and monitoring, are covered.

[[s-kubernetes-overview]]
=== Kubernetes Introduction

_Kubernetes_ is a container orchestrator. Kubernetes defines the behavior of
containers and related services, using declarative specifications. In this guide,
we will focus on using `kubectl` to manipulate YAML files that define the
specifications of Kubernetes objects.

[[s-kubernetes-deploy]]
=== Deploying LINSTOR on Kubernetes

LINBIT(R) provides a LINSTOR Operator to commercial support customers.
The Operator eases deployment of LINSTOR on Kubernetes by installing DRBD(R),
managing satellite and controller pods, and other related functions.

IMPORTANT: LINBIT's container image repository (https://drbd.io), used by LINSTOR Operator,
is only available to LINBIT customers or through LINBIT customer trial accounts.
link:https://linbit.com/contact-us/[Contact LINBIT for information on pricing or to begin a
trial]. Alternatively, you can use the LINSTOR SDS upstream project named
link:https://github.com/piraeusdatastore/piraeus-operator[Piraeus], without being a LINBIT
customer.

LINSTOR Operator v2 is the recommended way of deploying LINBIT SDS for Kubernetes on new clusters.
Users of existing Operator v1 deployments should continue to use their Helm deployments and skip to the,
<<s-kubernetes-deploy-linstor-operator-v1,Operator v1 deployment instructions>>.

[[s-kubernetes-deploy-linstor-operator-v2]]
=== Deploying LINSTOR Operator v2

You can deploy the LINSTOR Operator v2 by using either
link:https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization[the Kustomize tool],
integrated with `kubectl`, or else by using link:https://helm.sh/[Helm] and a LINBIT Helm chart.

TIP: If you have already deployed LINSTOR Operator v1 into your cluster, you can upgrade your
LINBIT SDS deployment in Kubernetes to Operator v2 by following
link:https://charts.linstor.io/migration/[the instructions at charts.linstor.io].

[[s-kubernetes-creating-operator-v2-kustomize]]
==== Creating the Operator v2 by Using Kustomize

To deploy the Operator, create a `kustomization.yaml` file. This will declare your pull secret for `drbd.io` and
allow you to pull in the Operator deployment. The Operator will be deployed in a new namespace `linbit-sds`.
Make sure to replace `MY_LINBIT_USER` and `MY_LINBIT_PASSWORD` with your own credentials. You can find the latest
releases on link:https://charts.linstor.io/[charts.linstor.io].

.kustomization.yaml
[source,yaml]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: linbit-sds
resources:
  - https://charts.linstor.io/static/v2.4.0.yaml # <1>
generatorOptions:
  disableNameSuffixHash: true
secretGenerator:
  - name: drbdio-pull-secret
    type: kubernetes.io/dockerconfigjson
    literals:
      - .dockerconfigjson={"auths":{"drbd.io":{"username":"MY_LINBIT_USER","password":"MY_LINBIT_PASSWORD"}}} # <2>
----
<1> Replace with the latest release manifest from link:https://charts.linstor.io/[charts.linstor.io].
<2> Replace `MY_LINBIT_USER` and `MY_LINBIT_PASSWORD` with your link:https://my.linbit.com/[my.linbit.com] credentials.

Then, apply the `kustomization.yaml` file, by using `kubectl` command, and wait for the Operator to start:

----
$ kubectl apply -k .
namespace/linbit-sds created
...
$ kubectl -n linbit-sds  wait pod --for=condition=Ready --all
pod/linstor-operator-controller-manager-6d9847d857-zc985 condition met
----

The Operator is now ready to deploy LINBIT SDS for Kubernetes.

[[s-kubernetes-deploy-linbit-sds-for-k8s-operator-v2-kubectl]]
==== Deploying LINBIT SDS for Kubernetes by Using the Command Line Tool

Deploying LINBIT SDS for Kubernetes with the Operator v2 is as simple as creating a new `LinstorCluster` resource and
waiting for the Operator to complete the setup:

----
$ kubectl create -f - <<EOF
apiVersion: piraeus.io/v1
kind: LinstorCluster
metadata:
  name: linstorcluster
spec: {}
EOF
$ kubectl wait pod --for=condition=Ready -n linbit-sds --timeout=3m --all
----

Output should eventually show that the wait-for condition has been met and the LINBIT SDS pods are up and running.

----
pod/ha-controller-4tgcg condition met
pod/k8s-1-26-10.test condition met
pod/linstor-controller-76459dc6b6-tst8p condition met
pod/linstor-csi-controller-75dfdc967d-dwdx6 condition met
pod/linstor-csi-node-9gcwj condition met
pod/linstor-operator-controller-manager-6d9847d857-zc985 condition met
----

[[s-kubernetes-creating-operator-helm]]
==== Creating the Operator v2 by Using Helm

To create the LINSTOR Operator v2 by Using Helm, first enter the following commands to add the
`linstor` Helm chart repository:

----
$ MY_LINBIT_USER=<my-linbit-customer-username>
$ MY_LINBIT_PASSWORD=<my-linbit-customer-password>
$ helm repo add linstor https://charts.linstor.io
----

Next, enter the following command to install the LINSTOR Operator v2:

----
$ helm install linstor-operator linstor/linstor-operator \
  --namespace linbit-sds \
  --create-namespace \
  --set imageCredentials.username=$MY_LINBIT_USER \
  --set imageCredentials.password=$MY_LINBIT_PASSWORD \
  --wait
----

[[s-kubernetes-deploy-linbit-sds-for-k8s-operator-v2-helm]]
==== Deploying LINBIT SDS for Kubernetes by Using Helm

After output from the command shows that the Operator v2 was installed, you can use Helm to
deploy LINBIT SDS by installing the `linbit-sds` chart:

----
$ helm install -n linbit-sds linbit-sds linstor/linbit-sds
----

Output from this final `helm install` command should show a success message.

----
[...]
LinstorCluster: linbit-sds

Successfully deployed!
[...]
----

[[s-kubernetes-configuring-storage-v2]]
==== Configuring Storage with Operator v2

By default, LINBIT SDS for Kubernetes does not configure any storage. To add storage, you can configure a
`LinstorSatelliteConfiguration`, which the Operator uses to configure one or more satellites.

The following example creates a simple `FILE_THIN` pool and it does not require any additional set up on the host:

----
$ kubectl apply -f - <<EOF
apiVersion: piraeus.io/v1
kind: LinstorSatelliteConfiguration
metadata:
  name: storage-pool
spec:
  storagePools:
    - name: pool1
      fileThinPool:
        directory: /var/lib/linbit-sds/pool1
EOF
----

Other types of storage pools can be configured as well. Refer to
link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md#specstoragepools[the examples upstream].

[[s-kubenetes-securing-deployment-v2]]
==== Securing Operator v2 Deployment

By configuring key and certificate based encryption, you can make communication between certain LINSTOR components, for example, between LINSTOR satellite nodes and a LINSTOR controller node, or between the LINSTOR client and the LINSTOR API, more secure.

[[s-kubernetes-configure-tls-between-controller-and-satellite-v2]]
===== Configuring TLS Between the LINSTOR Controller and Satellite

// https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/how-to/api-tls.md#how-to-configure-tls-for-the-linstor-api

To secure traffic between the LINSTOR controller and satellite nodes, you can configure TLS, either by using link:https://cert-manager.io/[cert-manager] or link:https://www.openssl.org/[OpenSSL] to create TLS certificates to encrypt the traffic.

[[s-kubernetes-provision-tls-using-cert-manager-v2]]
====== Provisioning Keys and Certificates By Using cert-manager

This method requires a working cert-manager deployment in your cluster. For an alternative way to provision keys and certificates, see the <<s-kubernetes-provision-tls-using-openssl-v2,OpenSSL>> section below.

The LINSTOR controller and satellite only need to trust each other. For that reason, you should only have a certificate authority (CA) for those components. Apply the following YAML configuration to your deployment to create a new cert-manager link:https://cert-manager.io/docs/concepts/issuer/[Issuer] resource:

.linstor-cert-manager.yaml
[source,yaml]
----
---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: ca-bootstrapper
  namespace: linbit-sds
spec:
  selfSigned: { }
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: linstor-internal-ca
  namespace: linbit-sds
spec:
  commonName: linstor-internal-ca
  secretName: linstor-internal-ca
  duration: 87600h # 10 years
  isCA: true
  usages:
    - signing
    - key encipherment
    - cert sign
  issuerRef:
    name: ca-bootstrapper
    kind: Issuer
---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: linstor-internal-ca
  namespace: linbit-sds
spec:
  ca:
    secretName: linstor-internal-ca
----

Next, configure the new issuer resource to let the LINSTOR Operator provision the certificates needed to encrypt the controller and satellite traffic, by applying the following YAML configuration:

.linstor-ca-issuer.yaml
[source,yaml]
----
---
apiVersion: piraeus.io/v1
kind: LinstorCluster
metadata:
  name: linstorcluster
spec:
  internalTLS:
    certManager:
      name: linstor-internal-ca
      kind: Issuer
---
apiVersion: piraeus.io/v1
kind: LinstorSatelliteConfiguration
metadata:
  name: internal-tls
spec:
  internalTLS:
    certManager:
      name: linstor-internal-ca
      kind: Issuer
----

After applying the configurations above to your deployment, you can <<s-kubernetes-tls-configuration-verifying-v2,verify that TLS traffic encryption is working>>.

[[s-kubernetes-provision-tls-using-openssl-v2]]
====== Provisioning Keys and Certificates By Using OpenSSL

If you completed the <<s-kubernetes-provision-tls-using-cert-manager-v2, Provisioning Keys and Certificates By Using cert-manager>> section above, you can skip this section and go to the <<s-kubernetes-tls-configuration-verifying-v2, Verifying TLS Configuration>> section.

This method requires the `openssl` program on the command line.

First, create a new CA by using a new key and a self-signed certificate. You can change options such as the encryption algorithm and expiry time to suit the requirements of your deployment.

----
# openssl req -new -newkey rsa:4096 -days 3650 -nodes -x509 \
-subj "/CN=linstor-internal-ca" \
-keyout ca.key -out ca.crt
----

Next, create two new keys, one for the LINSTOR controller, one for all satellites:

----
# openssl genrsa -out controller.key 4096
# openssl genrsa -out satellite.key 4096
----

Next, create a certificate for each key, valid for 10 years, signed by the CA that you created earlier:

----
# openssl req -new -sha256 -key controller.key -subj "/CN=linstor-controller" -out controller.csr
# openssl req -new -sha256 -key satellite.key -subj "/CN=linstor-satellite" -out satellite.csr
# openssl x509 -req -in controller.csr -CA ca.crt -CAkey ca.key \
-CAcreateserial -out controller.crt -days 3650 -sha256
# openssl x509 -req -in satellite.csr -CA ca.crt -CAkey ca.key \
-CAcreateserial -out satellite.crt -days 3650 -sha256
----

Next, create Kubernetes secrets from the created keys and certificates:

----
# kubectl create secret generic linstor-controller-internal-tls -n linbit-sds \
--type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=controller.crt \
--from-file=tls.key=controller.key
# kubectl create secret generic linstor-satellite-internal-tls -n linbit-sds \
--type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=satellite.crt \
--from-file=tls.key=satellite.key
----

Finally, configure the Operator resources to reference the newly created secrets, by applying the following YAML configuration to your deployment:

.linstor-internal-tls-secret.yaml
[source,yaml]
----
---
apiVersion: piraeus.io/v1
kind: LinstorCluster
metadata:
  name: linstorcluster
spec:
  internalTLS:
    secretName: linstor-controller-internal-tls
---
apiVersion: piraeus.io/v1
kind: LinstorSatelliteConfiguration
metadata:
  name: internal-tls
spec:
  internalTLS:
    secretName: linstor-satellite-internal-tls
----

[[s-kubernetes-tls-configuration-verifying-v2]]
====== Verifying TLS Configuration

After configuring LINSTOR controller and satellite traffic encryption, you can next verify the secure TLS connection between the LINSTOR controller and a satellite by examining the output of a `kubectl linstor node list` command. If TLS is enabled, the output will show `(SSL)` next to an active satellite address.

----
# kubectl linstor node list
+---------------------------------------------------------------------+
| Node               | NodeType  | Addresses                 | State  |
|=====================================================================|
| node01.example.com | SATELLITE | 10.116.72.142:3367 (SSL)  | Online |
| node02.example.com | SATELLITE | 10.127.183.140:3367 (SSL) | Online |
| node03.example.com | SATELLITE | 10.125.97.50:3367 (SSL)   | Online |
+---------------------------------------------------------------------+
----

NOTE: The above command relies on the `kubectl-linstor` command to simplify entering LINSTOR client commands in Kubernetes. You can install the tool by following the instructions in <<s-kubernetes-kubectl-linstor-utility,Simplifying LINSTOR Client Command Entry>>.

If the output shows `(PLAIN)` rather than `(SSL)`, this indicates that the TLS configuration was not applied successfully. Check the status of the `LinstorCluster` and `LinstorSatellite` resources.

If the output shows `(SSL)`, but the node remains offline, this usually indicates that a certificate is not trusted by the other party. Verify that the controller's `tls.crt` is trusted by the satellite's `ca.crt` and vice versa. The following shell function provides a quick way to verify that one TLS certificate is trusted by another:

----
function k8s_secret_trusted_by() {
	kubectl get secret -n linbit-sds \
    -ogo-template='{{ index .data "tls.crt" | base64decode }}' \
    "$1" > $1.tls.crt
	kubectl get secret -n linbit-sds \
    -ogo-template='{{ index .data "ca.crt" | base64decode }}' \
    "$2" > $2.ca.crt
	openssl verify -CAfile $2.ca.crt $1.tls.crt
}
# k8s_secret_trusted_by satellite-tls controller-tls
----

If TLS encryption was properly configured, output from running the above function should be:

----
satellite-tls.tls.crt: OK
----

The upstream Piraeus project's reference documentation shows all available link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md#specinternaltls[`LinstorCluster`] and link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md#specinternaltls[`LinstorSatelliteConfiguration`] resources options related to TLS.

===== Configuring TLS for the LINSTOR API
// https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/how-to/internal-tls.md

This section describes how to set up TLS for the LINSTOR API. The API, served by the LINSTOR controller, is used by clients such as the CSI Driver and the Operator itself to control the LINSTOR cluster.

To follow the instructions in this section, you should be familiar with:

    - Editing `LinstorCluster` resources
    - Using either link:https://cert-manager.io/[cert-manager] or OpenSSL to create TLS certificates

[[s-kubernetes-securing-linstor-api-provisioning-keys-cert-manager-v2]]
====== Provisioning Keys and Certificates By Using cert-manager

This method requires a working link:https://cert-manager.io/[cert-manager] deployment in your cluster. For an alternative way to provision keys and certificates, see the <<s-kubernetes-securing-linstor-api-provisioning-keys-openssl-v2,OpenSSL>> section below.

When using TLS, the LINSTOR API uses client certificates for authentication. It is good practice to have a separate CA just for these certificates. To do this, first apply the following YAML configuration to your deployment to create a certificate issuer.

----
---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: ca-bootstrapper
  namespace: linbit-sds
spec:
  selfSigned: { }
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: linstor-api-ca
  namespace: linbit-sds
spec:
  commonName: linstor-api-ca
  secretName: linstor-api-ca
  duration: 87600h # 10 years
  isCA: true
  usages:
    - signing
    - key encipherment
    - cert sign
  issuerRef:
    name: ca-bootstrapper
    kind: Issuer
---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: linstor-api-ca
  namespace: linbit-sds
spec:
  ca:
    secretName: linstor-api-ca
----

Next, configure this issuer to let the Operator provision the needed certificates, by applying the following configuration.

----
---
apiVersion: piraeus.io/v1
kind: LinstorCluster
metadata:
  name: linstorcluster
spec:
  apiTLS:
    certManager:
      name: linstor-api-ca
      kind: Issuer
----

This completes the necessary steps for securing the LINSTOR API with TLS by using cert-manager. Skip to the <<s-kubernetes-securing-linstor-api-verifying-tls-configuration-v2,Verifying LINSTOR API TLS Configuration>> section to verify that TLS is working.

[[s-kubernetes-securing-linstor-api-provisioning-keys-openssl-v2]]
====== Provisioning Keys and Certificates By Using OpenSSL

This method requires the `openssl` program on the command line. For an alternative way to provision keys and certificates, see the <<s-kubernetes-securing-linstor-api-provisioning-keys-cert-manager-v2,cert-manager>> section above.

First, create a new certificate authority (CA) by using a new key and a self-signed certificate. You can change options such as the encryption algorithm and expiry time to suit the requirements of your deployment.

----
# openssl req -new -newkey rsa:4096 -days 3650 -nodes -x509 \
-subj "/CN=linstor-api-ca" \
-keyout ca.key -out ca.crt
----

Next, create two new keys, one for the LINSTOR API server, and one for all LINSTOR API clients:

----
# openssl genrsa -out api-server.key 4096
# openssl genrsa -out api-client.key 4096
----

Next, create a certificate for the server. Because the clients might use different shortened service names, you need to specify multiple subject names:

----
# cat /etc/ssl/openssl.cnf > api-csr.cnf
# cat >> api-csr.cnf <<EOF
[ v3_req ]
subjectAltName = @alt_names
[ alt_names ]
DNS.0 = linstor-controller.linbit-sds.svc.cluster.local
DNS.1 = linstor-controller.linbit-sds.svc
DNS.2 = linstor-controller
EOF
# openssl req -new -sha256 -key api-server.key \
-subj "/CN=linstor-controller" -config api-csr.cnf \
-extensions v3_req -out api-server.csr
# openssl x509 -req -in api-server.csr -CA ca.crt -CAkey ca.key \
-CAcreateserial -config api-csr.cnf \
-extensions v3_req -out api-server.crt \
-days 3650 -sha256
----

For the client certificate, setting one subject name is enough.

----
# openssl req -new -sha256 -key api-client.key \
-subj "/CN=linstor-client" -out api-client.csr
# openssl x509 -req -in api-client.csr \
-CA ca.crt -CAkey ca.key -CAcreateserial \
-out api-client.crt \
-days 3650 -sha256
----

Next, create Kubernetes secrets from the created keys and certificates.

----
# kubectl create secret generic linstor-api-tls -n linbit-sds \
--type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=api-server.crt \
--from-file=tls.key=api-server.key
# kubectl create secret generic linstor-client-tls -n linbit-sds \
--type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=api-client.crt \
--from-file=tls.key=api-client.key
----

Finally, configure the Operator resources to reference the newly created secrets. For simplicity, you can configure the same client secret for all components.

----
apiVersion: piraeus.io/v1
kind: LinstorCluster
metadata:
  name: linstorcluster
spec:
  apiTLS:
    apiSecretName: linstor-api-tls
    clientSecretName: linstor-client-tls
    csiControllerSecretName: linstor-client-tls
    csiNodeSecretName: linstor-client-tls
----

[[s-kubernetes-securing-linstor-api-verifying-tls-configuration-v2]]
====== Verifying LINSTOR API TLS Configuration

You can verify that the API is running, secured by TLS, by manually connecting to the HTTPS endpoint using a `curl` command.

----
# kubectl exec -n linbit-sds deploy/linstor-controller -- \
curl --key /etc/linstor/client/tls.key \
--cert /etc/linstor/client/tls.crt \
--cacert /etc/linstor/client/ca.crt \
https://linstor-controller.linbit-sds.svc:3371/v1/controller/version
----

If the command is successful, the API is using HTTPS, clients are able to connect to the controller with their certificates, and the command output should show something similar to this:

----
{"version":"1.20.2","git_hash":"58a983a5c2f49eb8d22c89b277272e6c4299457a","build_time":"2022-12-14T14:21:28+00:00","rest_api_version":"1.16.0"}%
----

If the command output shows an error, verify that the client certificates are trusted by the API secret, and vice versa. The following shell function provides a quick way to verify that one TLS certificate is trusted by another:

----
function k8s_secret_trusted_by() {
    kubectl get secret -n linbit-sds \
    -ogo-template='{{ index .data "tls.crt" | base64decode }}' \
    "$1" > $1.tls.crt
    kubectl get secret -n linbit-sds \
    -ogo-template='{{ index .data "ca.crt" | base64decode }}' \
    "$2" > $2.ca.crt
    openssl verify -CAfile $2.ca.crt $1.tls.crt
}
# k8s_secret_trusted_by satellite-tls controller-tls
----

If TLS encryption was properly configured, output from running the above function should be:

----
satellite-tls.tls.crt: OK
----

Another issue might be the API endpoint using a certificate that is not using the expected service name. A typical error message for this issue would be:

[%autofit]
----
curl: (60) SSL: no alternative certificate subject name matches target host name 'linstor-controller.piraeus-datastore.svc'
----

In this case, make sure you have specified the right subject names when provisioning the certificates.

All available options are documented in the upstream Piraeus project's reference documentation for link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md#specapitls[`LinstorCluster`].

===== Creating a Passphrase For LINSTOR
// https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md#speclinstorpassphrasesecret

LINSTOR can use a passphrase for operations such as <<s-linstor-encrypted-volumes,encrypting volumes>> and storing access credentials for backups.

To configure a LINSTOR passphrase in a Kubernetes deployment, the referenced secret must exist in the same namespace as the operator (by default `linbit-sds`), and have a `MASTER_PASSPHRASE` entry.

The following example YAML configuration for the `.spec.linstorPassphraseSecret` configures a passphrase `example-passphrase`.

IMPORTANT: Choose a different passphrase for your deployment.

----
---
apiVersion: v1
kind: Secret
metadata:
  name: linstor-passphrase
  namespace: linbit-sds
data:
  # CHANGE THIS TO USE YOUR OWN PASSPHRASE!
  # Created by: echo -n "example-passphrase" | base64
  MASTER_PASSPHRASE: ZXhhbXBsZS1wYXNzcGhyYXNl
---
apiVersion: piraeus.io/v1
kind: LinstorCluster
metadata:
  name: linstorcluster
spec:
  linstorPassphraseSecret: linstor-passphrase
----

[[s-kubernetes-using-crds-v2]]
==== Using CustomResourceDefinitions in Operator v2 Deployments

Within LINSTOR Operator v2 deployments, you can change the cluster state by modifying LINSTOR related Kubernetes `CustomResourceDefinitions` (CRDs) or check the status of a resource. An overview list of these resources follows. Refer to the upstream Piraeus project's API reference (linked for each resource below) for more details.

link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md[`LinstorCluster`]:: This resource controls the state of the LINSTOR cluster and integration with Kubernetes.

link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md[`LinstorSatelliteConfiguration`]:: This resource controls the state of the LINSTOR satellites, optionally applying it to only a subset of nodes.

link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatellite.md[`LinstorSatellite`]:: This resource controls the state of a single LINSTOR satellite. This resource is not intended to be changed directly, rather it is created by the LINSTOR Operator by merging all matching `LinstorSatelliteConfiguration` resources.

link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstornodeconnection.md[`LinstorNodeConnection`]:: This resource controls the state of the LINSTOR node connections.

[[s-kubernetes-next-steps-after-deploying-operator-v2]]
==== Next Steps After Deploying LINSTOR Operator v2

After deploying LINBIT SDS for Kubernetes, you can continue with the
<<s-kubernetes-basic-configuration-and-deployment>>, <<s-kubernetes-drbd-module-loader-configuring-v2>>, <<s-kubernetes-drbd-replication-via-host-network-v2>> sections in this chapter, or refer to the
available link:https://github.com/piraeusdatastore/piraeus-operator/tree/v2/docs/tutorial[tutorials] in the upstream Piraeus project.

[[s-kubernetes-deploy-linstor-operator-v1]]
=== Deploying LINSTOR Operator v1

IMPORTANT: If you plan to deploy LINSTOR Operator on a new cluster, you should use
<<s-kubernetes-deploy-linstor-operator-v2, Operator v2>>. If you have already deployed the LINSTOR Operator v2, you can skip this section and proceed to other topics in the chapter, beginning with <<s-kubernetes-deploy-external-controller>>.

The Operator v1 is installed using a Helm v3 chart as follows:

* Create a Kubernetes secret containing your my.linbit.com credentials:
+
----
kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \
  --docker-username=<YOUR_LOGIN> --docker-email=<YOUR_EMAIL> --docker-password=<YOUR_PASSWORD>
----
+
The name of this secret must match the one specified in the Helm values,
by default `drbdiocred`.

* Configure the LINSTOR database back end. By default, the chart configures etcd as database
back end. The Operator can also configure LINSTOR to use
<<s-kubernetes-linstor-k8s-backend,Kubernetes as datastore>> directly. If you go the etcd
route, you should configure persistent storage for it:
** Use an existing storage provisioner with a default `StorageClass`.
** <<s-kubernetes-etcd-hostpath-persistence,Use `hostPath` volumes>>.
** Disable persistence, **for basic testing only**. This can be done by adding
   `--set etcd.persistentVolume.enabled=false` to the `helm install` command below.

* Read <<s-kubernetes-storage, the storage guide>> and configure a basic storage setup for LINSTOR

* Read the <<s-kubernetes-securing-deployment-v1,section on securing the deployment>> and configure as needed.

* Select the appropriate kernel module injector using `--set` with the `helm install` command in the final step.

** Choose the injector according to the distribution you are using. Select the latest version from one of `drbd9-rhel7`, `drbd9-rhel8`, and others, from http://drbd.io/ as appropriate. The `drbd9-rhel8` image should also be used for RHCOS (OpenShift). For the SUSE CaaS Platform use the SLES injector that matches the base system of the CaaS Platform you are using (e.g., `drbd9-sles15sp1`). For example:
+
----
operator.satelliteSet.kernelModuleInjectionImage=drbd.io/drbd9-rhel8:v9.1.8
----

** Only inject modules that are already present on the host machine. If a module is not found, it will be skipped.
+
----
operator.satelliteSet.kernelModuleInjectionMode=DepsOnly
----

** Disable kernel module injection if you are installing DRBD by other means. Deprecated by `DepsOnly`
+
----
operator.satelliteSet.kernelModuleInjectionMode=None
----

* Finally create a Helm deployment named `linstor-op` that will set up everything.
+
----
helm repo add linstor https://charts.linstor.io
helm install linstor-op linstor/linstor
----
Further deployment customization is discussed in the <<s-kubernetes-advanced-deployments,advanced deployment section>>

[[s-kubernetes-linstor-k8s-backend]]
==== Kubernetes Back End for LINSTOR

The LINSTOR controller can use the Kubernetes API directly to persist its cluster state. To enable
this back end, use the following override file during the chart installation:

.k8s-backend.yaml
[source,yaml]
----
etcd:
  enabled: false
operator:
  controller:
    dbConnectionURL: k8s
----

TIP: It is possible to migrate an existing cluster that uses an etcd back end to a Kubernetes
API back end, by following link:https://charts.linstor.io/migration/1-migrate-db.html[the
migration instructions at charts.linstor.io].

[[s-kubernetes-etcd-hostpath-persistence]]
==== Creating Persistent Storage Volumes

You can use the `pv-hostpath` Helm templates to create `hostPath` persistent
volumes. Create as many PVs as needed to satisfy your configured etcd
`replicas` (default 1).

Create the `hostPath` persistent volumes, substituting cluster node
names accordingly in the `nodes=` option:

----
helm repo add linstor https://charts.linstor.io
helm install linstor-etcd linstor/pv-hostpath
----

By default, a PV is created on every `control-plane` node. You can manually select the storage nodes by
passing `--set "nodes={<NODE0>,<NODE1>,<NODE2>}"` to the install command.

NOTE: The correct value to reference the node is the value of the `kubernetes.io/hostname` label. You can list the
value for all nodes by running `kubectl get nodes -o custom-columns="Name:{.metadata.name},NodeName:{.metadata.labels['kubernetes\.io/hostname']}"`

[[s-kubernetes-existing-database]]
==== Using an Existing Database

LINSTOR can connect to an existing PostgreSQL, MariaDB or etcd database. For
instance, for a PostgreSQL instance with the following configuration:

----
POSTGRES_DB: postgresdb
POSTGRES_USER: postgresadmin
POSTGRES_PASSWORD: admin123
----

The Helm chart can be configured to use this database rather than deploying an
etcd cluster, by adding the following to the Helm install command:

----
--set etcd.enabled=false --set "operator.controller.dbConnectionURL=jdbc:postgresql://postgres/postgresdb?user=postgresadmin&password=admin123"
----

[[s-kubernetes-configuring-storage-v1]]
==== Configuring Storage With Operator v1

The LINSTOR Operator v1 can automate some basic storage set up for LINSTOR.

===== Configuring Storage Pool Creation

The LINSTOR Operator can be used to create LINSTOR storage pools. Creation is under control of the
`LinstorSatelliteSet` resource:

[source]
----
$ kubectl get LinstorSatelliteSet.linstor.linbit.com linstor-op-ns -o yaml
kind: LinstorSatelliteSet
metadata:
[...]
spec:
  [...]
  storagePools:
    lvmPools:
    - name: lvm-thick
      volumeGroup: drbdpool
    lvmThinPools:
    - name: lvm-thin
      thinVolume: thinpool
      volumeGroup: ""
    zfsPools:
    - name: my-linstor-zpool
      zPool: for-linstor
      thin: true
----

===== Creating Storage Pools at Installation Time

At installation time, by setting the value of `operator.satelliteSet.storagePools` when running the `helm install` command.

First create a file with the storage configuration such as:

[source,yaml]
----
operator:
  satelliteSet:
    storagePools:
      lvmPools:
      - name: lvm-thick
        volumeGroup: drbdpool
----

This file can be passed to the Helm installation by entering the following command:

[source]
----
helm install -f <file> linstor-op linstor/linstor
----

===== Creating Storage Pools After Installation

On a cluster with the operator already configured (that is, after `helm install`),
you can edit the `LinstorSatelliteSet` configuration by entering the following command:

[source]
----
$ kubectl edit LinstorSatelliteSet.linstor.linbit.com <satellitesetname>
----

The storage pool configuration can be updated as in the example above.

===== Preparing Physical Devices

By default, LINSTOR expects the referenced VolumeGroups, ThinPools and so on to be present. You can use the
`devicePaths: []` option to let LINSTOR automatically prepare devices for the pool. Eligible for automatic configuration
are block devices that:

* Are a root device (no partition)
* do not contain partition information
* have more than 1 GiB

To enable automatic configuration of devices, set the `devicePaths` key on `storagePools` entries:

[source,yaml]
----
  storagePools:
    lvmPools:
    - name: lvm-thick
      volumeGroup: drbdpool
      devicePaths:
      - /dev/vdb
    lvmThinPools:
    - name: lvm-thin
      thinVolume: thinpool
      volumeGroup: linstor_thinpool
      devicePaths:
      - /dev/vdc
      - /dev/vdd
----

Currently, this method supports creation of LVM and LVMTHIN storage pools.

===== Configuring LVM Storage Pools

The available keys for `lvmPools` entries are:

* `name` name of the LINSTOR storage pool. [Required]

* `volumeGroup` name of the VG to create. [Required]

* `devicePaths` devices to configure for this pool. Must be empty and >= 1GiB to be recognized. [Optional]

* `raidLevel` LVM raid level. [Optional]

* `vdo` Enable [VDO] (requires VDO tools in the satellite). [Optional]

* `vdoLogicalSizeKib` Size of the created VG (expected to be bigger than the backing devices by using VDO). [Optional]

* `vdoSlabSizeKib` Slab size for VDO. [Optional]

[VDO]: https://www.redhat.com/en/blog/look-vdo-new-linux-compression-layer

===== Configuring LVM Thin Pools

* `name` name of the LINSTOR storage pool. [Required]

* `volumeGroup` VG to use for the thin pool. If you want to use `devicePaths`, you must set this to `""`. This is required because LINSTOR does not allow configuration of the VG name when preparing devices. `thinVolume` name of the thin pool. [Required]

* `devicePaths` devices to configure for this pool. Must be empty and >= 1GiB to be recognized. [Optional]

* `raidLevel` LVM raid level. [Optional]

NOTE: The volume group created by LINSTOR for LVM thin pools will always follow the scheme "linstor_$THINPOOL".

===== Configuring ZFS Storage Pools

* `name` name of the LINSTOR storage pool. [Required]
* `zPool` name of the `zpool` to use. Must already be present on all machines. [Required]
* `thin` `true` to use thin provisioning, `false` otherwise. [Required]

===== Automatic Storage Type Provisioning (DEPRECATED)

_ALL_ eligible devices will be prepared according to the value of `operator.satelliteSet.automaticStorageType`, unless
they are already prepared using the `storagePools` section. Devices are added to a storage pool based on the device
name (that is, all `/dev/nvme1` devices will be part of the pool `autopool-nvme1`)

The possible values for `operator.satelliteSet.automaticStorageType`:

* `None` no automatic set up (default)
* `LVM` create a LVM (thick) storage pool
* `LVMTHIN` create a LVM thin storage pool
* `ZFS` create a ZFS based storage pool (**UNTESTED**)

[[s-kubernetes-securing-deployment-v1]]
==== Securing Operator v1 Deployment

This section describes the different options for enabling security features available when
using a LINSTOR Operator v1 deployment (<<s-kubernetes-deploy-linstor-operator-v1,using Helm>>) in Kubernetes.

===== Secure Communication with an Existing etcd Instance

Secure communication to an `etcd` instance can be enabled by providing a CA certificate to the operator in form of a
Kubernetes secret. The secret has to contain the key `ca.pem` with the PEM encoded CA certificate as value.

The secret can then be passed to the controller by passing the following argument to `helm install`

----
--set operator.controller.dbCertSecret=<secret name>
----

===== Authentication with `etcd` Using Certificates

If you want to use TLS certificates to authenticate with an `etcd` database, you need to set the following option on
Helm install:

----
--set operator.controller.dbUseClientCert=true
----

If this option is active, the secret specified in the above section must contain two additional keys:

* `client.cert` PEM formatted certificate presented to `etcd` for authentication
* `client.key` private key **in PKCS8 format**, matching the above client certificate.

Keys can be converted into PKCS8 format using `openssl`:

----
openssl pkcs8 -topk8 -nocrypt -in client-key.pem -out client-key.pkcs8
----

[[s-kubenetes-secure-communication-between-linstor-components-v1]]
==== Configuring Secure Communication Between LINSTOR Components in Operator v1 Deployments

The default communication between LINSTOR components is not secured by TLS. If this is needed for your setup,
choose one of three methods:

// "cert-manager" is a product name so keep the original case

===== Generating Keys and Certificates Using cert-manager

Requires https://cert-manager.io/docs/[cert-manager] to be installed in your cluster.

Set the following options in your Helm override file:

[source,yaml]
----
linstorSslMethod: cert-manager
linstorHttpsMethod: cert-manager
----

===== Generate Keys and Certificates Using Helm

Set the following options in your Helm override file:

[source,yaml]
----
linstorSslMethod: helm
linstorHttpsMethod: helm
----

===== Generating Keys and Certificates Manually

Create a private key and self-signed certificate for your certificate authorities:

----
openssl req -new -newkey rsa:2048 -days 5000 -nodes -x509 -keyout ca.key \
  -out ca.crt -subj "/CN=linstor-system"
openssl req -new -newkey rsa:2048 -days 5000 -nodes -x509 -keyout client-ca.key \
  -out client-ca.crt -subj "/CN=linstor-client-ca"
----

Create private keys, two for the controller, one for all nodes and one for all clients:

----
openssl genrsa -out linstor-control.key 2048
openssl genrsa -out linstor-satellite.key 2048
openssl genrsa -out linstor-client.key 2048
openssl genrsa -out linstor-api.key 2048
----

Create trusted certificates for controller and nodes:

----
openssl req -new -sha256 -key linstor-control.key -subj "/CN=system:control" \
  -out linstor-control.csr
openssl req -new -sha256 -key linstor-satellite.key -subj "/CN=system:node" \
  -out linstor-satellite.csr
openssl req -new -sha256 -key linstor-client.key -subj "/CN=linstor-client" \
  -out linstor-client.csr
openssl req -new -sha256 -key linstor-api.key -subj "/CN=linstor-controller" \
  -out  linstor-api.csr
openssl x509 -req -in linstor-control.csr -CA ca.crt -CAkey ca.key -CAcreateserial \
  -out linstor-control.crt -days 5000 -sha256
openssl x509 -req -in linstor-satellite.csr -CA ca.crt -CAkey ca.key -CAcreateserial \
  -out linstor-satellite.crt -days 5000 -sha256
openssl x509 -req -in linstor-client.csr -CA client-ca.crt -CAkey client-ca.key \
  -CAcreateserial -out linstor-client.crt -days 5000 -sha256
openssl x509 -req -in linstor-api.csr -CA client-ca.crt -CAkey client-ca.key \
  -CAcreateserial -out linstor-api.crt -days 5000 -sha256 -extensions 'v3_req' \
  -extfile <(printf '%s\n' '[v3_req]' extendedKeyUsage=serverAuth \
  subjectAltName=DNS:linstor-op-cs.default.svc)
----

NOTE: `linstor-op-cs.default.svc` in the last command needs to match create service name. With Helm, this is always
`<release-name>-cs.<namespace>.svc`.

Create Kubernetes secrets that can be passed to the controller and node pods:

----
kubectl create secret generic linstor-control --type=kubernetes.io/tls \
  --from-file=ca.crt=ca.crt --from-file=tls.crt=linstor-control.crt \
  --from-file=tls.key=linstor-control.key
kubectl create secret generic linstor-satellite --type=kubernetes.io/tls \
  --from-file=ca.crt=ca.crt --from-file=tls.crt=linstor-satellite.crt \
  --from-file=tls.key=linstor-satellite.key
kubectl create secret generic linstor-api --type=kubernetes.io/tls \
  --from-file=ca.crt=client-ca.crt --from-file=tls.crt=linstor-api.crt \
  --from-file=tls.key=linstor-api.key
kubectl create secret generic linstor-client --type=kubernetes.io/tls \
  --from-file=ca.crt=client-ca.crt --from-file=tls.crt=linstor-client.crt \
  --from-file=tls.key=linstor-client.key
----

Pass the names of the created secrets to `helm install`:

[source,yaml]
----
linstorHttpsControllerSecret: linstor-api
linstorHttpsClientSecret: linstor-client
operator:
  controller:
    sslSecret: linstor-control
  satelliteSet:
    sslSecret: linstor-satellite
----

[[s-kubernetes-linstor-master-passphrase-v1]]
===== Automatically Set the Passphrase for LINSTOR

LINSTOR needs to store confidential data to support encrypted information. This data is protected by a master
passphrase. A passphrase is automatically generated on the first chart install.

If you want to use a custom passphrase, store it in a secret:

----
kubectl create secret generic linstor-pass --from-literal=MASTER_PASSPHRASE=<password>
----

On install, add the following arguments to the Helm command:

----
--set operator.controller.luksSecret=linstor-pass
----

[[s-kubernetes-helm-install-examples-v1]]
==== Helm Installation Examples for Operator v1

All the below examples use the following `sp-values.yaml` file. Feel
free to adjust this for your uses and environment. See <<Configuring storage pool creation>>
for further details.

----
operator:
  satelliteSet:
    storagePools:
      lvmThinPools:
      - name: lvm-thin
        thinVolume: thinpool
        volumeGroup: ""
        devicePaths:
        - /dev/sdb
----

NOTE: Default install. This does not setup any persistence for
the backing etcd key-value store.

WARNING: This is not suggested for any use outside of testing.

----
kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \
  --docker-username=<YOUR_LOGIN> --docker-password=<YOUR_PASSWORD>
helm repo add linstor https://charts.linstor.io
helm install linstor-op linstor/linstor
----

IMPORTANT: LINBIT's container image repository (http://drbd.io), used in the previous and
upcoming `kubectl create` commands, is only available to LINBIT customers or through LINBIT
customer trial accounts. link:https://linbit.com/contact-us/[Contact LINBIT for information on
pricing or to begin a trial]. Alternatively, you can use the LINSTOR SDS upstream project named
link:https://github.com/piraeusdatastore/piraeus-operator[Piraeus], without being a LINBIT
customer.

Install with LINSTOR storage-pools defined at install through
`sp-values.yaml`, persistent `hostPath` volumes, three etcd replicas, and by
compiling the DRBD kernel modules for the host kernels.

This should be adequate for most basic deployments. Note that
this deployment is not using the pre-compiled DRBD kernel modules just
to make this command more portable. Using the pre-compiled binaries
will make for a much faster install and deployment. Using the
`Compile` option would not be suggested for use in a large Kubernetes clusters.

----
kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \
  --docker-username=<YOUR_LOGIN> --docker-password=<YOUR_PASSWORD>
helm repo add linstor https://charts.linstor.io
helm install linstor-etcd linstor/pv-hostpath --set "nodes={<NODE0>,<NODE1>,<NODE2>}"
helm install -f sp-values.yaml linstor-op linstor/linstor --set etcd.replicas=3 \
  --set operator.satelliteSet.kernelModuleInjectionMode=Compile
----

Install with LINSTOR storage-pools defined at install through
`sp-values.yaml`, use an already created PostgreSQL DB (preferably
clustered), rather than etcd, and use already compiled kernel modules for
DRBD.

The PostgreSQL database in this particular example is reachable through a
service endpoint named `postgres`. PostgreSQL itself is configured with
`POSTGRES_DB=postgresdb`, `POSTGRES_USER=postgresadmin`, and
`POSTGRES_PASSWORD=admin123`

----
kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \
  --docker-username=<YOUR_LOGIN> --docker-email=<YOUR_EMAIL> --docker-password=<YOUR_PASSWORD>
helm repo add linstor https://charts.linstor.io
helm install -f sp-values.yaml linstor-op linstor/linstor --set etcd.enabled=false \
  --set "operator.controller.dbConnectionURL=jdbc:postgresql://postgres/postgresdb?user=postgresadmin&password=admin123"
----

[[s-kubernetes-helm-terminate]]
==== Terminating Helm Deployment

To protect the storage infrastructure of the cluster from accidentally deleting vital components, it is necessary to perform some manual steps before deleting a Helm deployment.

1. Delete all volume claims managed by LINSTOR components. You can use the following command to get a list of volume claims managed by LINSTOR. After checking that none of the listed volumes still hold needed data, you can delete them using the generated `kubectl delete` command.
+
----
$ kubectl get pvc --all-namespaces -o=jsonpath='{range .items[?(@.metadata.annotations.volume\.beta\.kubernetes\.io/storage-provisioner=="linstor.csi.linbit.com")]}kubectl delete pvc --namespace {.metadata.namespace} {.metadata.name}{"\n"}{end}'
kubectl delete pvc --namespace default data-mysql-0
kubectl delete pvc --namespace default data-mysql-1
kubectl delete pvc --namespace default data-mysql-2
----
+
WARNING: These volumes, once deleted, cannot be recovered.

2. Delete the LINSTOR controller and satellite resources.
+
Deployment of LINSTOR satellite and controller is controlled by the `LinstorSatelliteSet` and `LinstorController` resources. You can delete the resources associated with your deployment by using `kubectl`
+
----
kubectl delete linstorcontroller <helm-deploy-name>-cs
kubectl delete linstorsatelliteset <helm-deploy-name>-ns
----
+
After a short wait, the controller and satellite pods should terminate. If they continue to run, you can check the above resources for errors (they are only removed after all associated pods have terminated).

3. Delete the Helm deployment.
+
If you removed all PVCs and all LINSTOR pods have terminated, you can uninstall the Helm deployment
+
----
helm uninstall linstor-op
----
+
NOTE: Due to the Helm's current policy, the Custom Resource Definitions named `LinstorController` and `LinstorSatelliteSet` will not be deleted by the command.
More information regarding Helm's current position on CRDs can be found https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#method-1-let-helm-do-it-for-you[here].

[[s-kubernetes-advanced-deployments-v1]]
==== Advanced Deployment Options for Operator v1

The Helm charts provide a set of further customization options for advanced use cases.

IMPORTANT: LINBIT's container image repository (http://drbd.io), used in the Helm chart below, is only available to LINBIT customers or through LINBIT customer trial accounts. link:https://linbit.com/contact-us/[Contact LINBIT for information on pricing or to begin a trial]. Alternatively, you can use the LINSTOR SDS upstream project named link:https://github.com/piraeusdatastore/piraeus-operator[Piraeus], without being a LINBIT customer.

[source,yaml]
----
global:
  imagePullPolicy: IfNotPresent # empty pull policy means k8s default is used ("always" if tag == ":latest", "ifnotpresent" else) <1>
  setSecurityContext: true # Force non-privileged containers to run as non-root users
# Dependency charts
etcd:
  enabled: true
  persistentVolume:
    enabled: true
    storage: 1Gi
  replicas: 1 # How many instances of etcd will be added to the initial cluster. <2>
  resources: {} # resource requirements for etcd containers <3>
  image:
    repository: gcr.io/etcd-development/etcd
    tag: v3.4.15
stork:
  enabled: false
  storkImage: docker.io/openstorage/stork:2.8.2
  schedulerImage: registry.k8s.io/kube-scheduler
  schedulerTag: ""
  replicas: 1 <2>
  storkResources: {} # resources requirements for the stork plugin containers <3>
  schedulerResources: {} # resource requirements for the kube-scheduler containers <3>
  podsecuritycontext: {}
csi:
  enabled: true
  pluginImage: "drbd.io/linstor-csi:v1.1.0"
  csiAttacherImage: registry.k8s.io/sig-storage/csi-attacher:v4.3.0
  csiLivenessProbeImage: registry.k8s.io/sig-storage/livenessprobe:v2.10.0
  csiNodeDriverRegistrarImage: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0
  csiProvisionerImage: registry.k8s.io/sig-storage/csi-provisioner:v3.5.0
  csiSnapshotterImage: registry.k8s.io/sig-storage/csi-snapshotter:v6.2.1
  csiResizerImage: registry.k8s.io/sig-storage/csi-resizer:v1.8.0
  csiAttacherWorkerThreads: 10 <9>
  csiProvisionerWorkerThreads: 10 <9>
  csiSnapshotterWorkerThreads: 10 <9>
  csiResizerWorkerThreads: 10 <9>
  controllerReplicas: 1 <2>
  nodeAffinity: {} <4>
  nodeTolerations: [] <4>
  controllerAffinity: {} <4>
  controllerTolerations: [] <4>
  enableTopology: true
  resources: {} <3>
  customLabels: {}
  customAnnotations: {}
  kubeletPath: /var/lib/kubelet <7>
  controllerSidecars: []
  controllerExtraVolumes: []
  nodeSidecars: []
  nodeExtraVolumes: []
priorityClassName: ""
drbdRepoCred: drbdiocred
linstorSslMethod: "manual" # <- If set to 'helm' or 'cert-manager' the certificates will be generated automatically
linstorHttpsMethod: "manual" # <- If set to 'helm' or 'cert-manager' the certificates will be generated automatically
linstorHttpsControllerSecret: "" # <- name of secret containing linstor server certificates+key. See docs/security.md
linstorHttpsClientSecret: "" # <- name of secret containing linstor client certificates+key. See docs/security.md
controllerEndpoint: "" # <- override to the generated controller endpoint. use if controller is not deployed via operator
psp:
  privilegedRole: ""
  unprivilegedRole: ""
operator:
  replicas: 1 # <- number of replicas for the operator deployment <2>
  image: "drbd.io/linstor-operator:v1.10.4"
  affinity: {} <4>
  tolerations: [] <4>
  resources: {} <3>
  customLabels: {}
  customAnnotations: {}
  podsecuritycontext: {}
  args:
    createBackups: true
    createMonitoring: true
  sidecars: []
  extraVolumes: []
  controller:
    enabled: true
    controllerImage: "drbd.io/linstor-controller:v1.23.0"
    dbConnectionURL: ""
    luksSecret: ""
    dbCertSecret: ""
    dbUseClientCert: false
    sslSecret: ""
    affinity: {} <4>
    httpBindAddress: ""
    httpsBindAddress: ""
    tolerations: <4>
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
    resources: {} <3>
    replicas: 1 <2>
    additionalEnv: [] <5>
    additionalProperties: {} <6>
    sidecars: []
    extraVolumes: []
    customLabels: {}
    customAnnotations: {}
  satelliteSet:
    enabled: true
    satelliteImage: "drbd.io/linstor-satellite:v1.23.0"
    storagePools: {}
    sslSecret: ""
    automaticStorageType: None
    affinity: {} <4>
    tolerations: [] <4>
    resources: {} <3>
    monitoringImage: "drbd.io/drbd-reactor:v1.2.0"
    monitoringBindAddress: ""
    kernelModuleInjectionImage: "drbd.io/drbd9-rhel7:v9.1.14"
    kernelModuleInjectionMode: ShippedModules
    kernelModuleInjectionAdditionalSourceDirectory: "" <8>
    kernelModuleInjectionResources: {} <3>
    kernelModuleInjectionExtraVolumeMounts: []
    mountDrbdResourceDirectoriesFromHost: "" <10>
    additionalEnv: [] <5>
    sidecars: []
    extraVolumes: []
    customLabels: {}
    customAnnotations: {}
haController:
  enabled: false
  image: drbd.io/linstor-k8s-ha-controller:v0.3.0
  affinity: {} <4>
  tolerations: [] <4>
  resources: {} <3>
  replicas: 1 <2>
  customLabels: {}
  customAnnotations: {}
----
<1> Sets the pull policy for all images.

<2> Controls the number of replicas for each component.

<3> Set container resource requests and limits. See https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/[the Kubernetes docs].
 Most containers need a minimal amount of resources, except for:
    * `etcd.resources` See the https://etcd.io/docs/v3.4.0/op-guide/hardware/[etcd docs]
    * `operator.controller.resources` Around `700MiB` memory is required
    * `operater.satelliteSet.resources` Around `700MiB` memory is required
    * `operator.satelliteSet.kernelModuleInjectionResources` If kernel modules are compiled,
1GiB of memory is required.

<4> Affinity and toleration determine where pods are scheduled on the cluster. See the
https://kubernetes.io/docs/concepts/scheduling-eviction/[Kubernetes docs on affinity and
toleration]. This might be especially important for the `operator.satelliteSet` and `csi.node*`
values. To schedule a pod using a LINSTOR persistent volume, the node requires a running
LINSTOR satellite and LINSTOR CSI pod.

<5> Sets additional environments variables to pass to the LINSTOR controller and satellites.
Uses the same format as https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/[the
`env` value of a container]

<6> Sets additional properties on the LINSTOR controller. Expects a simple mapping of `<property-key>: <value>`.

<7> kubelet expects every CSI plugin to mount volumes under a specific subdirectory of its own state directory. By default, this state directory is `/var/lib/kubelet`. Some Kubernetes distributions use a different directory:

* microk8s: `/var/snap/microk8s/common/var/lib/kubelet`

<8> Directory on the host that is required for building kernel modules. Only needed if using the `Compile` injection method. Defaults to `/usr/src`, which is where the actual kernel sources are stored on most distributions. Use `"none"` to not mount any additional directories.

<9> Set the number of worker threads used by the CSI driver. Higher values put more load on the LINSTOR controller, which might lead to instability when creating many volumes at once.

<10> If set to true, the satellite containers will have the following files and directories mounted from the host operating system:
+
* `/etc/drbd/drbd.conf` (file)
* `/etc/drbd.d` (directory)
* `/var/lib/drbd` (directory)
* `/var/lib/linstor.d` (directory)
+
All files and directories must already exist on the host.

[[s-kubernetes-ha-deployment]]
==== High-Availability Deployment in Operator v1

To create a high-availability deployment of all components within a LINSTOR Operator v1 deployment, consult the https://github.com/piraeusdatastore/piraeus-operator/blob/b00fd34/doc/scheduling.md[upstream guide]
The default values are chosen so that scaling the components to multiple replicas ensures that the replicas are placed on different nodes. This ensures
that a single node failures will not interrupt the service.

NOTE: If you have deployed LINBIT SDS in Kubernetes by using the LINSTOR Operator v2, high availability is built into the deployment by default.

[[s-kubernetes-ha-controller-v1]]
===== Fast Workload Failover Using the High Availability Controller

When node failures occur, Kubernetes is very conservative in rescheduling stateful workloads. This means it can
take more than 15 minutes for Pods to be moved from unreachable nodes. With the information available to DRBD and
LINSTOR, this process can be sped up significantly.

The LINSTOR High Availability Controller (HA Controller) speeds up the failover process for stateful workloads using
LINSTOR for storage. It monitors and manages any Pod that is attached to at least one DRBD resource.

For the HA Controller to work properly, you need quorum, that is at least three replicas (or two replicas + one diskless
tiebreaker). If using lower replica counts, attached Pods will be ignored and are not eligible for faster failover.

The HA Controller is packaged as a Helm chart, and can be deployed using:

----
$ helm repo update
$ helm install linstor-ha-controller linstor/linstor-ha-controller
----

If you are using the HA Controller in your cluster you can set additional parameters in all StorageClasses. These
parameters ensure that the volume is not accidentally remounted as read-only, leading to degraded Pods.

[source,yaml]
----
parameters:
  property.linstor.csi.linbit.com/DrbdOptions/auto-quorum: suspend-io
  property.linstor.csi.linbit.com/DrbdOptions/Resource/on-no-data-accessible: suspend-io
  property.linstor.csi.linbit.com/DrbdOptions/Resource/on-suspended-primary-outdated: force-secondary
  property.linstor.csi.linbit.com/DrbdOptions/Net/rr-conflict: retry-connect
----

To exempt a Pod from management by the HA Controller, add the following annotation to the Pod:

----
$ kubectl annotate pod <podname> drbd.linbit.com/ignore-fail-over=""
----

[[s-kubernetes-etcd-backup]]
==== Backing up the etcd Database

To create a backup of the etcd database (in LINSTOR Operator v1 deployments) and store it on your control host, enter the following commands:

[source]
----
kubectl exec linstor-op-etcd-0 -- etcdctl snapshot save /tmp/save.db
kubectl cp linstor-op-etcd-0:/tmp/save.db save.db
----

These commands will create a file `save.db` on the machine you are running `kubectl` from.

[[s-kubernetes-deploy-external-controller]]
=== Deploying with an External LINSTOR Controller

The Operator can configure the satellites and CSI plugin to use an existing LINSTOR setup. This can be useful in cases
where the storage infrastructure is separate from the Kubernetes cluster. Volumes can be provisioned in diskless mode
on the Kubernetes nodes while the storage nodes will provide the backing disk storage.

[[s-kubernetes-external-linstor-controller-deployment-v2]]
==== Operator v2 Deployment with an External LINSTOR Controller
// see this GL issue:
// https://gitlab.at.linbit.com/linbit/linbit-documentation/-/issues/88

The instructions in this section describe how you can connect an Operator v2 LINBIT SDS deployment to an existing LINBIST SDS cluster that you manage outside Kubernetes.

To follow the steps in this section you should be familiar with editing link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md[`LinstorCluster`] resources.

[[s-kubernetes-external-linstor-controller-deployment-configuring-linstorcluster-v2]]
===== Configuring the `LinstorCluster` Resource

To use an externally managed LINSTOR cluster, specify the URL of the LINSTOR controller in the `LinstorCluster` resource in a YAML configuration and apply it to your deployment. In the following example, the LINSTOR controller is reachable at `http://linstor-controller.example.com:3370`.

----
apiVersion: piraeus.io/v1
kind: LinstorCluster
metadata:
  name: linstorcluster
spec:
  externalController:
    url: http://linstor-controller.example.com:3370
----

NOTE: You can also specify an IP address rather than a hostname and domain for the controller.

[[s-kubernetes-external-linstor-controller-deployment-configuring-host-networking-v2]]
===== Configuring Host Networking for LINSTOR Satellites

Normally the pod network is not reachable from outside the Kubernetes cluster. In this case the external LINSTOR controller would not be able to communicate with the satellites in the Kubernetes cluster. For this reason, you need to configure your satellites to use host networking.

To use host networking, deploy a `LinstorSatelliteConfiguration` resource by applying the following YAML configuration to your deployment:

[source,yaml]
----
apiVersion: piraeus.io/v1
kind: LinstorSatelliteConfiguration
metadata:
  name: host-network
spec:
  podTemplate:
    spec:
      hostNetwork: true
----

[[s-kubernetes-external-linstor-controller-deployment-verifying-v2]]
===== Verifying an External LINSTOR Controller Configuration

You can verify that you have correctly configured your Kubernetes deployment to use an external LINSTOR controller by verifying the following:

- The `Available` condition on the `LinstorCluster` resource reports the expected URL for the
  external LINSTOR controller:
+
[%autofit]
----
$ kubectl get LinstorCluster -ojsonpath='{.items[].status.conditions[?(@.type=="Available")].message}{"\n"}'
Controller 1.20.3 (API: 1.16.0, Git: 8d19a891df018f6e3d40538d809904f024bfe361) reachable at 'http://linstor-controller.example.com:3370'
----

- The `linstor-csi-controller` deployment uses the expected URL:
+
[%autofit]
----
$ kubectl get -n linbit-sds deployment linstor-csi-controller -ojsonpath='{.spec.template.spec.containers[?(@.name=="linstor-csi")].env[?(@.name=="LS_CONTROLLERS")].value}{"\n"}'
http://linstor-controller.example.com:3370
----

- The `linstor-csi-node` deployment uses the expected URL:
+
[%autofit]
----
$ kubectl get -n linbit-sds daemonset linstor-csi-node -ojsonpath='{.spec.template.spec.containers[?(@.name=="linstor-csi")].env[?(@.name=="LS_CONTROLLERS")].value}{"\n"}'
http://linstor-controller.example.com:3370
----

- The Kubernetes nodes are registered as satellite nodes on the LINSTOR controller:
+
[%autofit]
----
$ kubectl get nodes -owide
NAME               STATUS   ROLES           AGE   VERSION   INTERNAL-IP      [...]
k8s-1-26-10.test   Ready    control-plane   22m   v1.26.3   192.168.122.10   [...]
[...]
----
+
After getting the node names from the output of the above command, verify that the node names are also LINSTOR satellites by entering a LINSTOR `node list` command on your LINSTOR controller node.
+
----
$ linstor node list

 Node              NodeType   Addresses                    State  

 k8s-1-26-10.test  SATELLITE  192.168.122.10:3366 (PLAIN)  Online 
[...]
----

[[s-kubernetes-external-linstor-controller-deployment-v1]]
==== Operator v1 Deployment with an External LINSTOR Controller

To skip the creation of a LINSTOR controller deployment and configure the other components to use your existing LINSTOR
controller, use the following options when running `helm install`:

* `operator.controller.enabled=false` This disables creation of the `LinstorController`
resource
* `operator.etcd.enabled=false` Since no LINSTOR controller will run on Kubernetes, no
database is required.
* `controllerEndpoint=<url-of-linstor-controller>` The HTTP endpoint of the existing LINSTOR
controller. For example: `http://linstor.storage.cluster:3370/`

After all pods are ready, you should see the Kubernetes cluster nodes as satellites in your LINSTOR setup.

IMPORTANT: Your Kubernetes nodes must be reachable using their IP by the controller and storage nodes.

Create a storage class referencing an existing storage pool on your storage nodes.

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: linstor-on-k8s
provisioner: linstor.csi.linbit.com
parameters:
  autoPlace: "3"
  storagePool: existing-storage-pool
  resourceGroup: linstor-on-k8s
----

You can provision new volumes by creating PVCs using your storage class. The volumes will first be placed only on nodes
with the given storage pool, that is, your storage infrastructure. Once you want to use the volume in a pod, LINSTOR CSI
will create a diskless resource on the Kubernetes node and attach over the network to the diskful resource.

[[s-kubernetes-linstor-interacting]]
=== Interacting with LINSTOR in Kubernetes

The controller pod includes a LINSTOR Client, making it easy to interact directly with LINSTOR.
For example:

----
kubectl exec deploy/linstor-controller -- linstor storage-pool list
----

[[s-kubernetes-kubectl-linstor-utility]]
==== Simplifying LINSTOR Client Command Entry

To simplify entering LINSTOR client commands within a Kubernetes deployment, you can use the
`kubectl-linstor` utility. This utility is available from the upstream Piraeus datastore
project. To download it, enter the following commands on your Kubernetes control plane node:

[%autofit]
----
# KL_VERS=0.3.0 <1>
# KL_ARCH=linux_amd64 <2>
# curl -L -O \
https://github.com/piraeusdatastore/kubectl-linstor/releases/download/v$KL_VERS/kubectl-linstor_v"$KL_VERS"_$KL_ARCH.tar.gz
----

<1> Set the shell variable `KL_VERS` to the latest release version of the `kubectl-linstor`
utility, as shown on the
https://github.com/piraeusdatastore/kubectl-linstor/releases[`kubectl-linstor` releases page].
<2> Set the shell variable `KL_ARCH` to the architecture appropriate to your deployment and
supported by the utility's available releases.

IMPORTANT: If your deployment uses the LINSTOR Operator v2, you must use version 0.2.0 or higher
of the `kubectl-linstor` utility.

NOTE: It is possible that the tar archive asset name could change over time. If you have issues downloading the asset by using the commands shown above, verify the naming convention of the asset that you want on the link:https://github.com/piraeusdatastore/kubectl-linstor/releases[`kubectl-linstor` releases page] or else manually download the asset
that you want from the releases page.

To install the utility, first extract it and then move the extracted executable file to a
directory in your `$PATH`, for example, `/usr/bin`. Then you can use `kubectl-linstor` to get
access to the complete LINSTOR CLI.

----
$ kubectl linstor node list

 Node                            NodeType    Addresses                    State  

 kube-node-01.test               SATELLITE   10.43.224.26:3366 (PLAIN)    Online 
 kube-node-02.test               SATELLITE   10.43.224.27:3366 (PLAIN)    Online 
 kube-node-03.test               SATELLITE   10.43.224.28:3366 (PLAIN)    Online 

----

It also expands references to PVCs to the matching LINSTOR resource.

----
$ kubectl linstor resource list -r pvc:my-namespace/demo-pvc-1 --all
pvc:my-namespace/demo-pvc-1 -> pvc-2f982fb4-bc05-4ee5-b15b-688b696c8526

 ResourceName  Node               Port  Usage   Conns     State    CreatedOn           

 pvc-[...]     kube-node-01.test  7000  Unused  Ok       UpToDate  2021-02-05 09:16:09 
 pvc-[...]     kube-node-02.test  7000  Unused  Ok     TieBreaker  2021-02-05 09:16:08 
 pvc-[...]     kube-node-03.test  7000  InUse   Ok       UpToDate  2021-02-05 09:16:09 

----

It also expands references of the form `pod:[<namespace>/]<podname>` into a list resources in use by the pod.

This should only be necessary for investigating problems and accessing advanced functionality.
Regular operation such as creating volumes should be achieved through the
<<s-kubernetes-basic-configuration-and-deployment,Kubernetes integration>>.

[[s-kubernetes-basic-configuration-and-deployment]]
=== Getting Started with LINBIT SDS Storage in Kubernetes

Once all linstor-csi __Pod__s are up and running, you can provision volumes
using the usual Kubernetes workflows.

Configuring the behavior and properties of LINSTOR volumes deployed through Kubernetes
is accomplished using link:https://kubernetes.io/docs/concepts/storage/storage-classes/[Kubernetes __StorageClass__] objects.

IMPORTANT: The `resourceGroup` parameter is mandatory. Because Kubernetes StorageClass objects have a one-to-one correspondence with LINSTOR resource groups, you usually want the `resourceGroup` parameter to be unique and the same as the storage class name.

Here below is the simplest practical _StorageClass_ that can be used to deploy volumes:

.linstor-basic-sc.yaml
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  # The name used to identify this StorageClass.
  name: linstor-basic-storage-class
  # The name used to match this StorageClass with a provisioner.
  # linstor.csi.linbit.com is the name that the LINSTOR CSI plugin uses to identify itself
provisioner: linstor.csi.linbit.com
volumeBindingMode: WaitForFirstConsumer
parameters:
  # LINSTOR will provision volumes from the drbdpool storage pool configured
  # On the satellite nodes in the LINSTOR cluster specified in the plugin's deployment
  storagePool: "lvm-thin"
  resourceGroup: "linstor-basic-storage-class"
  # Setting a fstype is required for "fsGroup" permissions to work correctly.
  # Currently supported: xfs/ext4
  csi.storage.k8s.io/fstype: xfs
----

IMPORTANT: The `storagePool` value, `lvm-thin` in the example YAML configuration file above, must match an available LINSTOR _StoragePool_. You can list storage pool information using the `linstor storage-pool list` command, executed within the running `linstor-op-cs-controller` pod, or by using the `kubectl linstor storage-pool list` command if you have installed the <<s-kubernetes-kubectl-linstor-utility,`kubectl-linstor` utility>>.

You can create the storage class with the following command:

----
kubectl create -f linstor-basic-sc.yaml
----

Now that your storage class is created, you can now create a persistent volume claim (PVC)
which can be used to provision volumes known both to Kubernetes and LINSTOR:

.my-first-linstor-volume-pvc.yaml
[source,yaml]
----
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-first-linstor-volume
spec:
  storageClassName: linstor-basic-storage-class
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
----

You can create the _PersistentVolumeClaim_ with the following command:

----
kubectl create -f my-first-linstor-volume-pvc.yaml
----

This will create a _PersistentVolumeClaim_, but no volume will be created just yet.
The storage class we used specified `volumeBindingMode: WaitForFirstConsumer`, which
means that the volume is only created once a workload starts using it. This ensures
that the volume is placed on the same node as the workload.

For our example, we create a simple Pod, which mounts or volume by referencing the
_PersistentVolumeClaim_.
.my-first-linstor-volume-pod.yaml
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: fedora
  namespace: default
spec:
  containers:
  - name: fedora
    image: fedora
    command: [/bin/bash]
    args: ["-c", "while true; do sleep 10; done"]
    volumeMounts:
    - name: my-first-linstor-volume
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: my-first-linstor-volume
    persistentVolumeClaim:
      claimName: "my-first-linstor-volume"
----

You can create the _Pod_ with the following command:

----
kubectl create -f my-first-linstor-volume-pod.yaml
----

Running `kubectl describe pod fedora` can be used to confirm that _Pod_
scheduling and volume attachment succeeded. Examining the _PersistentVolumeClaim_,
we can see that it is now bound to a volume.

To remove a volume, verify that no pod is using it and then delete the
_PersistentVolumeClaim_ using the `kubectl` command. For example, to remove the volume that we
just made, run the following two commands, noting that the _Pod_ must be
unscheduled before the _PersistentVolumeClaim_ will be removed:

----
kubectl delete pod fedora # unschedule the pod.

kubectl get pod -w # wait for pod to be unscheduled

kubectl delete pvc my-first-linstor-volume # remove the PersistentVolumeClaim, the PersistentVolume, and the LINSTOR Volume.
----

[[s-kubernetes-sc-parameters]]
==== Available Parameters in a Storage Class

The following storage class contains all currently available parameters to configure the provisioned storage.

NOTE: `linstor.csi.linbit.com/` is an optional, but recommended prefix for LINSTOR CSI specific parameters.

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: full-example
provisioner: linstor.csi.linbit.com
parameters:
  # CSI related parameters
  csi.storage.k8s.io/fstype: xfs
  # LINSTOR parameters
  linstor.csi.linbit.com/autoPlace: "2"
  linstor.csi.linbit.com/placementCount: "2"
  linstor.csi.linbit.com/resourceGroup: "full-example"
  linstor.csi.linbit.com/storagePool: "my-storage-pool"
  linstor.csi.linbit.com/disklessStoragePool: "DfltDisklessStorPool"
  linstor.csi.linbit.com/layerList: "drbd storage"
  linstor.csi.linbit.com/placementPolicy: "AutoPlaceTopology"
  linstor.csi.linbit.com/allowRemoteVolumeAccess: "true"
  linstor.csi.linbit.com/encryption: "true"
  linstor.csi.linbit.com/nodeList: "diskful-a diskful-b"
  linstor.csi.linbit.com/clientList: "diskless-a diskless-b"
  linstor.csi.linbit.com/replicasOnSame: "zone=a"
  linstor.csi.linbit.com/replicasOnDifferent: "rack"
  linstor.csi.linbit.com/disklessOnRemaining: "false"
  linstor.csi.linbit.com/doNotPlaceWithRegex: "tainted.*"
  linstor.csi.linbit.com/fsOpts: "-E nodiscard"
  linstor.csi.linbit.com/mountOpts: "noatime"
  linstor.csi.linbit.com/postMountXfsOpts: "extsize 2m"
  # Linstor properties
  property.linstor.csi.linbit.com/*: <x>
  # DRBD parameters
  DrbdOptions/*: <x>
----

[[s-kubernetes-drbd-options-setting]]
==== Setting DRBD Options for Storage Resources in Kubernetes

As shown in the <<s-kubernetes-sc-parameters,>> section, you can set DRBD options within a
storage class configuration. Because of the one-to-one correspondence between a StorageClass
Kubernetes object and its named LINSTOR resource group (`resourceGroup` parameter), setting DRBD
options within a storage class configuration is similar to setting DRBD options on the LINSTOR
resource group.

There are some differences. If you set DRBD options within a storage class configuration, these
options will only affect new volumes that are created from the storage class. The options will
not affect existing volumes. Furthermore, because you cannot just update the storage class, you
will have to delete it and create it again if you add DRBD options to the storage class's
configuration. If you set DRBD options on the LINSTOR resource group object (`linstor
resource-group set-property <rg-name> DrbdOptions/<option-name>`), changes will affect future
and existing volumes belonging to the resource group.

WARNING: If you set DRBD options on a LINSTOR resource group that also corresponds to a
Kubernetes storage class, the next time a volume is created, the CSI driver will revert changes
to DRBD options that are only in the resource group, unless you also configure the DRBD options
in the storage class.

Because of the potential pitfalls here, it is simpler to set DRBD options on the LINSTOR
controller object. DRBD options set on the controller will apply to all resources groups,
resources, and volumes, unless you have also set the same options on any of those LINSTOR
objects. Refer to the <<s-linstor-introduction-linstor-object-hierarchy,>> section for more
details about LINSTOR object hierarchy.

You can list the properties, including DRBD options, that you can set on the LINSTOR controller
object by entering the following command:

----
# kubectl exec -n linbit-sds deployment/linstor-controller -- \
linstor controller set-property --help
----

[[s-kubernetes-drbd-options-setting-on-controller]]
===== Setting DRBD Options on the LINSTOR Controller in Kubernetes

To set DRBD options on the LINSTOR controller in a Kubernetes deployment, edit the
link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md#specproperties[`LinstorCluster`
configuration]. For example, to set transport encryption for all DRBD traffic:

----
apiVersion: piraeus.io/v1
kind: LinstorCluster
metadata:
  name: linstorcluster
spec:
  properties:
    # This one will be set on the controller, verify with: linstor controller list-properties
    # Enable TLS for all resources by default
    - name: "DrbdOptions/Net/tls"
      value: "yes"
----

After editing the `LinstorCluster` configuration, apply it to your deployment by entering
`kubectl apply -f <configuration-file>`.

[[s-kubernetes-drbd-options-setting-on-node-connection]]
===== Setting DRBD Options on a LINSTOR Node Connection in Kubernetes

You can set DRBD options on LINSTOR node connections in Kubernetes, by editing the Kubernetes
link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstornodeconnection.md#specproperties[`LinstorNodeConnection`]
configuration. Instructions are similar for editing and applying a `LinstorCluster`
configuration, described in the previous section.

DRBD options set at the node connection level will take precedence over DRBD options set at the
controller level and satellite node levels.

The following is an example node connection configuration that will do two things:

. Select pairs of nodes (a node connection by definition connects two nodes) that are in
  different geographical regions, for example, two data centers.
. Set a DRBD option to make DRBD use
  link:https://linbit.com/drbd-user-guide/drbd-guide-9_0-en/#s-replication-protocols[protocol
  A] (asynchronous replication) on the node level connection between nodes that match the
  selection criterion.

----
apiVersion: piraeus.io/v1
kind: LinstorNodeConnection
metadata:
  name: cross-region
spec:
  selector:
    # Select pairs of nodes (A, B) where A is in a different region than node B.
    - matchLabels:
        - key: topology.kubernetes.io/region
          op: NotSame
  properties:
    # This property will be set on the node connection, verify with:
    # linstor node-connection list-properties <node1> <node2>
    # Configure DRBD protocol A between regions for reduced latency
    - name: DrbdOptions/Net/protocol
      value: A
----

NOTE: Refer to
link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstornodeconnection.md#specselector[documentation
within the upstream Piraeus project] for more details about the node connection `selector`
specification.

[[s-kubernetes-drbd-options-setting-on-satellite]]
===== Setting DRBD Options on LINSTOR Satellite Nodes in Kubernetes

You can set DRBD options on LINSTOR satellite nodes in Kubernetes, by editing the Kubernetes
`LinstorSatelliteConfiguration`] configuration. Instructions are similar for editing and
applying a `LinstorCluster` or `LinstorNodeConnection` configuration, described in the previous
sections.

DRBD options set at the satellite node level will take precedence over DRBD options set at the
controller level.

To set a DRBD option that would prevent LINSTOR from automatically evicting a node, you could
use the following configuration file, provided that you apply an `under-maintenance` label to
the node that you want to disable the automatic eviction feature for. This might be useful
during a system maintenance window on a node.

----
apiVersion: piraeus.io/v1
kind: LinstorSatelliteConfiguration
metadata:
  name: nodes-under-maintenance
spec:
  nodeSelector:
    under-maintenance: "yes"
  properties:
    - name: DrbdOptions/AutoEvictAllowEviction
      value: "false"
----

[[s-kubernetes-linstor-properties-setting-on-storage-pools]]
====== Setting LINSTOR Properties on LINSTOR Storage Pools in Kubernetes

Additionally, you can specify LINSTOR properties (not DRBD options) on LINSTOR storage pools that
might exist on LINSTOR satellite nodes, as shown in the example configuration that follows.

The example configuration would apply to all LINSTOR satellite nodes in your Kubernetes
deployment. However, it is possible to select only certain nodes within a configuration, similar
to the configuration example in <<s-kubernetes-drbd-options-setting-on-satellite,>>. Refer to
link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md#linstorsatelliteconfiguration[documentation
in the upstream Piraeus project] for details.

----
apiVersion: piraeus.io/v1
kind: LinstorSatelliteConfiguration
spec:
  storagePools:
    - name: pool1
      lvmThinPool: {}
      properties:
        # This one will be set on the storage pool, verify with:
        # linstor storage-pool list-properties <node> <pool>
        # Set the oversubscription ratio on the storage pool to 1, i.e. no oversubscription.
        - name: MaxOversubscriptionRatio
          value: "1"
----

[[s-kubernetes-file-system]]
==== `csi.storage.k8s.io/fstype`

The `csi.storage.k8s.io/fstype` parameter sets the file system type to create for `volumeMode: FileSystem` PVCs. Currently supported are:

* `ext4` (default)
* `xfs`

[[s-kubernetes-autoplace]]
==== `autoPlace`

`autoPlace` is an integer that determines the amount of replicas a volume of
this _StorageClass_ will have. For example, `autoPlace: "3"` will produce
volumes with three-way replication. If neither `autoPlace` nor `nodeList` are
set, volumes will be <<s-autoplace-linstor,automatically placed>> on one node.

IMPORTANT: If you use this option, you must not use <<s-kubernetes-nodelist,`nodeList`>>.

IMPORTANT: You have to use quotes, otherwise Kubernetes will complain about a malformed _StorageClass_.

TIP: This option (and all options which affect auto-placement behavior) modifies the
number of LINSTOR nodes on which the underlying storage for volumes will be
provisioned and is orthogonal to which _kubelets_ those volumes will be accessible
from.

==== `placementCount`

`placementCount` is an alias for <<s-kubernetes-autoplace,`autoPlace`>>

==== `resourceGroup`

The <<s-linstor-resource-groups, LINSTOR Resource Group (RG)>> to associate with this StorageClass. If not set,
a new RG will be created for each new PVC.

[[s-kubernetes-storagepool]]
==== `storagePool`

`storagePool` is the name of the LINSTOR <<s-storage_pools,storage pool>> that
will be used to provide storage to the newly-created volumes.

CAUTION: Only nodes configured with this same _storage pool_ with be considered
for <<s-kubernetes-autoplace,auto-placement>>. Likewise, for _StorageClasses_ using
<<s-kubernetes-nodelist,`nodeList`>> all nodes specified in that list must have this
_storage pool_ configured on them.

[[s-kubernetes-disklessstoragepool]]
==== `disklessStoragePool`

`disklessStoragePool` is an optional parameter that only affects LINSTOR volumes
that are assigned as "diskless" to _kubelets_, that is, as clients. If you have a custom
diskless storage pool defined in LINSTOR, you will specify that here.

==== `layerList`

A comma-separated list of layers to use for the created volumes. The available layers and their order are described
towards the end of <<s-linstor-without-drbd, this section>>. Defaults to `drbd,storage`

[[s-kubernetes-placementpolicy]]
==== `placementPolicy`

Select from one of the available volume schedulers:

* `AutoPlaceTopology`, the default: Use topology information from Kubernetes together with
user provided constraints (see <<s-kubernetes-replicasonsame>> and
<<s-kubernetes-replicasondifferent>>).
* `AutoPlace` Use the LINSTOR auto-placement feature, influenced by <<s-kubernetes-replicasonsame>> and
<<s-kubernetes-replicasondifferent>>
* `FollowTopology`: Use CSI Topology information to place at least one volume in each
"preferred" zone. Only usable if CSI Topology is enabled.
* `Manual`: Use only the nodes listed in `nodeList` and `clientList`.
* `Balanced`: **EXPERIMENTAL** Place volumes across failure domains, using the least used
storage pool on each selected node.

[[s-kubernetes-params-allow-remote-volume-access]]
==== `allowRemoteVolumeAccess`

Control on which nodes a volume is accessible. The value for this option can take two different forms:

- A simple `"true"` or `"false"` allows access from all nodes, or only those nodes with
  diskful resources.

- Advanced rules, which allow more granular rules on which nodes can access the volume.
+
The current implementation can grant access to the volume for nodes that share the same labels. For example, if you want
to allow access from all nodes in the same region and zone as a diskful resource, you could use:
+
[source,yaml]
----
parameters:
  linstor.csi.linbit.com/allowRemoteVolumeAccess: |
    - fromSame:
      - topology.kubernetes.io/region
      - topology.kubernetes.io/zone
----
+
You can specify multiple rules. The rules are additive, a node only need to match one rule to be assignable.

[[s-kubernetes-encryption]]
==== `encryption`

`encryption` is an optional parameter that determines whether to encrypt
volumes. LINSTOR must be <<s-linstor-encrypted-volumes,configured for encryption>>
for this to work properly.

[[s-kubernetes-nodelist]]
==== `nodeList`

`nodeList` is a list of nodes for volumes to be assigned to. This will assign
the volume to each node and it will be replicated among all of them. This
can also be used to select a single node by hostname, but it's more flexible to use
<<s-kubernetes-replicasonsame,replicasOnSame>> to select a single node.

IMPORTANT: If you use this option, you must not use <<s-kubernetes-autoplace,`autoPlace`>>.

TIP: This option determines on which LINSTOR nodes the underlying storage for volumes
will be provisioned and is orthogonal from which _kubelets_ these volumes will be
accessible.

==== `clientList`

`clientList` is a list of nodes for diskless volumes to be assigned to. Use in conjunction with <<s-kubernetes-nodelist>>.

[[s-kubernetes-replicasonsame]]
==== `replicasOnSame`

// These should link to the linstor documentation about node properties, but those
// do not exist at the time of this commit.
`replicasOnSame` is a list of `key` or `key=value` items used as auto-placement selection
labels when <<s-kubernetes-autoplace,`autoPlace`>> is used to determine where to
provision storage. These labels correspond to LINSTOR node properties.

NOTE: The operator periodically synchronizes all labels from Kubernetes Nodes, so you can use them as keys for
scheduling constraints.

Let's explore this behavior with examples assuming a LINSTOR cluster such that `node-a` is configured with the
following auxiliary property `zone=z1` and `role=backups`, while `node-b` is configured with
only `zone=z1`.

If we configure a _StorageClass_ with `autoPlace: "1"` and `replicasOnSame: "zone=z1 role=backups"`,
then all volumes created from that _StorageClass_ will be provisioned on `node-a`,
since that is the only node with all of the correct key=value pairs in the LINSTOR
cluster. This is the most flexible way to select a single node for provisioning.

IMPORTANT: This guide assumes LINSTOR CSI version 0.10.0 or newer. All properties referenced in `replicasOnSame`
and `replicasOnDifferent` are interpreted as auxiliary properties. If you are using an older version of LINSTOR CSI, you
need to add the `Aux/` prefix to all property names. So `replicasOnSame: "zone=z1"` would be `replicasOnSame: "Aux/zone=z1"`
Using `Aux/` manually will continue to work on newer LINSTOR CSI versions.

If we configure a _StorageClass_ with `autoPlace: "1"` and `replicasOnSame: "zone=z1"`,
then volumes will be provisioned on either `node-a` or `node-b` as they both have
the `zone=z1` aux prop.

If we configure a _StorageClass_ with `autoPlace: "2"` and `replicasOnSame: "zone=z1 role=backups"`,
then provisioning will fail, as there are not two or more nodes that have
the appropriate auxiliary properties.

If we configure a _StorageClass_ with `autoPlace: "2"` and `replicasOnSame: "zone=z1"`,
then volumes will be provisioned on both `node-a` and `node-b` as they both have
the `zone=z1` aux prop.

You can also use a property key without providing a value to ensure all replicas are placed on nodes with the same property value,
with caring about the particular value. Assuming there are 4 nodes, `node-a1` and `node-a2` are configured with `zone=a`. `node-b1` and `node-b2`
are configured with `zone=b`. Using `autoPlace: "2"` and `replicasOnSame: "zone"` will place on either `node-a1` and `node-a2` OR on `node-b1` and `node-b2`.

[[s-kubernetes-replicasondifferent]]
==== `replicasOnDifferent`

`replicasOnDifferent` takes a list of properties to consider, same as <<s-kubernetes-replicasonsame,replicasOnSame>>.
There are two modes of using `replicasOnDifferent`:

* Preventing volume placement on specific nodes:
+
If a value is given for the property, the nodes which have that property-value pair assigned will be considered last.
+
Example: `replicasOnDifferent: "no-csi-volumes=true"` will place no volume on any node with property
`no-csi-volumes=true` unless there are not enough other nodes to fulfill the `autoPlace` setting.

* Distribute volumes across nodes with different values for the same key:
+
If no property value is given, LINSTOR will place the volumes across nodes with different values for that property if
possible.
+
Example: Assuming there are 4 nodes, `node-a1` and `node-a2` are configured with `zone=a`. `node-b1` and `node-b2`
are configured with `zone=b`. Using a _StorageClass_ with `autoPlace: "2"` and `replicasOnDifferent: "zone"`,
LINSTOR will create one replica on either `node-a1` or `node-a2` _and_ one replica on either `node-b1` or `node-b2`.

==== `disklessOnRemaining`

Create a diskless resource on _all_ nodes that were not assigned a diskful resource.

==== `doNotPlaceWithRegex`

Do not place the resource on a node which has a resource with a name matching the regular expression.

[[s-kubernetes-fsops]]
==== `fsOpts`
`fsOpts` is an optional parameter that passes options to the volume's
file system at creation time.

IMPORTANT: These values are specific to your chosen
<<s-kubernetes-file-system, file system>>.

[[s-kubernetes-mountops]]
==== `mountOpts`
`mountOpts` is an optional parameter that passes options to the volume's
file system at mount time.

==== `postMountXfsOpts`

Extra arguments to pass to `xfs_io`, which gets called before right before first use of the volume.

[[s-kubernetes-storage-class-properties]]
==== `property.linstor.csi.linbit.com/*`

Parameters starting with `property.linstor.csi.linbit.com/` are translated to LINSTOR properties that are set on the
<<s-linstor-resource-groups,Resource Group>> associated with the StorageClass.

For example, to set `DrbdOptions/auto-quorum` to `disabled`, use:

----
property.linstor.csi.linbit.com/DrbdOptions/auto-quorum: disabled
----

The full list of options is available https://app.swaggerhub.com/apis-docs/Linstor/Linstor/1.7.0#/developers/resourceDefinitionModify[here]

====  `DrbdOptions/*: <x>`

NOTE: This option is deprecated, use the more general <<s-kubernetes-storage-class-properties, `property.linstor.csi.linbit.com/*`>> form.

Advanced DRBD options to pass to LINSTOR. For example, to change the replication protocol, use
`DrbdOptions/Net/protocol: "A"`.

[[s-kubernetes-snapshots]]
=== Snapshots

Snapshots create a copy of the volume content at a particular point in time. This copy remains untouched when you make modifications to the volume content. This, for example, enables you to create backups of your data before performing modifications or deletions on your data.

Because a backup is useless unless you have a way to restore it, this section describes how to create a snapshot, and how to restore it, for example, in the case of accidental deletion of your data.

The next subsection contains instructions around snapshots within Operator v2 deployments. If you have deployed LINBIT SDS in Kubernetes by using Operator v1, skip ahead to the <<s-kubernetes-add-snaphot-support-v1>> subsection.

[[s-kubernetes-snapshots-working-with]]
==== Working With Snapshots

Before you can add snapshot support within a LINBIT SDS deployment, you need to meet the following environment prerequisites:

- Your cluster has a storage pool supporting snapshots. LINSTOR supports snapshots for `LVM_THIN`, `ZFS` and `ZFS_THIN` pools. Snapshots are also supported on `FILE_THIN` pools backed by a file system with `reflinks` support, such as XFS or Btrfs.

- You have a `StorageClass`, `PersistentVolumeClaim`, and `Deployment` that uses a storage pool
  that supports snapshots.

- Your cluster has a CSI snapshotter (link:https://github.com/kubernetes-csi/external-snapshotter/[`snapshot-controller`]) deployed. To verify if it is already deployed, you can enter the following command:
+
----
$ kubectl api-resources --api-group=snapshot.storage.k8s.io -oname
----
+
Output should be similar to the following if a snapshot controller is already deployed:
+
----
volumesnapshotclasses.snapshot.storage.k8s.io
volumesnapshotcontents.snapshot.storage.k8s.io
volumesnapshots.snapshot.storage.k8s.io
----
+
If output from the command is empty, you can deploy a snapshot controller by entering the following commands:
+
[%autofit]
----
$ kubectl apply -k https://github.com/kubernetes-csi/external-snapshotter//client/config/crd
$ kubectl apply -k https://github.com/kubernetes-csi/external-snapshotter//deploy/kubernetes/snapshot-controller
----

[[s-kubernetes-snapshot-creating]]
===== Creating a Snapshot

To create a volume snapshot, you first need to create a volume snapshot class (link:https://kubernetes.io/docs/concepts/storage/volume-snapshot-classes/[`VolumeSnapshotClass`]). This volume snapshot class will specify the `linstor.csi.linbit.com` provisioner, and sets the clean-up policy for the snapshots to `Delete`. This means that deleting the Kubernetes resources will also delete the snapshots in LINSTOR.

You can create a volume snapshot class by entering the following command:

----
$ kubectl apply -f - <<EOF
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: linbit-sds-snapshots
driver: linstor.csi.linbit.com
deletionPolicy: Delete
EOF
----

To create a snapshot, you create a link:https://kubernetes.io/docs/concepts/storage/volume-snapshots/#volumesnapshots[`VolumeSnapshot`] resource. The `VolumeSnapshot` resource needs to reference a snapshot-compatible `PersistentVolumeClaim` resource, and the `VolumeSnapshotClass` that you just created. For example, you could create a snapshot (named `data-volume-snapshot-1`) of a PVC named `data-volume` by entering the following command:

----
$ kubectl apply -f - <<EOF
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: data-volume-snapshot-1
spec:
  volumeSnapshotClassName: linbit-sds-snapshots
  source:
    persistentVolumeClaimName: data-volume
EOF
----

[[s-kubernetes-snapshot-verifying-creation]]
===== Verifying Snapshot Creation

You can verify the creation of a snapshot by entering the following commands:

----
$ kubectl wait volumesnapshot --for=jsonpath='{.status.readyToUse}'=true data-volume-snapshot-1
volumesnapshot.snapshot.storage.k8s.io/data-volume-snapshot-1 condition met
$ kubectl get volumesnapshot data-volume-snapshot-1
----

Output should show a table of information about the volume snapshot resource, similar to the following:

[%autofit]
----
NAME                     READYTOUSE   SOURCEPVC     SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS
data-volume-snapshot-1   true         data-volume                           1Gi           linbit-sds-snapshots
----

You can further verify the snapshot in LINSTOR, by entering the following command:

----
$ kubectl -n linbit-sds exec deploy/linstor-controller -- linstor snapshot list
----

Output should show a table similar to the following:

----
+-----------------------------------------------------------------------------------------+
| ResourceName | SnapshotName   | NodeNames | Volumes  | CreatedOn           | State      |
|=========================================================================================|
| pvc-[...]    | snapshot-[...] | kube-0    | 0: 1 GiB | 2023-02-13 15:36:18 | Successful |
+-----------------------------------------------------------------------------------------+
----

[[s-kubernetes-snapshots-restoring]]
===== Restoring a Snapshot

To restore a snapshot, you will need to create a new PVC to recover the volume snapshot to. You will replace the existing PVC, named `data-volume` in this example, with a new version based on the snapshot.

First, stop the deployment that uses the `data-volume` PVC. In this example, the deployment is named `volume-logger`.

----
$ kubectl scale deploy/volume-logger --replicas=0
deployment.apps "volume-logger" deleted
$ kubectl rollout status deploy/volume-logger
deployment "volume-logger" successfully rolled out
----

Next, remove the PVC. You still have the snapshot resource, so this is a safe operation.

----
$ kubectl delete pvc/data-volume
persistentvolumeclaim "data-volume" deleted
----

Next, create a new PVC by referencing a previously created snapshot. This will create a volume which uses the data from the referenced snapshot.

----
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-volume
spec:
  storageClassName: linbit-sds-storage
  resources:
    requests:
      storage: 1Gi
  dataSource:
    apiGroup: snapshot.storage.k8s.io
    kind: VolumeSnapshot
    name: data-volume-snapshot-1
  accessModes:
    - ReadWriteOnce
EOF
----

Because you named the new volume, `data-volume`, the same as the previous volume, you can just scale up the `Deployment` again, and the new pod will start using the restored volume.

----
$ kubectl scale deploy/volume-logger --replicas=1
deployment.apps/volume-logger scaled
----

==== Storing Snapshots on S3 Storage

LINSTOR can store snapshots on S3 compatible storage for disaster recovery. This is integrated in Kubernetes using
a special VolumeSnapshotClass:

[source,yaml]
----
---
kind: VolumeSnapshotClass
apiVersion: snapshot.storage.k8s.io/v1
metadata:
  name: linstor-csi-snapshot-class-s3
driver: linstor.csi.linbit.com
deletionPolicy: Retain
parameters:
  snap.linstor.csi.linbit.com/type: S3
  snap.linstor.csi.linbit.com/remote-name: backup-remote
  snap.linstor.csi.linbit.com/allow-incremental: "false"
  snap.linstor.csi.linbit.com/s3-bucket: snapshot-bucket
  snap.linstor.csi.linbit.com/s3-endpoint: s3.us-west-1.amazonaws.com
  snap.linstor.csi.linbit.com/s3-signing-region: us-west-1
  snap.linstor.csi.linbit.com/s3-use-path-style: "false"
  # Refer here to the secret that holds access and secret key for the S3 endpoint.
  # See below for an example.
  csi.storage.k8s.io/snapshotter-secret-name: linstor-csi-s3-access
  csi.storage.k8s.io/snapshotter-secret-namespace: storage
---
kind: Secret
apiVersion: v1
metadata:
  name: linstor-csi-s3-access
  namespace: storage
immutable: true
type: linstor.csi.linbit.com/s3-credentials.v1
stringData:
  access-key: access-key
  secret-key: secret-key
----

Refer to the instructions in the <<s-linstor-snapshots-shipping>> section for the exact meaning of the
`snap.linstor.csi.linbit.com/` parameters. The credentials used to log in are stored in a separate secret, as show in
the example above.

Referencing the above storage class when creating snapshots causes the snapshots to be automatically uploaded to the
configured S3 storage.

===== Restoring From Remote Snapshots

Restoring from remote snapshots is an important step in disaster recovery. A snapshot needs to be registered with
Kubernetes before it can be used to restore.

If the snapshot that should be restored is part of a backup to S3, the LINSTOR "remote" needs to be configured first.

----
linstor remote create s3 backup-remote s3.us-west-1.amazonaws.com \
  snapshot-bucket us-west-1 access-key secret-key
linstor backup list backup-remote
----

The snapshot you want to register needs to be one of the listed snapshots.

To register the snapshot with Kubernetes, you need to create two resources, one VolumeSnapshotContent referencing the
ID of the snapshot and one VolumeSnapshot, referencing the content.

[source,yaml]
----
---
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: example-backup-from-s3
  namespace: project
spec:
  source:
    volumeSnapshotContentName: restored-snap-content-from-s3
  volumeSnapshotClassName: linstor-csi-snapshot-class-s3
---
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotContent
metadata:
  name: restored-snap-content-from-s3
spec:
  deletionPolicy: Delete
  driver: linstor.csi.linbit.com
  source:
    snapshotHandle: snapshot-id
  volumeSnapshotClassName: linstor-csi-snapshot-class-s3
  volumeSnapshotRef:
    apiVersion: snapshot.storage.k8s.io/v1
    kind: VolumeSnapshot
    name: example-backup-from-s3
    namespace: project
----

Once applied, the VolumeSnapshot should be shown as `ready`, at which point you can reference it as a `dataSource` in a
PVC.

[[s-kubernetes-volume-accessibility-and-locality]]
=== Volume Accessibility and Locality
// This only covers DRBD volumes, section might change if linked docs are updated.
LINSTOR volumes are typically accessible both locally and <<s-drbd_clients,over the network>>. The CSI driver will
ensure that the volume is accessible on whatever node was selected for the consumer. The driver also provides options
to ensure volume locality (the consumer is placed on the same node as the backing data) and restrict accessibility
(only a subset of nodes can access the volume over the network).

Volume locality is achieved by setting `volumeBindingMode: WaitForFirstConsumer` in the storage class. This tell
Kubernetes and the CSI driver to wait until the first consumer (Pod) referencing the PVC is scheduled. The CSI driver
then provisions the volume with backing data on the same node as the consumer. In case a node without appropriate
storage pool was selected, a replacement node in the set of accessible nodes is chosen (see below).

Volume accessibility is controlled by the
<<s-kubernetes-params-allow-remote-volume-access,`allowRemoteVolumeAccess` parameter>>. Whenever the CSI plugin needs to
place a volume, this parameter is consulted to get the set of "accessible" nodes. This means they can share volumes
placed on them through the network. This information is also propagated to Kubernetes using label selectors on the PV.

==== Volume Accessibility and Locality Examples

The following example show common scenarios where you want to optimize volume accessibility and locality. It also
includes examples of how to spread volume replicas across zones in a cluster.

===== Single-Zone Homogeneous Clusters

The cluster only spans a single zone, so latency between nodes is low. The cluster is homogeneous, that is, all nodes
are configured similarly. All nodes have their own local storage pool.

.example-storage-class.yaml
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: linstor-storage
provisioner: linstor.csi.linbit.com
volumeBindingMode: WaitForFirstConsumer <1>
parameters:
  linstor.csi.linbit.com/storagePool: linstor-pool <2>
  linstor.csi.linbit.com/placementCount: "2" <3>
  linstor.csi.linbit.com/allowRemoteVolumeAccess: "true" <4>
----

<1> Enable late volume binding. This places one replica on the same node as the first
consuming pod, if possible.

<2> Set the storage pool(s) to use.

<3> Ensure that the data is replicated, so that at least 2 nodes store the data.

<4> Allow using the volume even on nodes without replica. Since all nodes are connected
equally, performance impact should be manageable.

===== Multi-Zonal Homogeneous Clusters

As before, in our homogeneous cluster all nodes are configured similarly with their own local storage pool. The cluster
spans now multiple zones, with increased latency across nodes in different zones. To ensure low latency, we want
to restrict access to the volume with a local replica to only those zones that do have a replica. At the same time,
we want to spread our data across multiple zones.

.example-storage-class.yaml
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: linstor-storage
provisioner: linstor.csi.linbit.com
volumeBindingMode: WaitForFirstConsumer <1>
parameters:
  linstor.csi.linbit.com/storagePool: linstor-pool <2>
  linstor.csi.linbit.com/placementCount: "2" <3>
  linstor.csi.linbit.com/allowRemoteVolumeAccess: | <4>
    - fromSame:
      - topology.kubernetes.io/zone
  linstor.csi.linbit.com/replicasOnDifferent: topology.kubernetes.io/zone <5>
----

<1> Enable late volume binding. This places one replica on the same node as the first
consuming pod, if possible.

<2> Set the storage pool(s) to use.

<3> Ensure that the data is replicated, so that at least 2 nodes store the data.

<4> Allow using the volume on nodes in the same zone as a replica, under the assumption that
zone internal networking is fast and low latency.

<5> Spread the replicas across different zones.

===== Multi-Region Clusters

If your cluster spans multiple regions, you do not want to incur the latency penalty to replicate your data across regions. To accomplish this, you can configure your storage class to just replicate data in the same zone.

.example-storage-class.yaml
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: linstor-storage
provisioner: linstor.csi.linbit.com
volumeBindingMode: WaitForFirstConsumer <1>
parameters:
  linstor.csi.linbit.com/storagePool: linstor-pool <2>
  linstor.csi.linbit.com/placementCount: "2" <3>
  linstor.csi.linbit.com/allowRemoteVolumeAccess: | <4>
    - fromSame:
      - topology.kubernetes.io/zone
  linstor.csi.linbit.com/replicasOnSame: topology.kubernetes.io/region <5>
----

<1> Enable late volume binding. This places one replica on the same node as the first
consuming pod, if possible.

<2> Set the storage pool(s) to use.

<3> Ensure that the data is replicated, so that at least 2 nodes store the data.

<4> Allow using the volume on nodes in the same zone as a replica, under the assumption that
zone internal networking is fast and low latency.

<5> Restrict replicas to only a single region.

===== Cluster with External Storage

Our cluster now only consists of compute nodes without local storage. Any volume access has to occur through remote
volume access.

.example-storage-class.yaml
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: linstor-storage
provisioner: linstor.csi.linbit.com
parameters:
  linstor.csi.linbit.com/storagePool: linstor-pool <1>
  linstor.csi.linbit.com/placementCount: "1" <2>
  linstor.csi.linbit.com/allowRemoteVolumeAccess: "true" <3>
----

<1> Set the storage pool(s) to use.

<2> Assuming we only have one storage host, we can only place a single volume without
additional replicas.

<3> Our worker nodes need to be allowed to connect to the external storage host.

[[s-kubernetes-affinity-controller]]
=== LINSTOR Affinity Controller

Volume Accessibility is controlled by the https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity[node affinity]
of the PersistentVolume (PV). This affinity is static, that is once defined it cannot be changed.

This can be an issue if you want to use a strict affinity: Your PV is pinned to specific nodes, but you might want
to remove or add nodes. While LINSTOR can move the volume (for example: this happens automatically if you remove a node
in Kubernetes), the PV affinity is not updated to reflect this.

This is where the LINSTOR Affinity Controller comes in: it watches PVs and compares their affinity with the volumes' states
in LINSTOR. If they go out of sync, the PV is replaced with an updated version.

The LINSTOR Affinity Controller is packaged in a Helm chart. If you install it in the same namespace as the Operator, simply
run:

----
$ helm repo update
$ helm install linstor-affinity-controller linstor/linstor-affinity-controller
----

Additional options for the chart are available at the https://github.com/piraeusdatastore/linstor-affinity-controller[upstream project].

[[s-kubernetes-volume-locality-optimization]]
=== Volume Locality Optimization

To ensure that pods are scheduled on nodes with local replicas of data, to reduce
latency for read operations, you can take the following steps:

1. Use a StorageClass with `volumeBindingMode: WaitForFirstConsumer`.
2. Set `allowRemoteVolumeAccess: "false"` in the StorageClass parameters.

Optionally, you can also deploy the [LINSTOR Affinity Controller](https://artifacthub.io/packages/helm/piraeus-charts/linstor-affinity-controller).

[[s-kubernetes-drbd-module-loader-configuring-v2]]
=== Configuring the DRBD Module Loader in Operator v2 Deployments
// https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/how-to/drbd-loader.md

NOTE: To follow the steps in this section, you should be familiar with editing
link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md#linstorsatelliteconfiguration[`LinstorSatelliteConfiguration`]
resources.

The DRBD module loader is the component responsible for making the DRBD kernel module available,
in addition to loading other useful kernel modules for LINBIT SDS in Kubernetes. This section
describes how you can configure various aspects of the DRBD kernel module loader, within a
LINSTOR Operator v2 deployment.

Besides the DRBD kernel module, these modules are also loaded if available:

[cols="1,1"]
|===
|Module | Purpose

| `libcrc32c` | dependency for DRBD
| `nvmet_rdma`, `nvme_rdma` | LINSTOR NVME layer
| `loop` | LINSTOR when using loop devices as backing disks
| `dm_writecache` | LINSTOR writecache layer
| `dm_cache` | LINSTOR cache layer
| `dm_thin_pool` | LINSTOR thin-provisioned storage
| `dm_snapshot` | LINSTOR Snapshots
| `dm_crypt` | LINSTOR encrypted volumes
|===

[[s-kubernetes-drbd-module-loader-disabling-v2]]
==== Disabling the DRBD Module Loader

In some circumstances it might be necessary to disable the DRBD module loader entirely. For
example, if you are using an immutable operating system, and DRBD and other modules are loaded
as
part of the host configuration.

To disable the DRBD module loader completely, apply the following YAML configuration to your deployment:

[source,yaml]
----
apiVersion: piraeus.io/v1
kind: LinstorSatelliteConfiguration
metadata:
  name: no-loader
spec:
  podTemplate:
    spec:
      initContainers:
      - name: drbd-module-loader
        $patch: delete
----

[[s-kubernetes-drbd-module-loader-selecting-version-v2]]
==== Selecting a Different DRBD Module Loader Version

By default, the Operator will try to find a DRBD module loader that matches the host operating system. The Operator determines the host distribution by inspecting the `.status.nodeInfo.osImage` field of the Kubernetes `Node` resource. A user-defined image can be used if the automatic mapping does not succeed or if you have different module loading requirements.

The following YAML configuration overrides the chosen DRBD module loader image with a user-defined image `example.com/drbd-loader:v9`:

[source,yaml]
----
apiVersion: piraeus.io/v1
kind: LinstorSatelliteConfiguration
metadata:
  name: custom-drbd-module-loader-image
spec:
  podTemplate:
    spec:
      initContainers:
      - name: drbd-module-loader
        image: example.com/drbd-loader:v9
----

`drbd.io`, available to LINBIT customers only, maintains the following module loader container images:

[cols "1,1"]
|===
| Image | Distribution

| `drbd.io/drbd9-amzn2:v9.2.5` | Amazon Linux 2
| `drbd.io/drbd9-bionic:v9.2.5` | Ubuntu 18.04
| `drbd.io/drbd9-focal:v9.2.5` | Ubuntu 20.04
| `drbd.io/drbd9-jammy:v9.2.5` | Ubuntu 22.04
| `drbd.io/drbd9-rhel7:v9.2.5` | Red Hat Enterprise Linux 7
| `drbd.io/drbd9-rhel8:v9.2.5` | Red Hat Enterprise Linux 8
| `drbd.io/drbd9-rhel9:v9.2.5` | Red Hat Enterprise Linux 9
|===

If you need to create a module loader image for your own distribution, you can refer to link:https://github.com/piraeusdatastore/piraeus/tree/master/dockerfiles/drbd-driver-loader[the container source files] which are available in the upstream Piraeus project.

[[s-kubernetes-drbd-module-loader-changing-how-it-loads-v2]]
==== Changing How the Module Loader Loads the DRBD Kernel Module

By default, the DRBD module loader will try to build the kernel module from source. The module loader can also be configured to load the module from a DEB or RPM package included in the image, or skip loading DRBD entirely.

To change the behavior of the DRBD module loader, set the `LB_HOW` environment variable to an appropriate value shown in the following table:

[cols "1,1"]
|===
| `LB_HOW` | Module Loader Behavior

| `compile` | The default value. Builds the DRBD module from source and tries to load all optional modules from the host.
| `shipped_modules` | Searches for `.rpm` or `.deb` packages at `/pkgs` and inserts contained the DRBD modules. Optional modules are loaded from the host if available.
| `deps_only` | Only tries to load the optional modules. No DRBD module will be loaded.
|===

After setting the `LB_HOW` environment variable, apply the following YAML configuration to your deployment. Based on the name within the metadata section, the example below would be used with an `LB_HOW` environment variable that was set to `deps_only`.

[source,yaml]
----
apiVersion: piraeus.io/v1
kind: LinstorSatelliteConfiguration
metadata:
  name: no-drbd-module-loader
spec:
  podTemplate:
    spec:
      initContainers:
      - name: drbd-module-loader
        env:
        - name: LB_HOW
          value: deps_only
----

[[s-kubernetes-drbd-replication-via-host-network-v2]]
=== Using the Host Network for DRBD Replication in Operator v2 Deployments
// https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/how-to/drbd-host-networking.md

Instructions in this section will describe how you can use the host network for DRBD replication traffic.

By default, DRBD will use the container network to replicate volume data. This ensures replication works on a wide range of clusters without further configuration. It also enables use of `NetworkPolicy` to block unauthorized access to DRBD traffic. Since the network interface of the pod is tied to the lifecycle of the pod, it also means DRBD will temporarily disrupt replication when the LINSTOR satellite pod is restarted.

In contrast, using the host network for DRBD replication will cause replication to work independently of the LINSTOR satellite pod. The host network might also offer better performance than the container network. As a downside, you will have to manually ensure connectivity between nodes on the relevant ports.

NOTE: To follow the steps in this section, you should be familiar with editing
link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md#linstorsatelliteconfiguration[`LinstorSatelliteConfiguration`]
resources.

[[s-kubernetes-drbd-replication-switching-from-container-to-host-network-v2]]
==== Configuring DRBD Replication to Use the Host Network

Switching from the default container network to the host network for DRBD replication is possible at any time. Existing DRBD resources will then be reconfigured to use the host network interface.

To configure the host network for the LINSTOR satellite, apply the following YAML configuration to your deployment:

[source,yaml]
----
apiVersion: piraeus.io/v1
kind: LinstorSatelliteConfiguration
metadata:
  name: host-network
spec:
  podTemplate:
    spec:
      hostNetwork: true
----

After the satellite pods are recreated, they will use the host network. Any existing DRBD resources are reconfigured to use a new IP address on the host network rather than an IP address on the container network.

[[s-kubernetes-drbd-replication-switching-from-host-to-container-network-v2]]
==== Configuring DRBD Replication to Use the Container Network

Switching back from host network to container network involves manually resetting the configured peer addresses used by DRBD. You can do this by rebooting every node, or by manually resetting the addresses by using the `drbdadm` CLI command on each node. Each method is described below.

[[s-kubernetes-drbd-replication-switching-from-host-to-container-network-node-rebooting-v2]]
===== Rebooting Nodes to Switch DRBD Replication from the Host to the Container Network

First, you need to remove the `LinstorSatelliteConfiguration` that set `hostNetwork: true`. You can do this by entering the following `kubectl` command:

----
$ kubectl delete linstorsatelliteconfigurations.piraeus.io host-network
linstorsatelliteconfiguration.piraeus.io "host-network" deleted
----

Next, reboot each cluster node, either serially, one by one, or else all at once. In general, replication will not work between rebooted nodes and non-rebooted nodes. The non-rebooted nodes will continue to use the host network addresses, which are generally not reachable from the container network.

After all nodes have restarted, all resources will be configured to use the container network, and all DRBD connections should be connected again.

[[s-kubernetes-drbd-replication-switching-from-host-to-container-network-node-drbdadm-v2]]
===== Using the DRBD Administration Tool to Switch DRBD Replication from the Host to the Container Network

WARNING: During this procedure, ensure that no new volumes or snapshots are created, otherwise the migration to the container network might not be applied to all resources.

First, you need to temporarily stop all DRBD replication and suspend all DRBD volume I/O operations by using the `drbdadm suspend-io all` command. Enter the command once on each LINSTOR satellite pod.

----
$ kubectl exec ds/linstor-satellite.node1.example.com -- drbdadm suspend-io all
$ kubectl exec ds/linstor-satellite.node2.example.com -- drbdadm suspend-io all
$ kubectl exec ds/linstor-satellite.node3.example.com -- drbdadm suspend-io all
----

Next, disconnect all DRBD connections on all nodes.

----
$ kubectl exec ds/linstor-satellite.node1.example.com -- drbdadm disconnect --force all
$ kubectl exec ds/linstor-satellite.node2.example.com -- drbdadm disconnect --force all
$ kubectl exec ds/linstor-satellite.node3.example.com -- drbdadm disconnect --force all
----

Next, you can safely reset all DRBD connection paths. This frees the connection on each node to be moved to the container network.

----
$ kubectl exec ds/linstor-satellite.node1.example.com -- drbdadm del-path all
$ kubectl exec ds/linstor-satellite.node2.example.com -- drbdadm del-path all
$ kubectl exec ds/linstor-satellite.node3.example.com -- drbdadm del-path all
----

Finally, remove the `LinstorSatelliteConfiguration` resource configuration that set `hostNetwork: true`. This will result in the creation of new LINSTOR satellite pods that use the container network.

----
$ kubectl delete linstorsatelliteconfigurations.piraeus.io host-network
linstorsatelliteconfiguration.piraeus.io "host-network" deleted
----

After the pods are recreated and the LINSTOR satellites are `Online`, the DRBD resource will be reconfigured and resume I/O operations.

[[s-kubernetes-evacuate-node]]
=== Evacuating a Node in Kubernetes

If you want to evacuate a LINSTOR node of its resources, so that they are placed onto other
nodes within your cluster, the process is detailed in <<s-linstor-node-evacuate>>. However,
before evacuating a LINSTOR node in Kubernetes, you need to take an additional action.

First, prevent new Kubernetes workloads from being scheduled to the node and then move the
node's workload to another node. You can do this by entering the following commands:

----
# kubectl cordon <node_name>
# kubectl drain --ignore-daemonsets <node_name>
----

After verifying that your cluster is running as expected, you can continue to follow the steps
in <<s-linstor-node-evacuate>>.

If you are planning on evacuating more than one node, enter the following command on all the
nodes that you will be evacuating:

----
# linstor node set-property n1.k8s-mwa.at.linbit.com AutoplaceTarget false
----

This ensures that LINSTOR will not place resources from a node that you are evacuating onto
another node that you plan on evacuating.

[[s-kubernetes-deleting-a-node]]
=== Deleting a LINSTOR Node in Kubernetes

Before you can delete a LINSTOR storage node from a Kubernetes cluster, the node must have no
deployed LINSTOR resources or Kubernetes workloads. To remove resources and workloads from a
node, you can follow the instructions in the <<s-kubernetes-evacuate-node>> section.

Alternatively, let the LINSTOR Operator handle the node evacuation for you. To do this, you need
to take a few preparatory steps.

First, use `kubectl` to apply a label, `marked-for-deletion` in the following example, to the
node that you want to delete from your Kubernetes cluster.

----
# kubectl label nodes <node-name> marked-for-deletion=
----

IMPORTANT: You must add an equals sign (`=`) to the end of your label name.

Next, configure the `linstorcluster` Kubernetes resource so that it does not deploy LINSTOR
resources to any node with the label that you have applied. To do this, edit the resource and
replace the contents with the lines shown in the following example, including the `...` line.
Change the `key` to match the label that you previously applied to your node.

----
# kubectl edit linstorclusters linstorcluster
...
spec:
  nodeAffinity:
    nodeSelectorTerms:
    - matchExpressions:
      - key: marked-for-deletion
        operator: DoesNotExist
----

After taking these steps, the LINSTOR Operator will automatically delete the corresponding
`LinstorSatellite` resource. That will stop the LINSTOR satellite service on that node, but only
after LINSTOR runs a `node evacuate` operation and there are no more resources on the node.
Enter the following command to wait for the Operator to finish evacuating the node:

----
# kubectl wait linstorsatellite/<node> --for=condition=EvacuationCompleted
----

This can take some time. You can also use `kubectl describe linstorsatellite/<node>` to get a status update on the evacuation process.

After the Operator finishes evacuating the node, you can delete the node by entering the following command:

----
# kubectl delete node <node-name>
----

[[s-kubernetes-monitoring]]
=== Monitoring With Prometheus

A LINSTOR deployment in Kubernetes offers the possibility of integrating with the
link:https://github.com/prometheus-operator/kube-prometheus[Prometheus monitoring stack]. You
can use https://prometheus.io/[Prometheus] to monitor LINSTOR and related components in
Kubernetes. This monitoring solution is configurable to suit your purposes and includes features
such as a Grafana dashboard for Prometheus scraped metrics, and the ability to set up alert
rules and get alerts for events.

The Prometheus monitoring integration with LINSTOR configures:

 - Metrics scraping for the LINSTOR and DRBD state.
 - Alerts based on the cluster state
 - A Grafana dashboard

To configure monitoring for your LINSTOR in Kubernetes deployment, you should be familiar with:

- Deploying workloads in Kubernetes using link:https://helm.sh/[Helm]
- Deploying resources using link:https://kubernetes.io/docs/tasks/tools/[`kubectl`]

[[s-kubernetes-monitoring-v2]]
==== Configuring Monitoring with Prometheus in Operator v2 Deployments

This section describes configuring monitoring with Prometheus for LINSTOR Operator v2
deployments in Kubernetes. If you need to configure monitoring with Prometheus for a LINSTOR
Operator v1 deployment, refer to the instructions in the <<s-kubernetes-monitoring-v1,>>
section.

[[s-kubernetes-prometheus-operator-deployment-v2]]
===== Deploying the Prometheus Operator for LINSTOR Operator v2 Deployments

NOTE: If you already have a working Prometheus Operator deployment, skip the steps in this
section.

To deploy the Prometheus monitoring integration in Kubernetes, you need to deploy the Prometheus
Operator. A simple way to do this is to use the Helm chart provided by the Prometheus Community.

First, add the Helm chart repository to your local Helm configuration, by entering the following
command:

----
# helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
----

Then, deploy the
link:https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack[`kube-prometheus-stack`]
chart. This chart will set up Prometheus, Alertmanager, and Grafana for your cluster. Configure
it to search for monitoring and alerting rules in all namespaces:

----
# helm install \
--create-namespace -n monitoring prometheus prometheus-community/kube-prometheus-stack \
--set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \
--set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \
--set prometheus.prometheusSpec.ruleSelectorNilUsesHelmValues=false
----

NOTE: By default, the deployment will only monitor resources in the `kube-system` and its own
namespace. LINSTOR is usually deployed in a different namespace, `linbit-sds`, so you need to
configure Prometheus to watch this namespace. In the example above, this is achieved by setting
the various `*NilUsesHelmValues` parameters to false.

[[s-kubernetes-prometheus-monitoring-and-alerting-deployment-v2]]
===== Deploying Prometheus Monitoring and Alerting Rules for LINSTOR

After creating a Prometheus Operator deployment and configuring it to watch all namespaces,
apply the monitoring and alerting resources for LINSTOR. You can do this by either using the
`kustomize` or `helm` utilities.

[[s-kubernetes-monitoring-deploying-prometheus-kustomize-v2]]
====== Using Kustomize to Deploy Prometheus Monitoring and Alerting Rules

To deploy Prometheus monitoring and alerting rules for LINSTOR by using the `kustomize` utility,
you can apply a LINBIT GitHub-hosted `kustomization` configuration, by entering the following command:

----
# kubectl apply -k \
"https://github.com/linbit/linstor-operator-builder//config/monitoring?ref=v2"
----

[IMPORTANT]
====
If you have configured SSL/TLS for your LINSTOR deployment in Kubernetes, you will need to apply
a different version of the monitoring configuration. You can do this by entering the
following command:

----
kubectl apply -k \
"https://github.com/linbit/linstor-operator-builder//config/monitoring-with-api-tls?ref=v2"
----
====

Output from applying the monitoring configuration to your deployment should show the following:

----
configmap/linbit-sds-dashboard created
podmonitor.monitoring.coreos.com/linstor-satellite created
prometheusrule.monitoring.coreos.com/linbit-sds created
servicemonitor.monitoring.coreos.com/linstor-controller created
----

[[s-kubernetes-monitoring-deploying-prometheus-helm-v2]]
====== Using Helm to Deploy Prometheus Monitoring and Alerting Rules

If you are using Helm, you can deploy Prometheus monitoring for your LINSTOR in Kubernetes
deployment by enabling monitoring in the `linbit-sds` chart. The following command will install a new LINSTOR in Kubernetes deployment with Prometheus monitoring, or upgrade an existing Helm-installed deployment and enable Prometheus monitoring:

----
# helm repo update linstor && \
helm upgrade --install linbit-sds linstor/linbit-sds \
--set monitoring.enabled=true
----

NOTE: Unlike the `kustomize` deployment method, you can use the same `helm upgrade --install` command for
a regular and an SSL/TLS-enabled LINSTOR deployment. The Helm deployment configures the correct
endpoint automatically.

Output from the command above should show that the `linbit-sds` chart was successfully deployed.

[[s-kubernetes-monitoring-verifying-deployment-v2]]
===== Verifying a Monitoring Deployment

You can verify that the monitoring configuration is working in your deployment by checking the
local Prometheus, Alertmanager, and Grafana web consoles.

[[s-kubernetes-monitoring-verifying-prometheus-web-console-v2]]
====== Verifying the Prometheus Web Console Deployment

First, get access to the Prometheus web console from your local browser by forwarding the
Prometheus web console service to local port 9090:

----
# kubectl port-forward -n monitoring services/prometheus-kube-prometheus-prometheus 9090:9090
----

NOTE: If you need to access the Prometheus instance from a system other than `localhost`, you
might need to add the `--address 0.0.0.0` argument to the previous command.

Next, in a web browser, open `http://localhost:9090/graph` and display the `linstor_info` and
`drbd_version` metrics, for example, by entering each metric into the search field, and clicking
the Execute button.

.The `linstor_info` Prometheus metric
image::images/linstor-k8s-prometheus-linstor_info-metic.png[the `linstor_info` Prometheus metric]

.The `drbd_version` Prometheus metric
image::images/linstor-k8s-prometheus-drbd_version-metic.png[the `drbd_version` Prometheus metric]

[[s-kubernetes-monitoring-verifying-alertmanager-web-console-v2]]
====== Verifying the Prometheus Alertmanager Web Console Deployment

To view the Alertmanager console, forward the Prometheus Alertmanager service to local port
9093, by entering the following command:

----
# kubectl port-forward -n monitoring services/prometheus-kube-prometheus-alertmanager 9093:9093
----

NOTE: If you need to access the Prometheus Alertmanager instance from a system other than
`localhost`, you might need to add the `--address 0.0.0.0` argument to the previous command.

Next, in a web browser, open `http://localhost:9093`. The Alertmanager console should show an
itemized list of alert groups for your deployment, including an alert group for the `linbit-sds`
namespace.

You can verify that an alert will fire and be shown in your Alertmanager console by running a
command that will cause an event applicable to an alert that you want to test. For example, you
can disconnect a DRBD resource. This will cause a `drbdResourceSuspended` alert. To test this,
assuming that you have a LINSTOR satellite pod named `kube-0` that is running in the
`linbit-sds` namespace and that the satellite is in a secondary role for a DRBD resource named
`my-res`, enter the following command:

----
# kubectl exec -it -n linbit-sds kube-0 -- drbdadm disconnect --force my-res
----

WARNING: Use caution with the types of events that you use to test alerts, especially on a
production system. The above `drbdadm disconnect` command should be safe if you run it on a
satellite node pod that is in a secondary role for the resource you disconnect. You can verify
the resource role state that the satellite node pod is in by using a `drbdadm status
<resource-name>` command.

.Alerts for the `my-res` DRBD resource in a disconnected state
image::images/linstor-k8s-prometheus-alertmanager-drbdconnectionnotconnected.png[alerts for the `my-res` DRBD resource in a disconnected state]

After verifying that the Prometheus Alertmanager web console shows new alerts that relate to the
event you caused, you should revert your deployment to its previous state. To do this for the
previous `drbdadm disconnect` example, enter the following command:

----
# kubectl exec -it -n linbit-sds kube-0 -- drbdadm connect my-res
----

You can verify that the `kube-0` satellite node pod is again connected to the `my-res` DRBD
resource by entering a `drbdadm status` command:

----
# kubectl exec -it -n linbit-sds kube-0 -- drbdadm status my-res
----

Output from the command should show that the resource is in an up-to-date state on all diskful
and connected nodes.

----
my-res role:Secondary
  disk:UpToDate
  kube-1 role:Secondary
    peer-disk:Diskless
  kube-2 role:Secondary
    peer-disk:UpToDate
----

[[s-kubernetes-monitoring-verifying-grafana-web-console-v2]]
====== Verifying the Grafana Web Console and LINBIT SDS Dashboard Deployment

To view the Grafana console and LINBIT SDS dashboard, first forward the Grafana service to local
port 3000, by entering the following command:

----
# kubectl port-forward -n monitoring services/prometheus-grafana 3000:http-web
----

NOTE: If you need to access the Grafana instance from a system other than the `localhost`, you
might need to add the `--address 0.0.0.0` argument to the previous command.

Next, in a web browser, open `http://localhost:3000` and log in. If you are using the example
deployment from above, enter username `admin` and password `prom-operator` to log in to the
Grafana instance. After logging in, change your password
(`http://192.168.121.20:3000/profile/password`) to something other than the default password.

Select "LINBIT SDS" from the available dashboards (`http://localhost:3000/dashboards`) to show
an overview of the health status of your LINSTOR deployment, including various metrics and
statistics.

.The LINBIT SDS Grafana dashboard
image::images/linstor-k8s-linbit-sds-grafana-dashboard.png[the LINBIT SDS Grafana dashboard]

[[s-kubernetes-monitoring-v1]]
==== Monitoring with Prometheus in Operator v1 Deployments

In Operator v1 deployments, the operator will set up monitoring containers along the existing components and make them available as a `Service`.

If you use the https://prometheus-operator.dev/[Prometheus Operator], the LINSTOR Operator will also set up the `ServiceMonitor`
instances. The metrics will automatically be collected by the Prometheus instance associated to the operator, assuming
https://prometheus-operator.dev/docs/kube/monitoring-other-namespaces/[watching the Piraeus namespace is enabled].

To disable exporting of metrics, set `operator.satelliteSet.monitoringImage` to an empty value.

[[s-kubernetes-monitoring-linstor-controller-v1]]
===== LINSTOR Controller Monitoring in Operator v1 Deployments

The LINSTOR controller exports cluster-wide metrics. Metrics are exported on the existing controller service, using the
path https://linbit.com/drbd-user-guide/linstor-guide-1_0-en/#s-linstor-monitoring[`/metrics`].

[[s-kubernetes-monitoring-drbd-resource-v1]]
===== DRBD Resource Monitoring in Operator v1 Deployments

All satellites are bundled with a secondary container that uses https://github.com/LINBIT/drbd-reactor/[`drbd-reactor`]
to export metrics directly from DRBD. The metrics are available on port 9942, for convenience a headless service named
`<linstorsatelliteset-name>-monitoring` is provided.

If you want to disable the monitoring container, set `monitoringImage` to `""` in your `LinstorSatelliteSet` resource.

