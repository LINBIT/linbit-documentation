[[ch-proxmox-linstor]]
== LINSTOR Volumes in Proxmox VE

indexterm:[Proxmox]This chapter describes DRBD(R) in Proxmox Virtual Environment (VE) using
the https://github.com/linbit/linstor-proxmox[LINSTOR(R) Proxmox Plugin].

[[s-proxmox-ls-overview]]
=== Introduction to Proxmox VE

http://www.proxmox.com/en/[Proxmox VE] is an easy to use, complete server
virtualization environment with KVM, Linux Containers and HA.

`linstor-proxmox` is a Perl plugin for Proxmox that, in combination with LINSTOR, allows you to replicate VM
//(LVM volumes on DRBD)
disks on several Proxmox VE nodes. This allows to live-migrate
active VMs within a few seconds and with no downtime without needing a central SAN, as the data is already
replicated to multiple nodes.

[[s-proxmox-ls-upgrades]]
=== Upgrading Proxmox
If this is a fresh installation, skip this section and continue with <<s-proxmox-ls-install>>.

[[s-proxmox-ls-upgrades-plug-x-8]]
==== Upgrading Plugin to 8.x
This requires LINSTOR 1.27.0 or greater.

In order to allow reassigning disks between VMs we had to change the naming scheme. New disks (also clones of
old ones) will have names like `pm-12cf742a_101` on PVE level, and `pm-12cf742a` on LINSTOR/DRBD level. This
is a static prefix (`pm-`), 8 characters of a UUID, and on PVE level the VMID separated by an underscore
(`_101`). The new naming obsoletes the old `vm-101-disk-1` like names. Old VMs with legacy names still work
with version 8 of the plugin. For the user this change should be completely transparent and should not require
any changes besides getting accustomed to the new names.

So far it was only possible to migrate data from external storage like LVM to LINSTOR/DRBD if the data was
migrated offline. If you want to migrate data online you can now *temporarily* set `exactsize yes` in your
`storage.cfg` for a particular DRBD storage and then migrate disks to it. After you are done, remove the
`exactsize` option from the `storage.cfg` again. The LINSTOR property that allowed temporary online migration
is then deleted when the disk is activated again (but not if it is currently active). If you want to delete
the property for all active disks after migration, or you want to be extra sure, you can run a command like
this:

----
# linstor -m --output-version v1 rd l | jq '.[][].name' | xargs -I {} linstor rd sp {} DrbdOptions/ExactSize False
----

[[s-proxmox-ls-upgrades-plug-x-7]]
==== Upgrading Plugin to 7.x
Version 7 of the plugin uses a LINSTOR controller API that is available from LINSTOR version 1.21.1 onwards.
Make sure that your LINSTOR setup (controller and satellites) use at least that version.

[[s-proxmox-ls-upgrades-plug-4-5]]
==== Upgrading Plugin from 4.x to 5.x
Version 5 of the plugin drops compatibility with the legacy configuration options "storagepool" and
"redundancy". Version 5 *requires* a "resourcegroup" option, and obviously a LINSTOR resource group. The old
options should be removed from the configuration.

Configuring LINSTOR is described in Section <<s-proxmox-ls-ls-configuration>>, a typical example follows:

Let's assume the pool was set to "mypool", and redundancy to 3.
----
# linstor resource-group create --storage-pool=mypool --place-count=3 drbdMypoolThree
# linstor volume-group create drbdMypoolThree
# vi /etc/pve/storage.cfg
drbd: drbdstorage
   content images,rootdir
   controller 10.11.12.13
   resourcegroup drbdMypoolThree
----

[[s-proxmox-ls-upgrades-plug-5-6]]
==== Upgrading Plugin from 5.x to 6.x
Version 6.0.0 of the plugin drops all code related to the `redundancy` setting. This is handled by LINSTOR
resource groups (`resourcegroup` setting) for a very long time. No change should be required.

The `controllervm` setting, which was intended for executing a LINSTOR controller in a VM manged by LINSTOR is
gone. Using `drbd-reactor` to realize a highly available LINSTOR controller is what we suggest.

The settings `statuscache` and `preferlocal` are now enabled by default.

[[s-proxmox-ls-upgrades-pve-5-6]]
==== Upgrading PVE from 5.x to 6.x
With version 6 PVE added additional parameters to some functions and rightfully reset their "APIAGE". This
means that old plugins, while actually usable as they don't use any of these changed functions do not work
anymore. Please upgrade to plugin version 5.2.1 at least.

[[s-proxmox-ls-install]]
=== Installing the LINSTOR Proxmox Plugin

To use LINSTOR in Proxmox, you will need to install the LINSTOR Proxmox plugin.

[[s-proxmox-pve-headers-install]]
==== Installing the Proxmox VE Kernel Headers

To use LINSTOR in Proxmox, you will need to install the DRBD kernel module. The DRBD 9 kernel
module is installed as a kernel module source package (`drbd-dkms`). Therefore, you will have
to install the Proxmox VE kernel headers package, `pve-headers`, before you install the DRBD
kernel module from the LINBIT(R) repositories. Following that order ensures that the kernel module
will build properly for your kernel.

If you do not plan to install the latest Proxmox kernel, you have to install kernel headers
matching your current running kernel (for example, `pve-headers-$(uname -r)`). If you missed
this step, then you can still rebuild the `drbd-dkms` package against your current kernel (so
long as you have installed kernel headers in advance) by entering the `apt install --reinstall
drbd-dkms` command.

IMPORTANT: You will need to add the Proxmox PVE repository to your APT sources list,
`/etc/apt/sources.list`, and then enter `apt update`, before you can install the `pve-headers`
package. Refer to
https://pve.proxmox.com/wiki/Package_Repositories#_repositories_in_proxmox_vei[the Proxmox wiki]
for instructions.

ifndef::de-brand[]
[[s-proxmox-installing-from-linbit-customer-repos]]
==== Installing the Proxmox Plugin By Using LINBIT Customer Repositories

If you are a LINBIT customer, or you have an evaluation account, you can enable LINBIT's
`drbd-9` repository on your node and then update your repositories by using an `apt update`
command.

You can then install the DRBD kernel module, DRBD utilities, and the LINSTOR Proxmox plugin by
entering:

----
# apt install drbd-utils linstor-proxmox drbd-dkms
----

NOTE: Refer to the <<linstor-administration.adoc#s-linbit-manage-nodes-script,Using a Script to
Manage LINBIT Cluster Nodes>> for instructions on registering a node with LINBIT and enabling
LINBIT repositories.
endif::de-brand[]

[[s-proxmox-installing-from-linbit-public-repos]]
==== Installing the Proxmox Plugin By Using LINBIT Public Repositories

LINBIT provides a dedicated public repository for Proxmox VE users. This repository not only contains the Proxmox plugin, but the whole LINBIT SDS stack including a DRBD kernel module and user-space utilities.


You can add LINBIT's public repository by entering the commands below, setting `$PVERS` to your
Proxmox VE *major version* (for example, "8", not "8.1"):

----
# wget -O /tmp/package-signing-pubkey.asc \
https://packages.linbit.com/package-signing-pubkey.asc
# gpg --yes -o /etc/apt/trusted.gpg.d/linbit-keyring.gpg --dearmor \
/tmp/package-signing-pubkey.asc
# PVERS=8 && echo "deb [signed-by=/etc/apt/trusted.gpg.d/linbit-keyring.gpg] \
http://packages.linbit.com/public/ proxmox-$PVERS drbd-9" > /etc/apt/sources.list.d/linbit.list
----

After adding the LINBIT package repository, you can install the Proxmox plugin and other
necessary components (DRBD kernel module and utilities), by entering the following command:

----
# apt update && apt -y install drbd-dkms drbd-utils linstor-proxmox
----

[[s-proxmox-ls-ls-configuration]]
=== Configuring LINSTOR

For the rest of this guide we assume that you have a LINSTOR cluster configured as described in
<<s-linstor-init-cluster>>. Start the "linstor-controller" on one node, and the "linstor-satellite" on all
nodes. The "linstor-satellite" service needs some extra configuration which should be done via
`systemctl edit linstor-satellite.service`:

----
[Service]
Type=notify
TimeoutStartSec=infinity
----

The preferred way to use the plugin, starting from version 4.1.0, is through LINSTOR resource groups and a
single volume group within every resource group. LINSTOR resource groups are described in
<<s-linstor-resource-groups>>. All the required LINSTOR configuration (e.g., redundancy count) has to be set
on the resource group.

[[s-proxmox-ls-configuration]]
=== Configuring the Proxmox Plugin
The final step is to provide a configuration for Proxmox itself. This can be done by adding an entry in the
`/etc/pve/storage.cfg` file, with a content similar to the following.

----
drbd: drbdstorage
   content images,rootdir
   controller 10.11.12.13
   resourcegroup defaultpool
----

The `drbd` entry is fixed and you are not allowed to modify it, as it tells to Proxmox to use DRBD as storage
back end. The "drbdstorage" entry can be modified and is used as a friendly name that will be shown in the PVE
web GUI to locate the DRBD storage. The "content" entry is also fixed, so do not change it. The redundancy
(specified in the resource group) specifies how many replicas of the data will be stored in the cluster. The recommendation is to set it
to 2 or 3 depending on your setup. The data is accessible from all nodes, even
if some of them do not have local copies of the data. For example, in a 5 node cluster, all nodes will be
able to access 3 copies of the data, no matter where they are stored in. The "controller" parameter must be
set to the IP of the node that runs the LINSTOR controller service. Only one node can be set to run as LINSTOR
controller at the same time. If that node fails, start the LINSTOR controller on another node and change that
value to its IP address.

A configuration using different storage pools in different resource groups would look like this:

----
drbd: drbdstorage
   content images,rootdir
   controller 10.11.12.13
   resourcegroup defaultpool

drbd: fastdrbd
   content images,rootdir
   controller 10.11.12.13
   resourcegroup ssd

drbd: slowdrbd
   content images,rootdir
   controller 10.11.12.13
   resourcegroup backup
----

By now, you should be able to create VMs using Proxmox's web GUI by selecting "__drbdstorage__", or any other of
the defined pools as storage location.

Starting from version 5 of the plugin, you can set the option "preferlocal yes". If it is set, the plugin tries
to create a diskful assignment on the node that issued the storage create command. With this option you can
ensure that the VM gets local storage if possible. Without that option LINSTOR might place the storage on nodes
'B' and 'C', while the VM is initially started on node 'A'. This would still work as node 'A' then would get a
diskless assignment, but having local storage might be preferred.

.NOTE: DRBD supports only the **raw** disk format at the moment.

At this point you can try to live migrate the VM - as all data is accessible on all nodes (even on Diskless
nodes) - it will take just a few seconds. The overall process might take a bit longer if the VM is under
load and if there is a significant amount of RAM being dirtied all the time. But in any case, the downtime should be minimal
and you will see no interruption at all.

.Table Configuration Options
|===
| Option | Meaning

| `controller`    | The IP of the LINSTOR controller (',' separated list allowed)
| `resourcegroup` | The name of a LINSTOR resource group which defines the deployment of new VMs. As described above
| `preferlocal`   | Prefer to create local storage (yes/no). As decribed above
| `statuscache`   | Time in seconds status information is cached, 0 means no extra cache. Relevant on huge clusters with hundreds of resources. This has to be set on *all* `drbd` storages in `/etc/pve/storage.cfg` to take effect.
| `exactsize`     | Set this temporarily to `yes` to allow storage migration from external storage such as LVM
to DRBD/LINSTOR.
| `apicrt`        | Path to the client certificate
| `apikey`        | Path to the client private key
| `apica`         | Path to the CA certificate
|===

[[s-proxmox-ls-HA]]
=== Making the Controller Highly Available (Optional Configuration)
Making LINSTOR highly available is a matter of making the LINSTOR controller
highly-available. This step is described in Section <<s-linstor_ha>>.

The last -- but crucial -- step is to configure the Proxmox plugin to be
able to connect to multiple LINSTOR controllers. It will use the first one it
receives an answer from. This is done by adding a comma-separated list of
controllers in the `controller` section of the plugin like this:

----
drbd: drbdstorage
   content images,rootdir
   controller 10.11.12.13,10.11.12.14,10.11.12.15
   resourcegroup defaultpool
----

[[s-proxmox-cloud-init]]
=== Storage for Cloud-init Images
Cloud-init VM images are only a few MB in size and Proxmox can generate them on demand. This is possible
because the settings saved in cloud-init images are stored cluster wide in Proxmox itself. This 
allows Proxmox to use local storage (for example, LVM) for such images. If a VM is started on a node where the cloud-init
image does not exist, it is generated from the stored settings.

While you can store cloud-init images on DRBD storage, there is no gain in doing that. Storing cloud-init images on local storage is enough.

