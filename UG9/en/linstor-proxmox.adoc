[[ch-proxmox-linstor]]
== LINSTOR Volumes in Proxmox VE

indexterm:[Proxmox]This chapter describes integrating LINSTOR(R) and DRBD(R) in Proxmox Virtual
Environment (VE) by using the https://github.com/linbit/linstor-proxmox[LINSTOR Proxmox Plugin].
The sections in this chapter describe how to <<s-proxmox-ls-install,install>>,
<<s-proxmox-ls-ls-configuration,configure>>, and <<s-proxmox-ls-upgrades,upgrade>> the LINSTOR
Proxmox plugin.

[[s-proxmox-ls-overview]]
=== Introduction to Proxmox VE

http://www.proxmox.com/en/[Proxmox VE] is an easy to use, complete server
virtualization environment with KVM, Linux Containers, and high availability (HA).

`linstor-proxmox`Â is a storage plugin for Proxmox that interfaces with LINSTOR, allowing you to
replicate virtual disk images and container volumes across several Proxmox VE nodes in your cluster,
all backed by DRBD. By integrating LINSTOR with Proxmox, you can live migrate active VMs within
seconds, with no downtime and without needing a centralized SAN. LINSTOR also unlocks other shared
storage functionality in Proxmox, allowing you to use features such as
https://pve.proxmox.com/wiki/High_Availability[high availability].

[[s-proxmox-ls-install]]
=== Installing the LINSTOR Proxmox Plugin

This section describes how to install the LINSTOR Proxmox plugin. If you have already installed
the plugin and you need to upgrade it to a newer version, refer to the <<s-proxmox-ls-upgrades>>
section.

[[s-proxmox-pve-headers-install]]
==== Installing the Proxmox VE Kernel Headers

To use LINSTOR in Proxmox, you will need to install the DRBD kernel module. The DRBD 9 kernel module
is installed as a kernel module source package (`drbd-dkms`). Therefore, you will have to install
the Proxmox VE kernel headers package, `proxmox-default-headers`, before you install the DRBD kernel
module from the LINBIT(R) repositories. Following that order ensures that the kernel module will
build properly for your kernel.

If you do not plan to install the latest Proxmox kernel, you have to install kernel headers that
match your current running kernel (for example, `proxmox-headers-$(uname -r)`). If you missed this
step and you installed different kernel headers already, you can still rebuild the `drbd-dkms`
package against your current kernel by entering the `apt install --reinstall drbd-dkms` command.

IMPORTANT: You will need to configure the Proxmox `pve-enterprise` or `pve-no-subscription`
repository in your APT sources list, `/etc/apt/sources.list`, and then enter `apt update`, before
you can install the `proxmox-headers-$(uname -r)` package. Refer to
https://pve.proxmox.com/wiki/Package_Repositories#_repositories_in_proxmox_ve[the Proxmox wiki] for
instructions.

ifndef::de-brand[]
[[s-proxmox-installing-from-linbit-customer-repos]]

==== Configuring LINBIT Customer Repositories for Proxmox
If you are a LINBIT customer, or you have an evaluation account, you can enable the LINBIT
`drbd-9` repository on your nodes and then update your repositories by using an `apt update`
command.

NOTE: Refer to the <<linstor-administration.adoc#s-linbit-manage-nodes-script,Using a Script to
Manage LINBIT Cluster Nodes>> for instructions on registering a node with LINBIT and enabling
LINBIT repositories.
endif::de-brand[]

[[s-proxmox-installing-from-linbit-public-repos]]
==== Configuring LINBIT Public Repositories for Proxmox

If you are familiar with the
https://pve.proxmox.com/wiki/Package_Repositories#sysadmin_no_subscription_repo[Proxmox VE
No-Subscription Repository], LINBIT also provides a publicly available repository for use with
Proxmox VE. This repository not only contains the Proxmox plugin, but the entire LINBIT SDS stack,
including DRBD, LINSTOR, and all user-space utilities.

WARNING: Although this can be a great option for homelabs, testing, and non-production use, LINBIT does
not maintain these packages to the same rigorous standards as the customer repositories. For
production deployments, using the official LINBIT customer repositories is recommended.

You can add the LINBIT public repository by entering the commands below, setting `$PVERS` to your
Proxmox VE *major version* (for example, "8", not "8.1"):

----
# wget -O /tmp/linbit-keyring.deb \
https://packages.linbit.com/public/linbit-keyring.deb
# dpkg -i /tmp/linbit-keyring.deb
# PVERS=8 && echo "deb [signed-by=/etc/apt/trusted.gpg.d/linbit-keyring.gpg] \
http://packages.linbit.com/public/ proxmox-$PVERS drbd-9" > /etc/apt/sources.list.d/linbit.list
# apt update
----

=== Installing the Proxmox Plugin

After configuring an appropriate LINBIT package repository, you can install the Proxmox plugin and
other necessary components (DRBD kernel module and utilities), by entering the following command:

----
# apt -y install drbd-dkms drbd-utils linstor-proxmox
----

[[s-proxmox-ls-ls-configuration]]
=== Configuring LINSTOR

The rest of this guide assumes that you have a LINSTOR cluster configured as described in
<<s-linstor-init-cluster>>. Start the `linstor-controller` service on one node, and the
`linstor-satellite` service on all nodes. The `linstor-satellite` service needs some extra
configuration which you can do by entering `systemctl edit linstor-satellite.service`:

----
[Service]
Type=notify
TimeoutStartSec=infinity
----

The preferred way to use the plugin, starting from version 4.1.0, is through LINSTOR resource
groups and a single volume group within every resource group. LINSTOR resource groups are
described in <<s-linstor-resource-groups>>. All the required LINSTOR configuration, for example,
data replica count, has to be set on the resource group.

[IMPORTANT]
====
When using LINSTOR together with LVM and DRBD, set the `global_filter` value in the LVM global configuration file (`lvm.conf`) to:

----
global_filter = [ "r|^/dev/drbd|" ]
----

This setting tells LVM to reject DRBD devices from operations such as scanning or opening attempts. In some cases, not setting this filter might lead to increased CPU load or stuck LVM operations.
====

[[s-proxmox-ls-configuration]]
=== Configuring the Proxmox Plugin

The final step is to provide a configuration for Proxmox itself. You can do this by adding an
entry in the `/etc/pve/storage.cfg` file, with content similar to the following:

----
drbd: drbdstorage
   content images,rootdir
   controller 10.11.12.13
   resourcegroup defaultpool
----

The `drbd` entry is fixed. You are not allowed to modify it because it tells Proxmox to use DRBD
as the storage back end. You can change the value for the `drbd` entry, here `drbdstorage`, as
you want. It is used as a friendly name that will be shown in the PVE web GUI to locate the DRBD
storage.

The `content` entry is also fixed, so do not change it. You can configure data redundancy by
specifying a <<linstor-administration.adoc#s-linstor-placement-count-rg-feature,placement
count>> for the backing LINSTOR resource group. This way, you can specify how many replicas of
the data will be stored in the cluster. The recommendation is to set the place count option to
two or three depending on your setup. By virtue of the DRBD diskless attachment feature, the
data is accessible from all nodes, even if some of them do not have local copies of the data.
For example, in a 5-node cluster, all nodes will be able to access three copies of the data, no
matter where they are stored in the cluster.

The `controller` parameter must be set to the IP of the node that runs the LINSTOR controller
service. Only one node in a cluster can run the LINSTOR controller service at any given time. If
that node fails, you will need to start the LINSTOR controller service on another node and
change the `controller` parameter value to the new node's IP address. It is also possible to
<<s-proxmox-ls-HA,make the LINSTOR controller itself highly available>>. By making the LINSTOR
controller highly available in your cluster, you have the option to specify either multiple IP
addresses for the `controller` parameter, or else specify a virtual IP address for the LINSTOR
controller.

The `resourcegroup` parameter specifies the name of the LINSTOR resource group that you want to
associate with the DRBD-backed storage in Proxmox.

[[s-linstor-proxmox-more-than-one-resource-group]]
==== Configuring More Than One LINSTOR Resource Group for Use in Proxmox

A configuration using different resource groups associated with different storage pools could
look similar to this:

----
drbd: drbdstorage
   content images,rootdir
   controller 10.11.12.13
   resourcegroup defaultpool

drbd: fastdrbd
   content images,rootdir
   controller 10.11.12.13
   resourcegroup ssd

drbd: slowdrbd
   content images,rootdir
   controller 10.11.12.13
   resourcegroup backup
----

After making this configuration, you will be able to create VMs by using the Proxmox web GUI, by
selecting "__drbdstorage__", or any other of the defined pools, "__fastdrbd__" or
"__slowdrbd__", as storage locations for your VM disk images.

Starting from version 5 of the plugin, you can set the option `preferlocal yes`. If you set this
option, the plugin tries to create a diskful assignment on the node that issued the storage
create command. With this option you can ensure that the VM gets local storage if possible.
Without the option LINSTOR might place the storage on nodes 'B' and 'C', while the VM is
initially started on node 'A'. This would still work as node 'A' then would get a diskless
assignment, but having local storage might be preferred.

.NOTE: DRBD supports only the **raw** disk format at the moment.

At this point you can try to live migrate the VM. Because all data is accessible on all nodes,
even on diskless nodes, it will take just a few seconds. The overall process might take a bit
longer if the VM is under load and if there is a significant amount of RAM being dirtied all the
time. But in any case, the downtime should be minimal and you will experience no operational
interruption at all.

.Table Configuration Options
|===
| Option | Meaning

| `controller`    | The IP of the LINSTOR controller (',' separated list allowed)
| `resourcegroup` | The name of a LINSTOR resource group which defines the deployment of new VMs. As described above
| `preferlocal`   | Prefer to create local storage (yes/no). As described above
| `statuscache`   | Time in seconds status information is cached, 0 means no extra cache. Relevant on huge clusters with hundreds of resources. This has to be set on *all* `drbd` storage entries in `/etc/pve/storage.cfg` to take effect.
| `exactsize`     | Set this temporarily to `yes` to allow storage migration from external storage such as LVM
to DRBD/LINSTOR.
| `apicrt`        | Path to the client certificate
| `apikey`        | Path to the client private key
| `apica`         | Path to the CA certificate
|===

[[s-proxmox-ls-HA]]
=== Configuring a Highly Available LINSTOR Controller in Proxmox

Making LINSTOR highly available is a matter of making the LINSTOR controller highly available.
Doing this is described in <<s-linstor_ha>>. This is an optional configuration that can make
your LINSTOR integration with Proxmox more fault-tolerant. 

After completing the steps in the linked section, the last and crucial step is to configure the
Proxmox plugin to be able to connect to different LINSTOR controllers. The plugin will use the
first controller it receives an answer from. You configure different LINSTOR controllers in
Proxmox by adding a comma-separated list of controller node IP addresses in the `controller`
section of the plugin, for example:

----
drbd: drbdstorage
   content images,rootdir
   controller 10.11.12.13,10.11.12.14,10.11.12.15
   resourcegroup defaultpool
----

An alternative is to configure a virtual IP (VIP) address for the LINSTOR controller by using an
OCF resource agent, `ocf:heartbeat:IPaddr2`, added to the DRBD Reactor promoter plugin's
services start list. If you do this, you could then specify the VIP address for the `controller`
parameter value.

[[s-proxmox-cloud-init]]
=== Storage for Cloud-init Images

Cloud-init VM images are only a few MB in size and Proxmox can generate them on-demand. This is
possible because the settings saved in cloud-init images are stored cluster wide in Proxmox
itself. This allows Proxmox to use local storage (for example, LVM) for such images. If a VM is
started on a node where the cloud-init image does not exist, it is generated from the stored
settings.

While you can store cloud-init images on DRBD storage, there is no gain in doing that. Storing
cloud-init images on local storage is enough.

[[s-linstor-proxmox-vm-image-naming]]
=== Virtual Machine Image Naming In Proxmox with LINSTOR

Starting with version 8 of the LINSTOR Proxmox plugin, VM disk images have names such as
`pm-12cf742a_101` within PVE, and `pm-12cf742a` within LINSTOR and DRBD. This is a static prefix
(`pm-`), 8 characters of a UUID, and on PVE level the VMID separated by an underscore (`_101`).
In older versions of the plugin, VM disk images had names such as `vm-101-disk-1`. If you
upgrade the plugin to version 8 or later, if you clone a VM that uses the older naming scheme,
the cloned disk image will have the version 8 naming scheme.

[[s-linstor-proxmox-migrating-storage]]
=== Migrating Storage to DRBD in Proxmox with LINSTOR

Sometimes, you might want to migrate existing Proxmox data to DRBD-backed storage. This section
details the steps that you need to take to do this, for example, when migrating existing LVM or
ZFS-backed Proxmox data. If your Proxmox data is already on DRBD-backed storage, these steps are
unnecessary, for example, to do a live migration of a VM from one DRBD-backed storage to another
DRBD-backed storage.

IMPORTANT: These instructions require version 8 or later of the LINSTOR Proxmox plugin.

If you want to migrate data, such as VM disk images, while your Proxmox VMs are online, you can
*temporarily* set `exactsize yes` in your `/etc/pve/storage.cfg` storage configuration file for
a particular DRBD storage, and then migrate disks from the non-DRBD-backed storage to the
DRBD-backed storage. After you are done, remove the `exactsize` option from the `storage.cfg`
configuration file. The LINSTOR property that the `exactsize` option enabled to temporarily
allow online migration will be deleted when the disk is activated again (but not if the disk is
currently active). If you want to delete the property for all active disks after migration, or
you want to be extra sure, you can run a command such as the following:

----
# linstor -m --output-version v1 rd l | \
jq '.[][].name' | \
xargs -I {} linstor rd sp {} DrbdOptions/ExactSize False
----

[[s-proxmox-ls-upgrades]]
=== Upgrading the LINSTOR Proxmox Plugin

This section describes changes to be aware of or actions that you might need to do when
upgrading an existing installation of the `linstor-proxmox` plugin.

If you need to do a fresh installation, skip this section and continue with
<<s-proxmox-ls-install>>.

[[s-proxmox-ls-upgrades-plug-x-8]]
==== Upgrading Plugin to 8.x

Upgrading to this plugin version requires LINSTOR 1.27.0 or greater.

This version of the LINSTOR Proxmox plugin introduced a new naming scheme for VM images created
on LINSTOR and DRBD backed storage. Existing VMs from earlier plugin versions will still work
with version 8 of the plugin. The naming scheme change requires no user intervention, besides
the user getting accustomed to the new naming scheme.

You can find more details about the naming scheme in <<s-linstor-proxmox-vm-image-naming>>.

Until version 8 of the plugin, it was only possible to migrate data such as VM disk images from
external storage such LVM to LINSTOR and DRBD backed storage if the data was migrated offline.
Starting with version 8 of the plugin, you can migrate data online. For more details, refer to
<<s-linstor-proxmox-migrating-storage>>.

[[s-proxmox-ls-upgrades-plug-x-7]]
==== Upgrading Plugin to 7.x

Version 7 of the plugin uses a LINSTOR controller API that is available from LINSTOR version
1.21.1 onward. Make sure that your LINSTOR setup (controller and satellites) use at least that
version.

[[s-proxmox-ls-upgrades-plug-4-5]]
==== Upgrading Plugin from 4.x to 5.x

Version 5 of the plugin drops compatibility with the legacy configuration options `storagepool`
and `redundancy`. Version 5 *requires* a `resourcegroup` option, and obviously a LINSTOR
resource group. The old options should be removed from the configuration.

Configuring LINSTOR is described in Section <<s-proxmox-ls-ls-configuration>>, a typical example
follows. The following example assumes that the `storagepool` was set to `mypool`, and
`redundancy` to 3.

----
# linstor resource-group create --storage-pool=mypool --place-count=3 drbdMypoolThree
# linstor volume-group create drbdMypoolThree
# vi /etc/pve/storage.cfg
drbd: drbdstorage
   content images,rootdir
   controller 10.11.12.13
   resourcegroup drbdMypoolThree
----

[[s-proxmox-ls-upgrades-plug-5-6]]
==== Upgrading Plugin from 5.x to 6.x

Version 6.0.0 of the plugin drops all code related to the `redundancy` setting. This is handled
by LINSTOR resource groups (`resourcegroup` setting) for a very long time. No change should be
required.

The `controllervm` setting, which was intended for executing a LINSTOR controller in a VM manged
by LINSTOR is gone. Using `drbd-reactor` to realize a highly available LINSTOR controller is
what we suggest.

The settings `statuscache` and `preferlocal` are now enabled by default.

[[s-proxmox-ls-upgrades-pve-5-6]]
==== Upgrading PVE from 5.x to 6.x

With version 6, PVE added additional parameters to some functions and rightfully reset their
"APIAGE". This means that old plugins, while they might actually be usable because they do not
use any of these changed functions, do not work anymore. Upgrade to plugin version 5.2.1 at
least.

