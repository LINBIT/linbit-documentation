[[ch-proxmox-linstor]]
== Proxmox VE での DRBD ボリューム

indexterm:[Proxmox]この章はhttps://github.com/LINBIT/linstor-proxmox.git[LINSTOR
Proxmox Plugin]にあるProxmox VEでのDRBDについて説明します。

[[s-proxmox-ls-overview]]
=== Proxmox VE概要

http://www.proxmox.com/en/[Proxmox
VE]はKVM、Linuxコンテナ、HAとともに使われる簡単で、完全なサーバ仮想化環境を提供します。

'linstor-proxmox'がProxmox用のPerlプラグインで、Proxmox
VEの複数のノードでVMディスクの複製にLINSTORとともに使われます。これにより実行中のVMのライブマイグレーションが無停止でかつ中央にSANディスクを用意せずに数秒で完了します。これはデータがすでに複数のノードに複製をされているからです。

[[s-proxmox-ls-install]]
=== Proxmoxプラグインのインストール

LINBITはProxmox VEユーザのために専用のレポジトリを公開しています。ここにはProxmoxプラグインだけでなくDRBD
SDSカーネルモジュールやユーザスペースユーティリティを含むすべてのDRBD SDSスタックが含まれます。

DRBD9のカーネルモジュールは `dkms` パッケージとして( `drbd-dkms` )インストールされるので、LINBITのレポジトリからインストールする前に `pve-headers` パッケージをインストールします。これに従うことでカーネルモジュールがこの環境用に構築されることが保証されます。最新のProxmoxカーネルでない場合は、現在のカーネルのバージョンに合う( `pve-headers-$(uname -r)` )カーネルヘッダーをインストールする必要があります。あるいは、現在のカーネルに対してdkmsパッケージを再構築する(ヘッダーをインストールする必要あります)必要がある場合は、`apt-get install --reinstall drbd-dkms` を実行します。

LINBITのレポジトリは以下の操作で有効にできます。"$PVERS" はProxmox VEのメジャーバージョンを指定します(例、"5")。

----------------------------
# wget -O- https://packages.linbit.com/package-signing-pubkey.asc | apt-key add -
# PVERS=5 && echo "deb http://packages.linbit.com/proxmox/ proxmox-$PVERS drbd-9.0" > \
	/etc/apt/sources.list.d/linbit.list
# apt-get update && apt-get install linstor-proxmox
----------------------------

[[s-proxmox-ls-ls-configuration]]
=== LINSTORの設定
以下では <<s-linstor-init-cluster>> に従ってLINSTORクラスタ構成がすでになされていると仮定します。最も簡単なケースは、１つのストレージプール(例えば"drbdpool")とすべてのPVEノードに同等のストレージプールを持つことです。複数のストレージプールがある場合、現在はプラグインがそのうちの１つを選択します。今のところそれらを選択はできませんが、通常はDRBDリソースは１つのプールにあります。また、すべてのノードを "Combined" ノードで作成してください。それで "linstor-controller" を１つのノードで、 "linstor-satellite" をすべてのノードで実行してください。

[[s-proxmox-ls-configuration]]
=== Proxmoxプライグインの設定
最後の手順ではProxmox自身を設定します。これは `/etc/pve/storage.cfg`
に以下の設定を加えることで行います。この例では３ノードクラスタを仮定しています。

----------------------------
drbd: drbdstorage
   content images,rootdir
   redundancy 3
   controller 10.11.12.13
----------------------------

"drbd" は固定で変更できません。これによりProxmoxにストレージとしてDRBDを使用することを宣言します。 "drbdstorage"
は変更可能で、Web GUI上で表示されるDRBDストレージの場所を示する名前です。"content" は固定で変更できません。
"redundancy"
はクラスタ内に保つ複製数を指定します。最低３つのノードをもつクラスタを仮定すると、推奨値は３です。例えば５ノードをもつクラスタの場合、すべのノードがデータの３つの複製にアクセスできます。"controller"
はLINSTORコントローラが動作するノードのIPアドレスを指定します。同時に１つのノードだけがLINSTORコントローラとして設定できます。このノードは現在動作していなければならず、すでに動作していない場合は、他のノードでLINSTORコントローラを動作させ、IPアドレスを変更しなければなりません。この問題を扱うより適切な方法があります。詳細はこの章の後半の「コントローラの高可用性」を参照ください。

これらの設定後、Web GUIでVMを作成し、ストレージとして "drbdstorage" を選択することでDRBDボリュームを使用できます。

.注意: DRBDは現時点で **raw** ディスクフォーマットのみをサポートします。

この時点で、VMのライブマイグレーションができます。すべてのノード(ディスクレスノードでも)ですべてのデータにアクセスできるため、わずか数秒で完了します。
VMに負荷がかかっていて、ダーティメモリが常に多い場合は、全体的な処理に少し時間がかかることがあります。しかし、いずれの場合でも、ダウンタイムは最小限で、中断は全く見られません。

[[s-proxmox-ls-HA]]
=== コントローラの高可用性
これ以降の説明では、<<s-proxmox-ls-ls-configuration>>
に基づいてLINSTORとProxmoxプラグインがすでにインストールされていると仮定します。

基本的な考え方は、ProxmoxとそのHA機能によって管理されるVM内でLINSTORコントローラを起動するということです。このVMのストレージはLINSTORによって管理されるDRBDに存在します。

最初の手順はVMのストレージを割り当てることです。通常の方法でVMを作成し、OSの選択時にどのメディアも選択しません。ハードディスクはもちろんDRBD上に
("drbdstorage")
作成します。ディスクスペースは2GBで十分でメモリは1GBを選択します。これらはLINBITがカスタマーに供給するアプライアンスの最小の構成要件です。もし独自のコントローラVMを設定し、リソースに成約がない場合はこれらの最小値を増やしてください。以下の例ではコントローラVMはID100で作成されたと仮定しますが、他のVMを作成した後にこのVMを作成する場合でも問題はありません。

LINBITは作成したストレージを実装するために使うことができるアプライアンスをカスタマーに供給しています。アプライアンスを動作させるためには、最初にシリアルポートを作成する必要があります。ハードウェアをクリックし、追加で、シリアルポートを選択してください。

[[img-pm_add_serial1_controller_vm.png]]
.シリアルポートの追加
image::images/pm_add_serial1_controller_vm.png[]

すべてがうまくいくと、VMの定義は以下のようになります。

[[img-pm_add_serial2_controller_vm.png]]
.シリアルポート付きのVM
image::images/pm_add_serial2_controller_vm.png[]

次の手順ではVMアプライアンスをVMディスクストレージにコピーします。これは `qemu-img` で行います。

IMPORTANT: VM IDを適切なものに変更するようにしてください。

------------------
# qemu-img dd -O raw if=/tmp/linbit-linstor-controller-amd64.img \
  of=/dev/drbd/by-res/vm-100-disk-1/0
------------------

この後、VMを起動しProxmox VNCビューワ経由でVMに接続することができます。デフォルトのユーザ名とパスワードはどちらも "linbit"
です。デフォルトのssh設定を維持しているので、これらのユーザ名とパスワードでsshログインできません。ログインを有効にする場合は、`/etc/ssh/sshd_config`
でこれらを有効にしsshサービスを再起動してください。このVMは "Ubuntu Bionic"
をベースにしているので、`/etc/netplan/config.yaml`
でネットワークの設定(スタティックIPなど)が変更できます。その後、VMにsshできるようになります。

[[img-pm_ssh_controller_vm.png]]
.LINBIT LINSTORコントローラアプライアンス
image::images/pm_ssh_controller_vm.png[]

次の手順で、コントローラVMを既存のクラスタに追加します。

------------
# linstor node create --node-type Controller \
  linstor-controller 10.43.7.254
------------

IMPORTANT: コントローラVMは他のVMと比較して、Proxmoxストレージプラグインによって特別な方法で扱われるので、PVE
HAがVMを開始する前に、すべてのホストがそのVMのストレージにアクセスできるようにします。そうでないとVMを開始できません。詳細は以下を参照ください。

我々のテストクラスタでは、コントローラVMディスクがDRBDストレージに作成され、１つのホストに割り当てられました(割り当てを確認するには
`linstor resource list` を使用)。そして、 `linstor resource create`
で他のノードにもリソースを追加しました。4つのノードで構成されているラボでは、すべてのノードにリソース割り当てを行いましたが、ディスクレスでの割り当てでも問題ありません。経験則として、冗長数を
"3" (それ以上はあまり使われない）に保ち、残りはディスクレスに割り当てます。

この特殊なVMのストレージは、なんらかの方法ですべてのPVEホストで有効になっていなければならいので、すべてのノードで `drbd.service`
を実行し有効にします(この段階ではLINSTORによって制御されていない)。

--------------
# systemctl enable drbd
# systemctl start drbd
--------------

`linstor-satellite` サービスは起動時にすべてのリソースファイル (`*.res`)
を削除し、再度それらを作成します。これはコントローラVMをスタートするために、これらを必要とする `drbd` サービスと競合します。LINSTORは
`-k/--keep-res` オプションをサポートし、ここで指定された正規表現にマッチするファイルを消しません。必要に応じで、systemctl
を通してサービスファイルを編集してください(ファイルを直接編集しないでください)。

--------------
systemctl edit linstor-satellite
[Unit]
After=drbd.service
# "ExecStart" の行を変更し、--keep-res=vm-100 を含むようにします
# "vm-100", 100 は VM ID です。この指定は正規表現であることに注意してください。
# "[Service]" を含て、前の値をリセットします。
# ファイルは以下のようになります。
[Service]
ExecStart=
ExecStart=/usr/share/linstor-server/bin/Satellite --logs=/var/log/linstor-satellite --config-directory=/etc/linstor --keep-res=vm-100
--------------

`linstor-satellite.service` を再起動することを忘れないでください。

最後の手順として、既存のコントローラから新しいコントローラに切り替えます。既存のコントローラを止めて、LINSTORコントローラデータベースをVMホストにコピーします。

-----------
# systemctl stop linstor-controller
# systemctl disable linstor-controller
# scp /var/lib/linstor/* root@10.43.7.254:/var/lib/linstor/
-----------

最後にコントローラVMのコントローラを有効にします。

-----------
# systemctl start linstor-controller # in the VM
# systemctl enable linstor-controller # in the VM
-----------

正しく動作しているか確認するには、PVE ホストで `linstor --controllers=10.43.7.254 node list`
でコントローラのVMに対してクラスタノードを問い合わせることです。ここで "OFFLINE"
と表示されることは問題ありません。この表示方法は将来より適切なものに変更される可能性があります。

最後に重要なこととして、 `/etc/pve/storage.cfg` に "controlervm" を追加し、controller
のIPアドレスをVMのIPアドレスに変更する必要があります。

----------------------------
drbd: drbdstorage
   content images,rootdir
   redundancy 3
   controller 10.43.7.254
   controllervm 100
----------------------------

ここで "controllervm"
の追加設定に注意してください。この設定は、DRBDストレージの他のVMと異なる方法でコントローラVMを処理するようPVEに指示することで、とても重要です。具体的には、コントローラVMの処理にLINSTORストレージプラグインを使用せず、代わりに他の方法を使用するようPVEに指示します。この理由は、単にLINSTORがこの段階では利用できないからです。コントローラVMが起動して実行されると、PVEホストはLINSTORストレージプラグインを使用してDRBDストレージに格納されている残りの仮想マシンを起動することができます。
"controllervm" の設定で正しいVM IDを設定してください。この例では、コントローラVMに割り当てられたID "100" が設定されます。

また、コントローラVMが常に動作していて、定期的に（通常はLINSTORクラスタに変更を加えたときに）バックアップを取っていることを確認することはとても重要です。VMがなくなってバックアップがない場合は、LINSTORクラスタをゼロから再作成する必要があります。

VMを誤って削除してしまうケースを防ぐ方法は見つかっていません。よって、PVE
GUIでVMを削除すると、VMのリストからそのVMが削除されます。ただし、そのような要求はストレージプラグインによって無視されるため、VMディスクはLINSTORクラスタから削除されません。したがって、以前と同じIDでVMを再作成することができます（PVEでVM構成ファイルを再作成し、古いVMで使用されているものと同じDRBDストレージデバイスを割り当てるだけです）。プラグインは
"OK" を返し、古いデータの古いVMを再び使用できます。コントローラVMを削除しないようにし、必要に応じたプロテクトをするようにしてください。


VMによって実行されるコントローラを設定しましたので、VMの１つのインスタンスが常に実行されているようにします。これにはProxmoxのHA機能を使用します。VMをクリックし
"More" から "Manage HA" を選択します。以下のパラメータをコントローラVM用に設定します。

[[img-pm_manage_ha_controller_vm.png]]
.コントローラVMのHA設定
image::images/pm_manage_ha_controller_vm.png[]

Proxmoxクラスタで動作しているノードがあれば、コントローラVMはどこで実行されてもよく、現在動作しているノードがシャットダウン、もしくは停止してしまった場合、Proxmox
HAは他のノードでコントローラVMが起動されるのを保証します。コントローラVMのIPアドレスはもちろん変更されてはなりません。このような場合がないよう、管理者の責任として正しく設定しておきます（例えば、静的IPアドレスの設定、ブリッジインターフェースのDHCPを介して同じIPアドレスを割り振るなど）。

LINSTORクラスタの専用ネットワークを使用している場合、PVEホストで構成されたネットワークインターフェイスがブリッジとして構成されているか確認する必要があります。それらがダイレクトインタフェース（eth0、eth1など）として設定されている場合、クラスタ内の残りのLINSTORノードと通信するようにコントローラVM
vNICを設定することはできません。ブリッジインターフェイスだけが設定できます。

この設定で完全には扱えない1つの制限に、すべてのクラスタノードが再起動する、クラスタ全体の停止(例えば、共通電源障害)があります。Proxmoxはその点でかなり制限されています。VMに対して
"HA Feature" を有効にし、 "Start and Shutdown Order"
で制約を定義することはできます。しかし、両者は完全に分離されています。従って、コントローラVMを起動してから、他のすべてのVMを起動することを保証するのは困難です。

コントローラVMが起動するまでProxmoxプラグイン自身でVMの起動を遅らせる回避策というのは可能かもしれません。すなわち、プラグインがコントローラVMを起動するように要求された場合はそのようにする、そうでなければ、待機し、コントローラにpingを送るという方法です。一見良いアイデアのようですが、シリアライズされた並列でないVM起動環境では動作しません。コントローラVMが起動するようスケジュールされる前にあるVMが起動されなければならないような環境です。これは明らかにデッドロックを引き起こします。

Proxmox側といろいろなオプションを話し合っていますが、今回示した方法は通常の使用法では価値があり、特にpacemakerの設定の複雑さと比較して価値があると考えます。クラスタ全体が同時にダウンしないこのようなシナリオでは、管理者はProxmox
HAサービスがコントローラVMを起動するまで待つだけでよく、その後、すべてのVMが自動的に起動されます。VMはコマンドラインで手動またはスクリプトで起動することができます。
