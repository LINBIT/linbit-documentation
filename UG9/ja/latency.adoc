[[ch-latency]]

== DRBDレイテンシの最適化

この章では、DRBDのレイテンシの最適化について説明します。レイテンシを最小にするためのハードウェアに関する検討事項と、チューニングを行う際の詳細な推奨事項について取り上げます。

[[s-latency-hardware]]
=== ハードウェアの検討事項

DRBDのレイテンシは、配下のI/Oサブシステム(ディスク、コントローラおよび対応するキャッシュ)のレイテンシとレプリケーションネットワークのレイテンシの両方の影響を受けます。

.I/Oサブシステムのレイテンシ
indexterm:[latency]HDDなど _回転するメディア_
の場合、I/Oサブシステムのレイテンシには、主としてディスクの回転速度が影響します。したがって、高速回転のディスクを使用すればI/Oサブシステムのレイテンシが短縮します。

SSDなど _回転しないメディア_
の場合は、フラッシュストレージのコントローラが重要な要素です。次に重要なのは未使用容量です。DRBDの<<s-trim-discard>>を使用すると、コントローラーにどのブロックがリサイクル可能かについて必要な情報を渡すことができます。そのため書き込み要求があった時に、事前に使用可能になっているブロックを
`すぐ`
に使用する事ができ、書き込めるスペースが空くまで待つ必要がありません。footnote:[ローエンドのハードウェアの場合は、10~20%のスペースを未使用のままにしておくと、多少スペースの節約になります。]

[[s-hardware-bbu]]
indexterm:[BBU] 同様に、indexterm:[battery-backed write cache]BBWC
(Battery-Backed Write
Cache)を使用すると、書き込み完了までの時間が短くなり、書き込みのレイテンシを短縮できます。手頃な価格のストレージサブシステムの多くは何らかのバッテリバックアップキャッシュを備えており、管理者がキャッシュのどの部分を読み書き操作に使用するか設定できます。ディスク読み取りキャッシュを完全に無効にし、すべてのキャッシュメモリをディスク書き込みキャッシュとして使用する方法をお勧めします。

.ネットワークレイテンシ
indexterm:[latency]ネットワークレイテンシは、基本的にはホスト間のindexterm:[round-trip-time]indexterm:[RTT]
round-trip time (RTT)RTT (Packet Round-Trip Time)です。
これ以外にもいくつかの要因がありますが、そのほとんどは、DRBDレプリケーションリンクとして推奨する、DRBD専用回線接続の場合は問題になりません。イーサネットリンクには常に一定のレイテンシが発生しますが、ギガビットイーサネットでは通常、100〜200マイクロ秒程度のパケット往復時間です。

ネットワークレイテンシをこれより短くするには、レイテンシが短いネットワークプロトコルを利用する以外ありません。たとえば、Dolphin
SuperSocketsのDolphin
Express、10GBeの直結などを介してDRDBを実行します。これらの場合にはおよそ50マイクロ秒程になります。InfiniBandを利用するとさらにレイテンシが小さくなります。


[[s-latency-overhead-expectations]]
=== レイテンシオーバーヘッドの予測値

スループットに関して、DRDBに関連するレイテンシオーバヘッドを見積もる際には、必ず次の制限を考慮してください。

* DRBDのレイテンシは下位I/Oサブシステムのレイテンシにより制限される。
* DRBDのレイテンシは使用可能なネットワークレイテンシにより制限される。

DRBDの理論上の _最小_ 待ち時間は、上記2つの _合計_
です。footnote:[プロトコルCの場合。他ノードも同様に書き込まなくてはならないため]。さらにわずかなオーバヘッドが追加されますが、これは1%未満だと予測されます。

* たとえば、ローカルディスクサブシステムの書き込みレイテンシが3msで、 ネットワークリンクのレイテンシが0.2msだとします予測されるDRBDの
  レイテンシは3.2msで、 ローカルディスクに書き込むだけのときと比べて約7%レイテンシが増加することになります。

NOTE: CPUのキャッシュミス、コンテキストスイッチなど、他にも待ち時間に影響する要因があります。


[[s-latency-iops]]
=== レイテンシ vs. IOPs

indexterm:[latency]indexterm:[IOPs] _IOPs_ は "I/O operations per second"
の略です。

一般的にマーケティングでは数字が小さくなったとは書かないものです。プレスリリースでは
"レイテンシが10マイクロ秒小さくなって、50マイクロ秒から40秒になりました!" とは書かずに、
"パフォーマンスが25%向上し、20000から25000IPOsになりました!" と書きます。"大きいものは良い"
のような事を言うためIOPsは作られました。

言い方を換えれば、IOPsとレイテンシは相互的なものです。<<s-measure-latency>>で書いた方法は、純粋にシーケンシャルのシングルスレッドのIOロードのIOPsごとのレイテンシです。一方で、よくある他のドキュメントでは、見栄えのいい数字を出すために多重並行処理でのIOPsを使用しているfootnote:[例えば"16スレッドでIO-depthが32"これは512のI/O要求が並行して行われたという事です。]という事を覚えておいてください。こういったトリックを使うのなら、どんなIOPsにでもできます。

そのため、シリアルのシングルスレッドでのレイテンシを敬遠しないでください。 もっとIOPsが欲しい場合には `fio` を `threads=8` で
`io-depth=16`
にするなどの設定をしてください。しかし、そうした数字はデータベースに多数のクライアントからのコネクションが同時にあった場合など以外には、意味がないという事を覚えておいてください。


[[s-latency-tuning]]
=== チューニングの推奨事項

[[s-latency-tuning-cpu-mask]]
==== CPUマスクの設定

DRBDでは、カーネルスレッドの明示的なCPUマスクを設定できます。これは、CPUサイクルがDRBDと競合するアプリケーションの場合に特に役立ちます。

CPUマスクは数字で、バイナリ表現の最下位ビットが第1のCPUを表し、その次のビットが第2のCPUを表します。ビットマスクの設定ビットは、対応するCPUがDRBDによって使用されていることを示し、クリアされたビットは使用されていないことを示します。たとえば、CPUマスク1(
`00000001` )は、DRBDが第1のCPUだけを使用することを示します。マスク12( `00001100`
)はDRBDが第3と第4のCPUを使用することを示します。


次に、リソースのCPUマスク設定の例を示します。

[source, drbd]
----------------------------
resource <resource> {
  options {
    cpu-mask 2;
    ...
  }
  ...
}
----------------------------

IMPORTANT: DRBDとこれを使用するアプリケーションとのCPU競合を最小限に抑えるためには、もちろん、DRBDが使用していないCPUだけをアプリケーションが使用するように設定する必要があります。

一部のアプリケーションは、DRBD自体と同様に設定ファイルでこの設定を行うことができます。アプリケーションによっては、initスクリプトで
`taskset` コマンドを呼び出す必要があります。

[NOTE]
====================

複数のDRBDスレッドに同一のL2/L3キャッシュを使わせることは意味があります。

しかし、CPU番号の物理パーティショニングとの関連付けは必要ありません。 X11環境では `lstopo` (または `hwloc-ls`
)プログラムを、コンソールでは `hwloc-info -v -p` を使うとトポロジーの概要を確認することができます。
====================


[[s-latency-tuning-mtu-size]]
==== ネットワークMTUの変更

レプリケーションネットワークの最大転送ユニット(MTU)サイズをデフォルトの1500バイトよりも大きくすることにはメリットがあります。いわゆるindexterm:[Jumbo
frames] "ジャンボフレームを使用する" ことです。

MTUは、次のコマンドを使用して変更できます。
----------------------------
# ifconfig <interface> mtu <size>
----------------------------
または
----------------------------
# ip link set <interface> mtu <size>
----------------------------

_<interface>_ にはDRBDのレプリケーションに使用するネットワークインタフェース名を指定します。_<size>_ の一般的な値は9000
(バイト)です。

[[s-latency-tuning-deadline-scheduler]]
==== deadline I/Oスケジューラを有効にする

indexterm:[io scheduler] 高性能なライトバックに対応したハードウェアRAIDコントローラを使う場合、CFQの代わりに単純な
`deadline` をI/Oスケジューラに指定する方がDRBDのレイテンシを小さくできることがあります。通常はCFQがデフォルトで有効になっています。

I/Oスケジューラ構成に変更を加える場合は、`/sys` にマウントされる `sysfs` 仮想ファイルシステムを使用できます。スケジューラ構成は
`/sys/block/<device>` に置かれています。<device>はDRBDが使用する下位デバイスです。

`deadline` スケジューラを有効にするには、次のコマンドを使用します。
----------------------------
# echo deadline > /sys/block/<device>/queue/scheduler
----------------------------

次の値も設定することにより、さらに待ち時間を短縮できます。

* フロントマージを無効にします。
+
----------------------------
# echo 0 > /sys/block/<device>/queue/iosched/front_merges
----------------------------

* 読み取りI/O deadlineを150ミリ秒にします(デフォルトは500ms)。
+
----------------------------
# echo 150 > /sys/block/<device>/queue/iosched/read_expire
----------------------------

* 書き込みI/Oデッドラインを1500ミリ秒にします(デフォルトは3000ms)。
+
----------------------------
# echo 1500 > /sys/block/<device>/queue/iosched/write_expire
----------------------------

上記の値の変更により待ち時間が大幅に改善した場合は、システム起動時に自動的に設定されるようにしておくと便利です。indexterm:[Debian
GNU/Linux]Debianおよび indexterm:[Ubuntu Linux]Ubuntuシステムの場合は、 `sysfsutils`
パッケージと `/etc/sysfs.conf` 設定ファイルでこの設定を行うことができます。

グローバルI/Oスケジューラを選択するには、カーネルコマンドラインを使用して `elevator`
オプションを渡します。そのためには、ブートローダ構成(GRUBブートローダを使用する場合通常は `/etc/default/grub`
に格納)を編集し、カーネルブートオプションのリストに elevator=deadline を追加します。
