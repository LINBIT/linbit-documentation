[[ch-troubleshooting]]
== トラブルシューティングとエラーからの回復

この章では、ハードウェアやシステムに障害が発生した場合に必要な手順について説明します。

[[s-hard-drive-failure]]

=== ハードドライブの障害の場合

indexterm:[drive failure]indexterm:[disk
failure]ハードドライブの障害への対処方法は、DRBDがディスクI/Oエラーを処理する方法(<<s-handling-disk-errors>>参照)、また設定されているメタデータの種類(<<s-metadata>>参照)によって異なります。

[NOTE]
==================
ほとんどの場合、ここで取り上げる手順はDRBDを直接物理ハードドライブ上で実行している場合にのみ適用されます。次に示す層の上でDRBDを実行している場合には通常は適用されません。

* MDソフトウェアRAIDセット( `mdadm` を使用してドライブ交換を管理)
* デバイスマッパRAID( `dmraid` 使用)
* ハードウェアRAID機器 (障害が発生したドライブの扱いについてはベンダーの指示に従ってください)
* 一部の非標準デバイスマッパ仮想ブロックデバイス(デバイスマッパのマニュアルを参照してください)
==================

[[s-detach-hard-drive-manual]]
==== 手動でDRBDをハードドライブから切り離す

indexterm:[drbdadm]DRBDが<<fp-io-error-pass-on,I/Oエラーを伝えるように設定>>(非推奨)している場合、まずDRBDリソースを切り離すとよいでしょう。つまり、下位デバイスからの切り離しです。

----------------------------
# drbdadm detach <resource>
----------------------------

`drbdadm status` footnote:[以前は `drbdadm dstate`
コマンドを推奨していました]コマンドを実行することで、現在の状態を確認することができます。

----------------------------
# drbdadm status <resource>
<resource> role:Primary
  volume:0 disk:Diskless
  <peer> role:Secondary
    volume:0 peer-disk:UpToDate
# drbdadm dstate <resource>
Diskless/UpToDate
----------------------------

ディスク障害がプライマリノードで発生した場合、スイッチオーバーと、この手順を組み合わせることもできます。

[[s-detach-hard-drive-auto]]
==== I/Oエラー時の自動切り離し

DRBDが<<fp-io-error-detach,I/Oエラー時に自動的に切り離しを行うように設定>>(推奨オプション)されている場合、
手動での操作なしで、DRBDはすでにリソースを下位ストレージから自動的に切り離しているはずです。その場合でも `drbdadm status`
コマンドを使用して、リソースが実際にディスクレスモードで実行されているか確認します。

[[s-replace-disk-internal-metadata]]
==== 障害が発生したディスクの交換(内部メタデータを使用している場合)

<<s-internal-meta-data,内部メタデータ>>を使用している場合、新しいハードディスクでDRBDデバイスを再構成するだけで十分です。交換したハードディスクのデバイス名が交換前と異なる場合は、DRBD設定ファイルを適切に変更してください。

新しいメタデータを作成してから、リソースを再接続します。 indexterm:[drbdadm]

----------------------------
# drbdadm create-md <resource>
v08 Magic number not found
Writing meta data...
initialising activity log
NOT initializing bitmap
New drbd meta data block sucessfully created.

# drbdadm attach <resource>
----------------------------

新しいハードディスクの完全同期がただちに自動的に始まります。通常のバックグラウンド同期と同様、同期の進行状況を `drbdadm status
--verbose` で監視することができます。

[[s-replace-disk-external-metadata]]
==== 障害の発生したディスクの交換(外部メタデータを使用している場合)

<<s-external-meta-data,外部メタデータ>>を使用している場合でも、手順は基本的に同じです。ただし、DRBDだけではハードドライブが交換されたことを認識できないため、追加の手順が必要です。

----------------------------
# drbdadm create-md <resource>
v08 Magic number not found
Writing meta data...
initialising activity log
NOT initializing bitmap
New drbd meta data block sucessfully created.

# drbdadm attach <resource>
# drbdadm invalidate <resource>
----------------------------

WARNING: `drbdadm invalidate` は、必ず破棄するデータのある側のノードでを実行してください。
このコマンドはローカルのデータを対向ノードからのデータで上書きさせます。つまり、このコマンドを実行すると破棄する側のノードのデータが失われます。

上記の `drbdadm invalidate` コマンドが同期をトリガーします。この場合でも、同期の進行状況は `drbdadm status
--verbose` で確認できます。

[[s-node-failure]]
=== ノード障害に対処する

indexterm:[node
failure]DRBDが(実際のハードウェア障害であれ手動による介入であれ)対向ノードがダウンしていることを検出すると、DRBDは自身のコネクションステータスを
`Connected` から `WFConnection` に変更し、対向ノードが再び現れるのを待ちます。その後、DRBDリソースは
_disconnected mode(切断モード)_
で動作します。切断モードでは、リソースおよびリソースに関連付けられたブロックデバイスが完全に利用可能で、必要に応じて昇格したり降格したりします。ただし、ブロックの変更は対向ノードにレプリケートされません。切断中に変更されたブロックについての内部情報は対向ノードごとにDRBDが格納します。

[[s-temp-node-failure-secondary]]
==== 一時的なセカンダリノードの障害に対処する

indexterm:[node
failure]セカンダリロールでリソースを持っているノードに一時的に障害が生じた場合(たとえばメモリ交換で直るようなメモリの問題)には、障害が発生したノードを修復してオンラインに戻すだけで十分です。修正したノードを起動すると、ノード間の接続が再確立され、停止中にプライマリノードに加えられた変更内容すべてがセカンダリノードに同期されます。

IMPORTANT: この時点で、DRBDの再同期アルゴリズムの性質により、セカンダリノードのリソースの一貫性が一時的に失われます。この短時間の再同期の間は、対向ホストが使用できない場合でも、セカンダリノードをプライマリロールに切り替えることができません。したがって、セカンダリノードの実際のダウンタイムとその後の再同期の間は、クラスタが冗長性を持たない期間になります。

DRBD9では各リソースに3ノード以上が接続することができます。そのため、例えば4ノードでは一度フェイルオーバーしてもまだ2回フェイルオーバーすることができます。

[[s-temp-node-failure-primary]]
==== 一時的なプライマリノードの障害に対処する

indexterm:[node
failure]DRBDからみると、プライマリノードの障害とセカンダリノードの障害はほぼ同じです。生き残ったノードが対向ノードの障害を検出し、切断モードに切り替わります。DRBDは生き残ったノードをプライマリロールに
_昇格しません_ 。これはクラスタ管理システムが管理します。

障害が発生したノードが修復されてクラスタに戻る際に、セカンダリロールになります。すでに述べたように、それ以上の手動による介入は必要ありません。このときもDRBDはリソースのロールを元に戻しません。変更を行うように設定されている場合は、クラス管理システムがこの変更を行います。

プライマリノードに障害が発生すると、DRBDはアクティビティログというメカニズムによってブロックデバイスの整合性を確保します。詳細は<<s-activity-log>>を参照ください。

[[s-perm-node-failure]]
==== 永続的なノード障害に対処する

indexterm:[node failure]ノードに回復不能な問題が発生した場合やノードが永久的に破損した場合は、次の手順を行う必要があります。

* 障害が発生したハードウェアを同様のパフォーマンスと ディスク容量を持つハードウェアと交換します。
+
NOTE: 障害が発生したノードを、それよりパフォーマンスが低いものと置き換えることも可能ですが、お勧めはできません。障害が発生したノードを、それよりディスク容量が小さいものと置き換えることはできません。このような場合には、DRBDを置き換えたノードへの接続は拒否されます。footnote:[結局はレプリケーションできません]

* OSとアプリケーションをインストールします。
* DRBDをインストールし、生き残ったノードから `/etc/drbd.conf` と、全ての `/etc/drbd.d/` を コピーします。
* <<ch-configure>>に記載された手順を、
 <<s-initial-full-sync>>の前まで実行します。

この時点で、デバイスの完全同期を手動で開始する必要はありません。生き残ったプライマリノードへの接続時に、同期が自動的に開始します。

[[s-resolve-split-brain]]
=== スプリットブレインからの手動回復

indexterm:[split
brain]ノード間の接続が可能になると、ノード間で初期ハンドシェイクのプロトコルが交換されます。この時点でDRBDはスプリットブレインが発生したかどうかを判断できます。両方のノードがプライマリロールであるか、もしくは切断中に両方がプライマリロールになったことを検出すると、DRBDは即座にレプリケーション接続を切断します。その場合、システムログにたとえば次のようなメッセージが記録されます。

----------------------------
Split-Brain detected, dropping connection!
----------------------------

スプリットブレインが検出されると、1つのノードは常にindexterm:[StandAlone (connection
state)]indexterm:[connection state]_StandAlone_ の状態でリソースを保持します。もう一方のノードもまた
_StandAlone_ 状態になる(両方のノードが同時にスプリットブレインを検出した場合)、または indexterm:[WFConnection
(connection state)] indexterm:[connection state] _WFConnection_
状態になります(一方のノードがスプリットブレインを検出する前に対向ノードが切断をした場合)。

DRBDがスプリットブレインから自動的に回復するように設定されていない場合は、この時点で手動で介入して、変更内容を破棄するほうのノードを選択する必要があります(このノードは
_スプリットブレインの犠牲ノード_ と呼ばれる)。indexterm:[split brain]この介入は次のコマンドで行います。

[WARNING]
================
ここは現在 _作業中_ です。

構成の変更が行われる予定です。

================



----------------------------
# drbdadm disconnect <resource>
# drbdadm secondary <resource>
# drbdadm connect --discard-my-data <resource>
----------------------------

他方のノード(スプリットブレインのindexterm:[split brain]生存ノード)のコネクションステータスも `StandAlone`
の場合には、次のコマンド実行します。

----------------------------
# drbdadm disconnect <resource>
# drbdadm connect <resource>
----------------------------

ノードがすでに _Connecting_ の状態の場合は自動的に再接続するので、この手順は省略できます。


[IMPORTANT]
================

プレリリースのDRBD9では、 `StandAlone` 状態にあり、再接続しようとしている時にまぎらわしいエラーメッセージが表示されることがあります

---------
Failure: (102) Local address(port) already in use.
---------

これは、まだカーネルモジュールがアクティブな接続データを保持しているためです。

このような場合には `drbdadm disconnect` でネットワーク接続を切り、それから通常通り `drbdadm connect`
で接続してください。
================


スプリットブレインになったのが<<s-three-nodes,スタックリソース>>だった場合には、単に `drbdadm` ではなく、 `drbdadm
--stacked` を使用します。

接続すると、スプリットブレインの犠牲ノードのコネクションステータスがすぐに _SyncTarget_
に変化し、他のノードによって変更内容が上書きされます。

NOTE: スプリットブレインの犠牲ノードは、デバイスのフル同期の対象にはなりません。代わりに、ローカル側での変更がロールバックされ、スプリットブレインの生存ノードに対して加えられた変更が犠牲ノードに伝播されます。

再同期が完了すると、スプリットブレインが解決したとみなされ、2つのノードが再び完全に一致した冗長レプリケーションストレージシステムとして機能します。
