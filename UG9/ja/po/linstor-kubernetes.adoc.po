# SOME DESCRIPTIVE TITLE
# Copyright (C) YEAR Free Software Foundation, Inc.
# This file is distributed under the same license as the PACKAGE package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
msgid ""
msgstr ""
"Project-Id-Version: \n"
"POT-Creation-Date: 2024-05-24 13:18+0900\n"
"PO-Revision-Date: 2024-06-05 17:34+0900\n"
"Last-Translator: 黒木　博 <h-kuroki@sios.com>\n"
"Language-Team: \n"
"Language: ja\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"X-Generator: Poedit 3.4.4\n"

#. type: Title ==
#, no-wrap
msgid "LINSTOR Volumes in Kubernetes"
msgstr "Kubernetes で LINSTOR ボリューム"

#. type: Plain text
msgid "indexterm:[Kubernetes]This chapter describes the usage of LINSTOR(R) in Kubernetes (K8s)  as managed by the operator and with volumes provisioned using the https://github.com/LINBIT/linstor-csi[LINSTOR CSI plugin]."
msgstr "indexterm:[Kubernetes] この章では、オペレータによって管理され、https://github.com/LINBIT/linstor-csi[LINSTOR CSIプラグイン]を使用してプロビジョニングされた ボリュームを使用する方法について、Kubernetes(K8s)でLINSTOR(R)の使用方法について説明します。"

#. type: Plain text
msgid "This chapter goes into great detail regarding all the install time options and various configurations possible with LINSTOR and Kubernetes. The chapter begins with some explanatory remarks and then moves onto deployment instructions. After that, there are instructions for getting started with LINSTOR to configure storage within a Kubernetes deployment. Following that, more advanced topics and configurations, such as snapshots and monitoring, are covered."
msgstr "この章では、LINSTORとKubernetesを使用したすべてのインストールオプションや可能な構成について詳細に説明しています。この章は説明的なコメントから始まり、その後展開手順に移ります。その後、Kubernetes展開内でストレージを構成するためにLINSTORを始める手順が示されています。その後、スナップショットやモニタリングなどのより高度なトピックや構成について説明されています。"

#. type: Title ===
#, no-wrap
msgid "Kubernetes Introduction"
msgstr "Kubernetesの概要"

#. type: Plain text
msgid "_Kubernetes_ is a container orchestrator. Kubernetes defines the behavior of containers and related services, using declarative specifications. In this guide, we will focus on using `kubectl` to manipulate YAML files that define the specifications of Kubernetes objects."
msgstr "Kubernetes_は、コンテナオーケストレーターです。Kubernetes は、宣言した仕様に基づいてコンテナと関連サービスの動作を定義します。このガイドでは、Kubernetes オブジェクトの仕様を定義する AML ファイルを介して `kubectl` を使用することに焦点を当てます。"

#. type: Title ===
#, no-wrap
msgid "Deploying LINSTOR on Kubernetes"
msgstr "Kubernetes への LINSTOR のデプロイ"

#. type: Plain text
msgid "LINBIT(R) provides a LINSTOR Operator to commercial support customers.  The Operator eases deployment of LINSTOR on Kubernetes by installing DRBD(R), managing satellite and controller pods, and other related functions."
msgstr "LINBITは、商用サポートの顧客向けにLINSTOR Operatorを提供しています。このOperatorは、DRBDをインストールし、サテライトおよびコントローラーポッドを管理し、その他関連する機能を提供することで、Kubernetes上でのLINSTORの展開を容易にします。"

#. type: Plain text
msgid "LINBIT's container image repository (https://drbd.io), used by LINSTOR Operator, is only available to LINBIT customers or through LINBIT customer trial accounts.  link:https://linbit.com/contact-us/[Contact LINBIT for information on pricing or to begin a trial]. Alternatively, you can use the LINSTOR SDS upstream project named link:https://github.com/piraeusdatastore/piraeus-operator[Piraeus], without being a LINBIT customer."
msgstr "LINBITのコンテナイメージリポジトリ（https://drbd.io）、LINSTOR Operatorによって使用されていますが、LINBITの顧客またはLINBIT顧客トライアルアカウントを通じてのみ利用可能です。[価格情報の取得やトライアルの開始については、LINBITにお問い合わせください。https://linbit.com/contact-us/]また、LINBITの顧客でなくても、LINSTOR SDSのアップストリームプロジェクトである[Piraeus]（https://github.com/piraeusdatastore/piraeus-operator）を使用することもできます。"

#. type: Plain text
msgid "LINSTOR Operator v2 is the recommended way of deploying LINBIT SDS for Kubernetes on new clusters.  Users of existing Operator v1 deployments should continue to use their Helm deployments and skip to the, <<s-kubernetes-deploy-linstor-operator-v1,Operator v1 deployment instructions>>."
msgstr "LINSTOR Operator v2は、新しいクラスタにLINBIT SDSをKubernetesに展開する推奨方法です。 既存のOperator v1の展開を使用しているユーザーは、引き続きHelmの展開を使用し、<<s-kubernetes-deploy-linstor-operator-v1、Operator v1の展開手順>>にスキップしてください。"

#. type: Title ===
#, no-wrap
msgid "Deploying LINSTOR Operator v2"
msgstr "LINSTOR Operator v2 のデプロイメント"

#. type: Plain text
msgid "You can deploy the LINSTOR Operator v2 by using either link:https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization[the Kustomize tool], integrated with `kubectl`, or else by using link:https://helm.sh/[Helm] and a LINBIT Helm chart."
msgstr "LINSTOR Operator v2を展開する際は、link:https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization[Kustomizeツール]を使用し、`kubectl`と統合するか、あるいはlink:https://helm.sh/[Helm]とLINBITのHelmチャートを使用することができます。"

#. type: Plain text
msgid "If you have already deployed LINSTOR Operator v1 into your cluster, you can upgrade your LINBIT SDS deployment in Kubernetes to Operator v2 by following link:https://charts.linstor.io/migration/[the instructions at charts.linstor.io]."
msgstr "もしすでにあなたのクラスターにLINSTOR Operator v1がデプロイされている場合、KubernetesでLINBIT SDSデプロイメントをOperator v2にアップグレードすることができます。詳細な手順は、リンク:https://charts.linstor.io/migration/[charts.linstor.io]で確認できます。"

#. type: Title ====
#, no-wrap
msgid "Creating the Operator v2 by Using Kustomize"
msgstr "Kustomizeを使用してオペレーターv2を作成する"

#. type: Plain text
msgid "To deploy the Operator, create a `kustomization.yaml` file. This will declare your pull secret for `drbd.io` and allow you to pull in the Operator deployment. The Operator will be deployed in a new namespace `linbit-sds`.  Make sure to replace `MY_LINBIT_USER` and `MY_LINBIT_PASSWORD` with your own credentials. You can find the latest releases on link:https://charts.linstor.io/[charts.linstor.io]."
msgstr "オペレータを展開するには、`kustomization.yaml`ファイルを作成してください。これにより、`drbd.io`のプルシークレットを宣言し、オペレータの展開を許可します。オペレータは新しい名前空間`linbit-sds`に展開されます。`MY_LINBIT_USER`と`MY_LINBIT_PASSWORD`をご自身の認証情報に置き換えてください。最新リリースはlink:https://charts.linstor.io/[charts.linstor.io]で確認できます。"

#. type: Block title
#, no-wrap
msgid "kustomization.yaml"
msgstr "kustomization.yaml"

#. type: delimited block -
#, no-wrap
msgid ""
"apiVersion: kustomize.config.k8s.io/v1beta1\n"
"kind: Kustomization\n"
"namespace: linbit-sds\n"
"resources:\n"
"  - https://charts.linstor.io/static/v2.4.0.yaml # <1>\n"
"generatorOptions:\n"
"  disableNameSuffixHash: true\n"
"secretGenerator:\n"
"  - name: drbdio-pull-secret\n"
"    type: kubernetes.io/dockerconfigjson\n"
"    literals:\n"
"      - .dockerconfigjson={\"auths\":{\"drbd.io\":{\"username\":\"MY_LINBIT_USER\",\"password\":\"MY_LINBIT_PASSWORD\"}}} # <2>\n"
msgstr ""
"apiVersion: kustomize.config.k8s.io/v1beta1\n"
"kind: Kustomization\n"
"namespace: linbit-sds\n"
"resources:\n"
"  - https://charts.linstor.io/static/v2.4.0.yaml # <1>\n"
"generatorOptions:\n"
"  disableNameSuffixHash: true\n"
"secretGenerator:\n"
"  - name: drbdio-pull-secret\n"
"    type: kubernetes.io/dockerconfigjson\n"
"    literals:\n"
"      - .dockerconfigjson={\"auths\":{\"drbd.io\":{\"username\":\"MY_LINBIT_USER\",\"password\":\"MY_LINBIT_PASSWORD\"}}} # <2>\n"

#. type: Plain text
msgid "Replace with the latest release manifest from link:https://charts.linstor.io/[charts.linstor.io]."
msgstr "リンクから最新のリリースマニフェストに置き換えてください：link:https://charts.linstor.io/[charts.linstor.io]"

#. type: Plain text
msgid "Replace `MY_LINBIT_USER` and `MY_LINBIT_PASSWORD` with your link:https://my.linbit.com/[my.linbit.com] credentials."
msgstr "`MY_LINBIT_USER`と`MY_LINBIT_PASSWORD`をあなたの[user.linbit.comの](https://my.linbit.com/)資格情報で置き換えてください。"

#. type: Plain text
msgid "Then, apply the `kustomization.yaml` file, by using `kubectl` command, and wait for the Operator to start:"
msgstr "次に、`kubectl`コマンドを使用して`kustomization.yaml`ファイルを適用し、オペレーターの開始を待ちます:"

#. type: delimited block -
#, no-wrap
msgid ""
"$ kubectl apply -k .\n"
"namespace/linbit-sds created\n"
"...\n"
"$ kubectl -n linbit-sds  wait pod --for=condition=Ready --all\n"
"pod/linstor-operator-controller-manager-6d9847d857-zc985 condition met\n"
msgstr ""
"$ kubectl apply -k .\n"
"namespace/linbit-sds created\n"
"...\n"
"$ kubectl -n linbit-sds  wait pod --for=condition=Ready --all\n"
"pod/linstor-operator-controller-manager-6d9847d857-zc985 condition met\n"

#. type: Plain text
msgid "The Operator is now ready to deploy LINBIT SDS for Kubernetes."
msgstr "オペレーターはLINBIT SDS for Kubernetes を展開する準備が整いました。"

#. type: Title ====
#, no-wrap
msgid "Deploying LINBIT SDS for Kubernetes by Using the Command Line Tool"
msgstr "コマンドラインツールを使用して、LINBIT SDS for Kubernetes をデプロイ"

#. type: Plain text
msgid "Deploying LINBIT SDS for Kubernetes with the Operator v2 is as simple as creating a new `LinstorCluster` resource and waiting for the Operator to complete the setup:"
msgstr "KubernetesでのLINBIT SDSのオペレーターv2をデプロイするのは、新しい`LinstorCluster`リソースを作成して、オペレーターがセットアップを完了するのを待つだけで簡単です:"

#. type: delimited block -
#, no-wrap
msgid ""
"$ kubectl create -f - <<EOF\n"
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec: {}\n"
"EOF\n"
"$ kubectl wait pod --for=condition=Ready -n linbit-sds --timeout=3m --all\n"
msgstr ""
"$ kubectl create -f - <<EOF\n"
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec: {}\n"
"EOF\n"
"$ kubectl wait pod --for=condition=Ready -n linbit-sds --timeout=3m --all\n"

#. type: Plain text
msgid "Output should eventually show that the wait-for condition has been met and the LINBIT SDS pods are up and running."
msgstr "出力には、待機条件が満たされ、LINBIT SDSポッドが起動して実行されていることが最終的に表示されるはずです。"

#. type: delimited block -
#, no-wrap
msgid ""
"pod/ha-controller-4tgcg condition met\n"
"pod/k8s-1-26-10.test condition met\n"
"pod/linstor-controller-76459dc6b6-tst8p condition met\n"
"pod/linstor-csi-controller-75dfdc967d-dwdx6 condition met\n"
"pod/linstor-csi-node-9gcwj condition met\n"
"pod/linstor-operator-controller-manager-6d9847d857-zc985 condition met\n"
msgstr ""
"pod/ha-controller-4tgcg condition met\n"
"pod/k8s-1-26-10.test condition met\n"
"pod/linstor-controller-76459dc6b6-tst8p condition met\n"
"pod/linstor-csi-controller-75dfdc967d-dwdx6 condition met\n"
"pod/linstor-csi-node-9gcwj condition met\n"
"pod/linstor-operator-controller-manager-6d9847d857-zc985 condition met\n"

#. type: Title ====
#, no-wrap
msgid "Creating the Operator v2 by Using Helm"
msgstr "Helmを使用してオペレーターv2を作成する"

#. type: Plain text
msgid "To create the LINSTOR Operator v2 by Using Helm, first enter the following commands to add the `linstor` Helm chart repository:"
msgstr "Helmを使用してLINSTOR Operator v2を作成するには、まず次のコマンドを入力して`linstor` Helmチャートリポジトリを追加してください:"

#. type: delimited block -
#, no-wrap
msgid ""
"$ MY_LINBIT_USER=<my-linbit-customer-username>\n"
"$ MY_LINBIT_PASSWORD=<my-linbit-customer-password>\n"
"$ helm repo add linstor https://charts.linstor.io\n"
msgstr ""
"$ MY_LINBIT_USER=<my-linbit-customer-username>\n"
"$ MY_LINBIT_PASSWORD=<my-linbit-customer-password>\n"
"$ helm repo add linstor https://charts.linstor.io\n"

#. type: Plain text
msgid "Next, enter the following command to install the LINSTOR Operator v2:"
msgstr "次に、次のコマンドを入力して、LINSTOR Operator v2をインストールしてください:"

#. type: delimited block -
#, no-wrap
msgid ""
"$ helm install linstor-operator linstor/linstor-operator \\\n"
"  --set imageCredentials.username=$MY_LINBIT_USER \\\n"
"  --set imageCredentials.password=$MY_LINBIT_PASSWORD \\\n"
"  --wait\n"
msgstr ""
"$ helm install linstor-operator linstor/linstor-operator \\\n"
"  --set imageCredentials.username=$MY_LINBIT_USER \\\n"
"  --set imageCredentials.password=$MY_LINBIT_PASSWORD \\\n"
"  --wait\n"

#. type: Title ====
#, no-wrap
msgid "Deploying LINBIT SDS for Kubernetes by Using Helm"
msgstr "Helmを使用してLINBIT SDS for Kubernetesをデプロイする"

#. type: Plain text
msgid "After output from the command shows that the Operator v2 was installed, you can use Helm to deploy LINBIT SDS by installing the `linbit-sds` chart:"
msgstr "コマンドの出力により、Operator v2 がインストールされたことが確認できたら、Helm を使用して `linbit-sds` チャートをインストールして LINBIT SDS をデプロイできます:"

#. type: delimited block -
#, no-wrap
msgid "$ helm install linbit-sds linstor/linbit-sds\n"
msgstr "$ helm install linbit-sds linstor/linbit-sds\n"

#. type: Plain text
msgid "Output from this final `helm install` command should show a success message."
msgstr "この最終的な `helm install` コマンドの出力には、成功メッセージが表示されるべきです。"

#. type: delimited block -
#, no-wrap
msgid ""
"[...]\n"
"LinstorCluster: linbit-sds\n"
msgstr ""
"[...]\n"
"LinstorCluster: linbit-sds\n"

#. type: delimited block -
#, no-wrap
msgid ""
"Successfully deployed!\n"
"[...]\n"
msgstr ""
"Successfully deployed!\n"
"[...]\n"

#. type: Title ====
#, no-wrap
msgid "Configuring Storage with Operator v2"
msgstr "オペレーターv2を使用したストレージの設定"

#. type: Plain text
msgid "By default, LINBIT SDS for Kubernetes does not configure any storage. To add storage, you can configure a `LinstorSatelliteConfiguration`, which the Operator uses to configure one or more satellites."
msgstr "デフォルトでは、LINBIT SDS for Kubernetesはストレージを構成しません。ストレージを追加するには、Operatorが1つ以上のサテライトを構成するために使用する「LinstorSatelliteConfiguration」を構成できます。"

#. type: Plain text
msgid "The following example creates a simple `FILE_THIN` pool and it does not require any additional set up on the host:"
msgstr "次の例は、シンプルな`FILE_THIN`プールを作成し、ホストへの追加設定は不要です:"

#. type: delimited block -
#, no-wrap
msgid ""
"$ kubectl apply -f - <<EOF\n"
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: storage-pool\n"
"spec:\n"
"  storagePools:\n"
"    - name: pool1\n"
"      fileThinPool:\n"
"        directory: /var/lib/linbit-sds/pool1\n"
"EOF\n"
msgstr ""
"$ kubectl apply -f - <<EOF\n"
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: storage-pool\n"
"spec:\n"
"  storagePools:\n"
"    - name: pool1\n"
"      fileThinPool:\n"
"        directory: /var/lib/linbit-sds/pool1\n"
"EOF\n"

#. type: Plain text
msgid "Other types of storage pools can be configured as well. Refer to link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md#specstoragepools[the examples upstream]."
msgstr "他の種類のストレージプールも設定することができます。詳細は、以下のリンクを参照してください。https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md#specstoragepools[upstreamの例]。"

#  no-wrap
#. type: Title ====
#, no-wrap
msgid "Securing Operator v2 Deployment"
msgstr "オペレーターv2デプロイメントのセキュリティ化"

#. type: Plain text
msgid "By configuring key and certificate based encryption, you can make communication between certain LINSTOR components, for example, between LINSTOR satellite nodes and a LINSTOR controller node, or between the LINSTOR client and the LINSTOR API, more secure."
msgstr "キーと証明書に基づく暗号化を設定することで、例えば、LINSTORのサテライトノードとLINSTORコントローラーノード、またはLINSTORクライアントとLINSTOR API間など、特定のLINSTORコンポーネント間の通信をより安全にすることができます。"

#. type: Title =====
#, no-wrap
msgid "Configuring TLS Between the LINSTOR Controller and Satellite"
msgstr "LINSTORコントローラーとサテライト間のTLSの設定"

#.  https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/how-to/api-tls.md#how-to-configure-tls-for-the-linstor-api
#. type: Plain text
msgid "To secure traffic between the LINSTOR controller and satellite nodes, you can configure TLS, either by using link:https://cert-manager.io/[cert-manager] or link:https://www.openssl.org/[OpenSSL] to create TLS certificates to encrypt the traffic."
msgstr "サテライトノードとLINSTORコントローラー間のトラフィックをセキュアにするためには、link:https://cert-manager.io/[cert-manager]またはlink:https://www.openssl.org/[OpenSSL]を使用してTLSを構成して、トラフィックを暗号化するTLS証明書を作成することができます。"

#. type: Plain text
msgid "====== Provisioning Keys and Certificates By Using cert-manager"
msgstr "======「cert-manager」を使用してキーと証明書をプロビジョニングする"

#. type: Plain text
msgid "This method requires a working cert-manager deployment in your cluster. For an alternative way to provision keys and certificates, see the <<s-kubernetes-provision-tls-using-openssl-v2,OpenSSL>> section below."
msgstr "この方法を使用するには、クラスター内で機能するcert-managerのデプロイメントが必要です。キーと証明書をプロビジョニングする別の方法については、以下の<<s-kubernetes-provision-tls-using-openssl-v2,OpenSSL>>セクションを参照してください。"

#. type: Plain text
msgid "The LINSTOR controller and satellite only need to trust each other. For that reason, you should only have a certificate authority (CA) for those components. Apply the following YAML configuration to your deployment to create a new cert-manager link:https://cert-manager.io/docs/concepts/issuer/[Issuer] resource:"
msgstr "LINSTORコントローラーとサテライトはお互いを信頼する必要があります。そのため、これらのコンポーネントには証明書機関（CA）だけが必要です。新しいcert-managerリンクを作成するために、デプロイメントに以下のYAML構成を適用してください：link:https://cert-manager.io/docs/concepts/issuer/[Issuer]リソース:"

#. type: Block title
#, no-wrap
msgid "linstor-cert-manager.yaml"
msgstr "linstor-cert-manager.yaml"

#. type: Plain text
#, no-wrap
msgid "---\n"
msgstr "---\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: cert-manager.io/v1\n"
"kind: Issuer\n"
"metadata:\n"
"  name: ca-bootstrapper\n"
"  namespace: linbit-sds\n"
"spec:\n"
"  selfSigned: { }\n"
"---\n"
msgstr ""
"apiVersion: cert-manager.io/v1\n"
"kind: Issuer\n"
"metadata:\n"
"  name: ca-bootstrapper\n"
"  namespace: linbit-sds\n"
"spec:\n"
"  selfSigned: { }\n"
"---\n"

#. type: delimited block -
#, no-wrap
msgid ""
"apiVersion: cert-manager.io/v1\n"
"kind: Certificate\n"
"metadata:\n"
"  name: linstor-internal-ca\n"
"  namespace: linbit-sds\n"
"spec:\n"
"  commonName: linstor-internal-ca\n"
"  secretName: linstor-internal-ca\n"
"  duration: 87600h # 10 years\n"
"  isCA: true\n"
"  usages:\n"
"    - signing\n"
"    - key encipherment\n"
"    - cert sign\n"
"  issuerRef:\n"
"    name: ca-bootstrapper\n"
"    kind: Issuer\n"
"---\n"
msgstr ""
"apiVersion: cert-manager.io/v1\n"
"kind: Certificate\n"
"metadata:\n"
"  name: linstor-internal-ca\n"
"  namespace: linbit-sds\n"
"spec:\n"
"  commonName: linstor-internal-ca\n"
"  secretName: linstor-internal-ca\n"
"  duration: 87600h # 10 years\n"
"  isCA: true\n"
"  usages:\n"
"    - signing\n"
"    - key encipherment\n"
"    - cert sign\n"
"  issuerRef:\n"
"    name: ca-bootstrapper\n"
"    kind: Issuer\n"
"---\n"

#. type: delimited block -
#, no-wrap
msgid ""
"apiVersion: cert-manager.io/v1\n"
"kind: Issuer\n"
"metadata:\n"
"  name: linstor-internal-ca\n"
"  namespace: linbit-sds\n"
"spec:\n"
"  ca:\n"
"    secretName: linstor-internal-ca\n"
msgstr ""
"apiVersion: cert-manager.io/v1\n"
"kind: Issuer\n"
"metadata:\n"
"  name: linstor-internal-ca\n"
"  namespace: linbit-sds\n"
"spec:\n"
"  ca:\n"
"    secretName: linstor-internal-ca\n"

#. type: Plain text
msgid "Next, configure the new issuer resource to let the LINSTOR Operator provision the certificates needed to encrypt the controller and satellite traffic, by applying the following YAML configuration:"
msgstr "次に、新しい発行者リソースを構成し、LINSTORオペレーターがコントローラーおよびサテライトのトラフィックを暗号化するために必要な証明書を提供できるように、次のYAML構成を適用してください:"

#. type: Block title
#, no-wrap
msgid "linstor-ca-issuer.yaml"
msgstr "linstor-ca-issuer.yaml"

#. type: delimited block -
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec:\n"
"  internalTLS:\n"
"    certManager:\n"
"      name: linstor-internal-ca\n"
"      kind: Issuer\n"
"---\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec:\n"
"  internalTLS:\n"
"    certManager:\n"
"      name: linstor-internal-ca\n"
"      kind: Issuer\n"
"---\n"

#. type: delimited block -
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: internal-tls\n"
"spec:\n"
"  internalTLS:\n"
"    certManager:\n"
"      name: linstor-internal-ca\n"
"      kind: Issuer\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: internal-tls\n"
"spec:\n"
"  internalTLS:\n"
"    certManager:\n"
"      name: linstor-internal-ca\n"
"      kind: Issuer\n"

#. type: Plain text
msgid "After applying the configurations above to your deployment, you can <<s-kubernetes-tls-configuration-verifying-v2,verify that TLS traffic encryption is working>>."
msgstr "デプロイメントに上記の設定を適用した後、<<s-kubernetes-tls-configuration-verifying-v2,TLSトラフィックの暗号化が機能している>> ことを確認できます。"

#. type: Plain text
msgid "====== Provisioning Keys and Certificates By Using OpenSSL"
msgstr "====== OpenSSLを使用してキーと証明書をプロビジョニング"

#. type: Plain text
msgid "If you completed the <<s-kubernetes-provision-tls-using-cert-manager-v2, Provisioning Keys and Certificates By Using cert-manager>> section above, you can skip this section and go to the <<s-kubernetes-tls-configuration-verifying-v2, Verifying TLS Configuration>> section."
msgstr "もしあなたが上記の <<s-kubernetes-provision-tls-using-cert-manager-v2, Provisioning Keys and Certificates By Using cert-manager>> セクションを完了した場合、このセクションをスキップして <<s-kubernetes-tls-configuration-verifying-v2, Verifying TLS Configuration>> セクションに進むことができます。"

#. type: Plain text
msgid "This method requires the `openssl` program on the command line."
msgstr "この方法はコマンドライン上で`openssl`プログラムを必要とします。"

#. type: Plain text
msgid "First, create a new CA by using a new key and a self-signed certificate. You can change options such as the encryption algorithm and expiry time to suit the requirements of your deployment."
msgstr "最初に、新しいキーと自己署名証明書を使用して新しいCAを作成します。暗号化アルゴリズムや有効期限などのオプションを、デプロイメントの要件に合わせて変更することができます。"

#. type: delimited block -
#, no-wrap
msgid ""
"# openssl req -new -newkey rsa:4096 -days 3650 -nodes -x509 \\\n"
"-subj \"/CN=linstor-internal-ca\" \\\n"
"-keyout ca.key -out ca.crt\n"
msgstr ""
"# openssl req -new -newkey rsa:4096 -days 3650 -nodes -x509 \\\n"
"-subj \"/CN=linstor-internal-ca\" \\\n"
"-keyout ca.key -out ca.crt\n"

#. type: Plain text
msgid "Next, create two new keys, one for the LINSTOR controller, one for all satellites:"
msgstr "次に、新しいキーを2つ作成してください。1つはLINSTORコントローラ用、もう1つはすべてのサテライト用です:"

#. type: delimited block -
#, no-wrap
msgid ""
"# openssl genrsa -out controller.key 4096\n"
"# openssl genrsa -out satellite.key 4096\n"
msgstr ""
"# openssl genrsa -out controller.key 4096\n"
"# openssl genrsa -out satellite.key 4096\n"

#. type: Plain text
msgid "Next, create a certificate for each key, valid for 10 years, signed by the CA that you created earlier:"
msgstr "次に、前に作成したCAによって署名され、有効期限が10年の各キーのための証明書を作成してください:"

#. type: delimited block -
#, no-wrap
msgid ""
"# openssl req -new -sha256 -key controller.key -subj \"/CN=linstor-controller\" -out controller.csr\n"
"# openssl req -new -sha256 -key satellite.key -subj \"/CN=linstor-satellite\" -out satellite.csr\n"
"# openssl x509 -req -in controller.csr -CA ca.crt -CAkey ca.key \\\n"
"-CAcreateserial -out controller.crt -days 3650 -sha256\n"
"# openssl x509 -req -in satellite.csr -CA ca.crt -CAkey ca.key \\\n"
"-CAcreateserial -out satellite.crt -days 3650 -sha256\n"
msgstr ""
"# openssl req -new -sha256 -key controller.key -subj \"/CN=linstor-controller\" -out controller.csr\n"
"# openssl req -new -sha256 -key satellite.key -subj \"/CN=linstor-satellite\" -out satellite.csr\n"
"# openssl x509 -req -in controller.csr -CA ca.crt -CAkey ca.key \\\n"
"-CAcreateserial -out controller.crt -days 3650 -sha256\n"
"# openssl x509 -req -in satellite.csr -CA ca.crt -CAkey ca.key \\\n"
"-CAcreateserial -out satellite.crt -days 3650 -sha256\n"

#. type: Plain text
msgid "Next, create Kubernetes secrets from the created keys and certificates:"
msgstr "次に、作成したキーと証明書からKubernetesのシークレットを作成してください:"

#. type: delimited block -
#, no-wrap
msgid ""
"# kubectl create secret generic linstor-controller-internal-tls -n linbit-sds \\\n"
"--type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=controller.crt \\\n"
"--from-file=tls.key=controller.key\n"
"# kubectl create secret generic linstor-satellite-internal-tls -n linbit-sds \\\n"
"--type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=satellite.crt \\\n"
"--from-file=tls.key=satellite.key\n"
msgstr ""
"# kubectl create secret generic linstor-controller-internal-tls -n linbit-sds \\\n"
"--type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=controller.crt \\\n"
"--from-file=tls.key=controller.key\n"
"# kubectl create secret generic linstor-satellite-internal-tls -n linbit-sds \\\n"
"--type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=satellite.crt \\\n"
"--from-file=tls.key=satellite.key\n"

#. type: Plain text
msgid "Finally, configure the Operator resources to reference the newly created secrets, by applying the following YAML configuration to your deployment:"
msgstr "最後に、Operatorリソースを新たに作成されたシークレットを参照するように設定し、次のYAML構成をデプロイメントに適用してください:"

#. type: Block title
#, no-wrap
msgid "linstor-internal-tls-secret.yaml"
msgstr "linstor-internal-tls-secret.yaml"

#. type: delimited block -
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec:\n"
"  internalTLS:\n"
"    secretName: linstor-controller-internal-tls\n"
"---\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec:\n"
"  internalTLS:\n"
"    secretName: linstor-controller-internal-tls\n"
"---\n"

#. type: delimited block -
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: internal-tls\n"
"spec:\n"
"  internalTLS:\n"
"    secretName: linstor-satellite-internal-tls\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: internal-tls\n"
"spec:\n"
"  internalTLS:\n"
"    secretName: linstor-satellite-internal-tls\n"

#. type: Plain text
msgid "====== Verifying TLS Configuration"
msgstr "====== TLS設定の検証"

#. type: Plain text
msgid "After configuring LINSTOR controller and satellite traffic encryption, you can next verify the secure TLS connection between the LINSTOR controller and a satellite by examining the output of a `kubectl linstor node list` command. If TLS is enabled, the output will show `(SSL)` next to an active satellite address."
msgstr "LINSTORコントローラと<<satellite,サテライト>>トラフィックの暗号化を構成した後、`kubectl linstor node list`コマンドの出力を調べることで、LINSTORコントローラとサテライト間の安全なTLS接続を確認できます。TLSが有効になっている場合、出力にはアクティブなサテライトアドレスの横に`(SSL)`が表示されます。"

#. type: Table
#, no-wrap
msgid ""
"# kubectl linstor node list\n"
"+---------------------------------------------------------------------+\n"
"| Node               | NodeType  | Addresses                 | State  |\n"
"| node01.example.com | SATELLITE | 10.116.72.142:3367 (SSL)  | Online |\n"
"| node02.example.com | SATELLITE | 10.127.183.140:3367 (SSL) | Online |\n"
"| node03.example.com | SATELLITE | 10.125.97.50:3367 (SSL)   | Online |\n"
"+---------------------------------------------------------------------+\n"
"----\n"
msgstr ""
"# kubectl linstor node list\n"
"+---------------------------------------------------------------------+\n"
"| Node               | NodeType  | Addresses                 | State  |\n"
"| node01.example.com | SATELLITE | 10.116.72.142:3367 (SSL)  | Online |\n"
"| node02.example.com | SATELLITE | 10.127.183.140:3367 (SSL) | Online |\n"
"| node03.example.com | SATELLITE | 10.125.97.50:3367 (SSL)   | Online |\n"
"+---------------------------------------------------------------------+\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"NOTE: The above command relies on the `kubectl-linstor` command to simplify entering LINSTOR client commands in Kubernetes. You can install the tool by following the instructions in <<s-kubernetes-kubectl-linstor-utility,Simplifying LINSTOR Client Command Entry>>.\n"
"\n"
"If the output shows `(PLAIN)` rather than `(SSL)`, this indicates that the TLS configuration was not applied successfully. Check the status of the `LinstorCluster` and `LinstorSatellite` resources.\n"
"\n"
"If the output shows `(SSL)`, but the node remains offline, this usually indicates that a certificate is not trusted by the other party. Verify that the controller's `tls.crt` is trusted by the satellite's `ca.crt` and vice versa. The following shell function provides a quick way to verify that one TLS certificate is trusted by another:\n"
"\n"
"----\n"
msgstr ""
"\n"
"注意: 上記のコマンドは、Kubernetes内でLINSTORクライアントコマンドを簡素化するために`kubectl-linstor`コマンドに依存しています。ツールのインストール方法については、<<s-kubernetes-kubectl-linstor-utility,LINSTORクライアントコマンド入力の簡略化>>の手順に従ってインストールすることができます。\n"
"\n"
"もし出力が`(SSL)`ではなく`(PLAIN)`になっている場合、それはTLS構成が正常に適用されていないことを示します。`LinstorCluster`と`LinstorSatellite`リソースの状態を確認してください。\n"
"\n"
"出力に`(SSL)`が表示される場合でも、ノードがオフラインのままである場合、通常は証明書が他の側で信頼されていないことを示しています。コントローラーの `tls.crt` がサテライトの `ca.crt` に信頼されていること、そしてその逆も確認してください。次のシェル関数は、1つのTLS証明書が他のTLS証明書に信頼されているかどうかを迅速に確認する方法を提供します。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"function k8s_secret_trusted_by() {\n"
"\tkubectl get secret -n linbit-sds \\\n"
"    -ogo-template='{{ index .data \"tls.crt\" | base64decode }}' \\\n"
"    \"$1\" > $1.tls.crt\n"
"\tkubectl get secret -n linbit-sds \\\n"
"    -ogo-template='{{ index .data \"ca.crt\" | base64decode }}' \\\n"
"    \"$2\" > $2.ca.crt\n"
"\topenssl verify -CAfile $2.ca.crt $1.tls.crt\n"
"}\n"
"# k8s_secret_trusted_by satellite-tls controller-tls\n"
"----\n"
msgstr ""
"function k8s_secret_trusted_by() {\n"
"\tkubectl get secret -n linbit-sds \\\n"
"    -ogo-template='{{ index .data \"tls.crt\" | base64decode }}' \\\n"
"    \"$1\" > $1.tls.crt\n"
"\tkubectl get secret -n linbit-sds \\\n"
"    -ogo-template='{{ index .data \"ca.crt\" | base64decode }}' \\\n"
"    \"$2\" > $2.ca.crt\n"
"\topenssl verify -CAfile $2.ca.crt $1.tls.crt\n"
"}\n"
"# k8s_secret_trusted_by satellite-tls controller-tls\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"If TLS encryption was properly configured, output from running the above function should be:\n"
"\n"
"----\n"
msgstr ""
"\n"
"TLS暗号化が適切に設定されている場合、上記の関数を実行した出力は次のようになるはずです。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"satellite-tls.tls.crt: OK\n"
"----\n"
msgstr ""
"satellite-tls.tls.crt: OK\n"
"----\n"

#. type: Table
msgid "  The upstream Piraeus project's reference documentation shows all available link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md#specinternaltls[`LinstorCluster`] and link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md#specinternaltls[`LinstorSatelliteConfiguration`] resources options related to TLS."
msgstr "上流のピレウスプロジェクトのリファレンスドキュメントには、TLSに関連するすべての利用可能な link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md#specinternaltls[`LinstorCluster`] および link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md#specinternaltls[`LinstorSatelliteConfiguration`] リソースオプションが表示されます。"

#. type: Title =====
#, no-wrap
msgid "Configuring TLS for the LINSTOR API"
msgstr "LINSTOR APIのTLSの設定"

#. type: Table
#, no-wrap
msgid ""
"// https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/how-to/internal-tls.md\n"
"\n"
"This section describes how to set up TLS for the LINSTOR API. The API, served by the LINSTOR controller, is used by clients such as the CSI Driver and the Operator itself to control the LINSTOR cluster.\n"
"\n"
"To follow the instructions in this section, you should be familiar with:\n"
"\n"
"    - Editing `LinstorCluster` resources\n"
"    - Using either link:https://cert-manager.io/[cert-manager] or OpenSSL to create TLS certificates\n"
"\n"
"[[s-kubernetes-securing-linstor-api-provisioning-keys-cert-manager-v2]]\n"
"====== Provisioning Keys and Certificates By Using cert-manager\n"
"\n"
"This method requires a working link:https://cert-manager.io/[cert-manager] deployment in your cluster. For an alternative way to provision keys and certificates, see the <<s-kubernetes-securing-linstor-api-provisioning-keys-openssl-v2,OpenSSL>> section below.\n"
"\n"
"When using TLS, the LINSTOR API uses client certificates for authentication. It is good practice to have a separate CA just for these certificates. To do this, first apply the following YAML configuration to your deployment to create a certificate issuer.\n"
"\n"
"----\n"
msgstr ""
"// https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/how-to/internal-tls.md\n"
"\n"
"このセクションでは、LINSTOR APIのTLS設定方法について説明します。このAPIは、LINSTORコントローラーによって提供され、CSIドライバーやオペレータ自体などのクライアントによって使用され、LINSTORクラスターを制御します。\n"
"\n"
"このセクションの指示に従うには、以下に慣れている必要があります:\n"
"\n"
"    - Editing `LinstorCluster` resources\n"
"    - Using either link:https://cert-manager.io/[cert-manager] or OpenSSL to create TLS certificates\n"
"\n"
"[[s-kubernetes-securing-linstor-api-provisioning-keys-cert-manager-v2]]\n"
"====== cert-managerを使用してキーと証明書のプロビジョニング\n"
"\n"
"この方法には、クラスター内のcert-managerデプロイメントに対する動作するリンク:https://cert-manager.io/[cert-manager]が必要です。キーと証明書を提供する代替方法については、以下の<<s-kubernetes-securing-linstor-api-provisioning-keys-openssl-v2,OpenSSL>>セクションを参照してください。\n"
"\n"
"TLSを使用する場合、LINSTOR APIはクライアント証明書を認証に使用します。これらの証明書のためだけに別のCAを持つことが良い慣行です。これを行うには、まずデプロイメントに以下のYAML構成を適用して、証明書発行者を作成してください。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: cert-manager.io/v1\n"
"kind: Certificate\n"
"metadata:\n"
"  name: linstor-api-ca\n"
"  namespace: linbit-sds\n"
"spec:\n"
"  commonName: linstor-api-ca\n"
"  secretName: linstor-api-ca\n"
"  duration: 87600h # 10 years\n"
"  isCA: true\n"
"  usages:\n"
"    - signing\n"
"    - key encipherment\n"
"    - cert sign\n"
"  issuerRef:\n"
"    name: ca-bootstrapper\n"
"    kind: Issuer\n"
"---\n"
msgstr ""
"apiVersion: cert-manager.io/v1\n"
"kind: Certificate\n"
"metadata:\n"
"  name: linstor-api-ca\n"
"  namespace: linbit-sds\n"
"spec:\n"
"  commonName: linstor-api-ca\n"
"  secretName: linstor-api-ca\n"
"  duration: 87600h # 10 years\n"
"  isCA: true\n"
"  usages:\n"
"    - signing\n"
"    - key encipherment\n"
"    - cert sign\n"
"  issuerRef:\n"
"    name: ca-bootstrapper\n"
"    kind: Issuer\n"
"---\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: cert-manager.io/v1\n"
"kind: Issuer\n"
"metadata:\n"
"  name: linstor-api-ca\n"
"  namespace: linbit-sds\n"
"spec:\n"
"  ca:\n"
"    secretName: linstor-api-ca\n"
"----\n"
msgstr ""
"apiVersion: cert-manager.io/v1\n"
"kind: Issuer\n"
"metadata:\n"
"  name: linstor-api-ca\n"
"  namespace: linbit-sds\n"
"spec:\n"
"  ca:\n"
"    secretName: linstor-api-ca\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Next, configure this issuer to let the Operator provision the needed certificates, by applying the following configuration.\n"
"\n"
"----\n"
msgstr ""
"\n"
"次に、この発行者を設定して、オペレーターが必要な証明書を提供できるようにし、次の設定を適用してください。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec:\n"
"  apiTLS:\n"
"    certManager:\n"
"      name: linstor-api-ca\n"
"      kind: Issuer\n"
"----\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec:\n"
"  apiTLS:\n"
"    certManager:\n"
"      name: linstor-api-ca\n"
"      kind: Issuer\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"This completes the necessary steps for securing the LINSTOR API with TLS by using cert-manager. Skip to the <<s-kubernetes-securing-linstor-api-verifying-tls-configuration-v2,Verifying LINSTOR API TLS Configuration>> section to verify that TLS is working.\n"
"\n"
"[[s-kubernetes-securing-linstor-api-provisioning-keys-openssl-v2]]\n"
"====== Provisioning Keys and Certificates By Using OpenSSL\n"
"\n"
"This method requires the `openssl` program on the command line. For an alternative way to provision keys and certificates, see the <<s-kubernetes-securing-linstor-api-provisioning-keys-cert-manager-v2,cert-manager>> section above.\n"
"\n"
"First, create a new certificate authority (CA) by using a new key and a self-signed certificate. You can change options such as the encryption algorithm and expiry time to suit the requirements of your deployment.\n"
"\n"
"----\n"
msgstr ""
"\n"
"これで、cert-managerを使用してTLSでLINSTOR APIを保護するために必要な手順が完了しました。 TLSが機能していることを確認するには、<<s-kubernetes-securing-linstor-api-verifying-tls-configuration-v2,Verifying LINSTOR API TLS Configuration>>セクションにスキップしてください。\n"
"\n"
"[[s-kubernetes-securing-linstor-api-provisioning-keys-openssl-v2]]\n"
"====== OpenSSLを使用してキーと証明書をプロビジョニング\n"
"\n"
"この方法では、コマンドライン上で`openssl`プログラムが必要です。キーと証明書を取得する別の方法については、上記の<<s-kubernetes-securing-linstor-api-provisioning-keys-cert-manager-v2,cert-manager>>セクションを参照してください。\n"
"\n"
"最初に、新しいキーと自己署名証明書を使用して新しい証明書機関（CA）を作成します。暗号化アルゴリズムや有効期限などのオプションを変更して、デプロイメントの要件に合わせることができます。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"# openssl req -new -newkey rsa:4096 -days 3650 -nodes -x509 \\\n"
"-subj \"/CN=linstor-api-ca\" \\\n"
"-keyout ca.key -out ca.crt\n"
"----\n"
msgstr ""
"# openssl req -new -newkey rsa:4096 -days 3650 -nodes -x509 \\\n"
"-subj \"/CN=linstor-api-ca\" \\\n"
"-keyout ca.key -out ca.crt\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Next, create two new keys, one for the LINSTOR API server, and one for all LINSTOR API clients:\n"
"\n"
"----\n"
msgstr ""
"\n"
"次に、LINSTOR APIサーバー用の1つとLINSTOR APIクライアント用の1つの新しいキーを作成してください。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"# openssl genrsa -out api-server.key 4096\n"
"# openssl genrsa -out api-client.key 4096\n"
"----\n"
msgstr ""
"# openssl genrsa -out api-server.key 4096\n"
"# openssl genrsa -out api-client.key 4096\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Next, create a certificate for the server. Because the clients might use different shortened service names, you need to specify multiple subject names:\n"
"\n"
"----\n"
msgstr ""
"\n"
"次に、サーバーのための証明書を作成します。クライアントが異なる短縮されたサービス名を使用する可能性があるため、複数のサブジェクト名を指定する必要があります。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"# cat /etc/ssl/openssl.cnf > api-csr.cnf\n"
"# cat >> api-csr.cnf <<EOF\n"
"[ v3_req ]\n"
"subjectAltName = @alt_names\n"
"[ alt_names ]\n"
"DNS.0 = linstor-controller.linbit-sds.svc.cluster.local\n"
"DNS.1 = linstor-controller.linbit-sds.svc\n"
"DNS.2 = linstor-controller\n"
"EOF\n"
"# openssl req -new -sha256 -key api-server.key \\\n"
"-subj \"/CN=linstor-controller\" -config api-csr.cnf \\\n"
"-extensions v3_req -out api-server.csr\n"
"# openssl x509 -req -in api-server.csr -CA ca.crt -CAkey ca.key \\\n"
"-CAcreateserial -config api-csr.cnf \\\n"
"-extensions v3_req -out api-server.crt \\\n"
"-days 3650 -sha256\n"
"----\n"
msgstr ""
"# cat /etc/ssl/openssl.cnf > api-csr.cnf\n"
"# cat >> api-csr.cnf <<EOF\n"
"[ v3_req ]\n"
"subjectAltName = @alt_names\n"
"[ alt_names ]\n"
"DNS.0 = linstor-controller.linbit-sds.svc.cluster.local\n"
"DNS.1 = linstor-controller.linbit-sds.svc\n"
"DNS.2 = linstor-controller\n"
"EOF\n"
"# openssl req -new -sha256 -key api-server.key \\\n"
"-subj \"/CN=linstor-controller\" -config api-csr.cnf \\\n"
"-extensions v3_req -out api-server.csr\n"
"# openssl x509 -req -in api-server.csr -CA ca.crt -CAkey ca.key \\\n"
"-CAcreateserial -config api-csr.cnf \\\n"
"-extensions v3_req -out api-server.crt \\\n"
"-days 3650 -sha256\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"For the client certificate, setting one subject name is enough.\n"
"\n"
"----\n"
msgstr ""
"\n"
"クライアント証明書について、1つのサブジェクト名を設定するだけで十分です。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"# openssl req -new -sha256 -key api-client.key \\\n"
"-subj \"/CN=linstor-client\" -out api-client.csr\n"
"# openssl x509 -req -in api-client.csr \\\n"
"-CA ca.crt -CAkey ca.key -CAcreateserial \\\n"
"-out api-client.crt \\\n"
"-days 3650 -sha256\n"
"----\n"
msgstr ""
"# openssl req -new -sha256 -key api-client.key \\\n"
"-subj \"/CN=linstor-client\" -out api-client.csr\n"
"# openssl x509 -req -in api-client.csr \\\n"
"-CA ca.crt -CAkey ca.key -CAcreateserial \\\n"
"-out api-client.crt \\\n"
"-days 3650 -sha256\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Next, create Kubernetes secrets from the created keys and certificates.\n"
"\n"
"----\n"
msgstr ""
"\n"
"次に、作成した鍵と証明書からKubernetesのシークレットを作成してください。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "kubectl create secret generic linstor-control --type=kubernetes.io/tls \\\n"
#| "  --from-file=ca.crt=ca.crt --from-file=tls.crt=linstor-control.crt \\\n"
#| "  --from-file=tls.key=linstor-control.key\n"
#| "kubectl create secret generic linstor-satellite --type=kubernetes.io/tls \\\n"
#| "  --from-file=ca.crt=ca.crt --from-file=tls.crt=linstor-satellite.crt \\\n"
#| "  --from-file=tls.key=linstor-satellite.key\n"
#| "kubectl create secret generic linstor-api --type=kubernetes.io/tls \\\n"
#| "  --from-file=ca.crt=client-ca.crt --from-file=tls.crt=linstor-api.crt \\\n"
#| "  --from-file=tls.key=linstor-api.key\n"
#| "kubectl create secret generic linstor-client --type=kubernetes.io/tls \\\n"
#| "  --from-file=ca.crt=client-ca.crt --from-file=tls.crt=linstor-client.crt \\\n"
#| "  --from-file=tls.key=linstor-client.key\n"
msgid ""
"# kubectl create secret generic linstor-api-tls -n linbit-sds \\\n"
"--type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=api-server.crt \\\n"
"--from-file=tls.key=api-server.key\n"
"# kubectl create secret generic linstor-client-tls -n linbit-sds \\\n"
"--type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=api-client.crt \\\n"
"--from-file=tls.key=api-client.key\n"
"----\n"
msgstr ""
"# kubectl create secret generic linstor-api-tls -n linbit-sds \\\n"
"--type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=api-server.crt \\\n"
"--from-file=tls.key=api-server.key\n"
"# kubectl create secret generic linstor-client-tls -n linbit-sds \\\n"
"--type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=api-client.crt \\\n"
"--from-file=tls.key=api-client.key\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Finally, configure the Operator resources to reference the newly created secrets. For simplicity, you can configure the same client secret for all components.\n"
"\n"
"----\n"
msgstr ""
"\n"
"最後に、オペレータのリソースを新しく作成したシークレットを参照するように設定してください。シンプルにするため、すべてのコンポーネントに同じクライアントシークレットを設定することができます。.\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec:\n"
"  apiTLS:\n"
"    apiSecretName: linstor-api-tls\n"
"    clientSecretName: linstor-client-tls\n"
"    csiControllerSecretName: linstor-client-tls\n"
"    csiNodeSecretName: linstor-client-tls\n"
"----\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec:\n"
"  apiTLS:\n"
"    apiSecretName: linstor-api-tls\n"
"    clientSecretName: linstor-client-tls\n"
"    csiControllerSecretName: linstor-client-tls\n"
"    csiNodeSecretName: linstor-client-tls\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"[[s-kubernetes-securing-linstor-api-verifying-tls-configuration-v2]]\n"
"====== Verifying LINSTOR API TLS Configuration\n"
"\n"
"You can verify that the API is running, secured by TLS, by manually connecting to the HTTPS endpoint using a `curl` command.\n"
"\n"
"----\n"
msgstr ""
"\n"
"[[s-kubernetes-securing-linstor-api-verifying-tls-configuration-v2]]\n"
"====== LINSTOR API TLS設定の検証\n"
"\n"
"APIが実行され、TLSでセキュリティが確保されていることを、HTTPSエンドポイントに手動で`curl`コマンドを使用して接続することで検証できます。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"# kubectl exec -n linbit-sds deploy/linstor-controller -- \\\n"
"curl --key /etc/linstor/client/tls.key \\\n"
"--cert /etc/linstor/client/tls.crt \\\n"
"--cacert /etc/linstor/client/ca.crt \\\n"
"https://linstor-controller.linbit-sds.svc:3371/v1/controller/version\n"
"----\n"
msgstr ""
"# kubectl exec -n linbit-sds deploy/linstor-controller -- \\\n"
"curl --key /etc/linstor/client/tls.key \\\n"
"--cert /etc/linstor/client/tls.crt \\\n"
"--cacert /etc/linstor/client/ca.crt \\\n"
"https://linstor-controller.linbit-sds.svc:3371/v1/controller/version\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"If the command is successful, the API is using HTTPS, clients are able to connect to the controller with their certificates, and the command output should show something similar to this:\n"
"\n"
"----\n"
msgstr ""
"\n"
"コマンドが成功した場合、APIはHTTPSを使用しており、クライアントは自分の証明書を使用してコントローラに接続できるようになっています。コマンドの出力は、次のような内容が表示されるはずです:\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"{\"version\":\"1.20.2\",\"git_hash\":\"58a983a5c2f49eb8d22c89b277272e6c4299457a\",\"build_time\":\"2022-12-14T14:21:28+00:00\",\"rest_api_version\":\"1.16.0\"}%\n"
"----\n"
msgstr ""
"{\"version\":\"1.20.2\",\"git_hash\":\"58a983a5c2f49eb8d22c89b277272e6c4299457a\",\"build_time\":\"2022-12-14T14:21:28+00:00\",\"rest_api_version\":\"1.16.0\"}%\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"If the command output shows an error, verify that the client certificates are trusted by the API secret, and vice versa. The following shell function provides a quick way to verify that one TLS certificate is trusted by another:\n"
"\n"
"----\n"
msgstr ""
"\n"
"コマンドの出力にエラーが表示された場合、クライアント証明書がAPIシークレットに信頼されているか、その逆も確認してください。次のシェル関数は、1つのTLS証明書が別のTLS証明書に信頼されているかを簡単に確認する方法を提供します。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"function k8s_secret_trusted_by() {\n"
"    kubectl get secret -n linbit-sds \\\n"
"    -ogo-template='{{ index .data \"tls.crt\" | base64decode }}' \\\n"
"    \"$1\" > $1.tls.crt\n"
"    kubectl get secret -n linbit-sds \\\n"
"    -ogo-template='{{ index .data \"ca.crt\" | base64decode }}' \\\n"
"    \"$2\" > $2.ca.crt\n"
"    openssl verify -CAfile $2.ca.crt $1.tls.crt\n"
"}\n"
"# k8s_secret_trusted_by satellite-tls controller-tls\n"
"----\n"
msgstr ""
"function k8s_secret_trusted_by() {\n"
"    kubectl get secret -n linbit-sds \\\n"
"    -ogo-template='{{ index .data \"tls.crt\" | base64decode }}' \\\n"
"    \"$1\" > $1.tls.crt\n"
"    kubectl get secret -n linbit-sds \\\n"
"    -ogo-template='{{ index .data \"ca.crt\" | base64decode }}' \\\n"
"    \"$2\" > $2.ca.crt\n"
"    openssl verify -CAfile $2.ca.crt $1.tls.crt\n"
"}\n"
"# k8s_secret_trusted_by satellite-tls controller-tls\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Another issue might be the API endpoint using a certificate that is not using the expected service name. A typical error message for this issue would be:\n"
"\n"
"[%autofit]\n"
"----\n"
msgstr ""
"\n"
"別の問題は、予期していないサービス名を使用している証明書を使用しているAPIエンドポイントかもしれません。この問題に対する典型的なエラーメッセージは、次のようになります：\n"
"\n"
"[%autofit]\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"curl: (60) SSL: no alternative certificate subject name matches target host name 'linstor-controller.piraeus-datastore.svc'\n"
"----\n"
msgstr ""
"curl: (60) SSL: no alternative certificate subject name matches target host name 'linstor-controller.piraeus-datastore.svc'\n"
"----\n"

#. type: Table
msgid "  In this case, make sure you have specified the right subject names when provisioning the certificates.  All available options are documented in the upstream Piraeus project's reference documentation for link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md#specapitls[`LinstorCluster`]."
msgstr "この場合、証明書を提供する際に正しいサブジェクト名が指定されていることを確認してください。利用可能なすべてのオプションは、上流のPiraeusプロジェクトのリファレンスドキュメントに詳しく記載されています: link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md#specapitls[`LinstorCluster`]。"

#. type: Title =====
#, no-wrap
msgid "Creating a Passphrase For LINSTOR"
msgstr "LINSTORのパスフレーズ作成"

#. type: Table
#, no-wrap
msgid ""
"// https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md#speclinstorpassphrasesecret\n"
"\n"
"LINSTOR can use a passphrase for operations such as <<s-linstor-encrypted-volumes,encrypting volumes>> and storing access credentials for backups.\n"
"\n"
"To configure a LINSTOR passphrase in a Kubernetes deployment, the referenced secret must exist in the same namespace as the operator (by default `linbit-sds`), and have a `MASTER_PASSPHRASE` entry.\n"
"\n"
"The following example YAML configuration for the `.spec.linstorPassphraseSecret` configures a passphrase `example-passphrase`.\n"
"\n"
"IMPORTANT: Choose a different passphrase for your deployment.\n"
"\n"
"----\n"
msgstr ""
"// https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md#speclinstorpassphrasesecret\n"
"\n"
"LINSTORは、<<s-linstor-encrypted-volumes,ボリュームを暗号化>>してアクセス認証情報をバックアップするなどの操作にパスフレーズを使用できます。\n"
"\n"
"KubernetesデプロイメントでLINSTORパスフレーズを設定するには、参照されるシークレットはオペレータと同じ名前空間（デフォルトは `linbit-sds` ）に存在し、`MASTER_PASSPHRASE`エントリを持っている必要があります。\n"
"\n"
"次の例のYAML構成は、`.spec.linstorPassphraseSecret`のパスフレーズを`example-passphrase`に設定します。\n"
"\n"
"重要: デプロイメント用に異なるパスフレーズを選択してください。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: Secret\n"
"metadata:\n"
"  name: linstor-passphrase\n"
"  namespace: linbit-sds\n"
"data:\n"
"  # CHANGE THIS TO USE YOUR OWN PASSPHRASE!\n"
"  # Created by: echo -n \"example-passphrase\" | base64\n"
"  MASTER_PASSPHRASE: ZXhhbXBsZS1wYXNzcGhyYXNl\n"
"---\n"
msgstr ""
"apiVersion: v1\n"
"kind: Secret\n"
"metadata:\n"
"  name: linstor-passphrase\n"
"  namespace: linbit-sds\n"
"data:\n"
"  # CHANGE THIS TO USE YOUR OWN PASSPHRASE!\n"
"  # Created by: echo -n \"example-passphrase\" | base64\n"
"  MASTER_PASSPHRASE: ZXhhbXBsZS1wYXNzcGhyYXNl\n"
"---\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec:\n"
"  linstorPassphraseSecret: linstor-passphrase\n"
"----\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec:\n"
"  linstorPassphraseSecret: linstor-passphrase\n"
"----\n"

#. type: Table
msgid "  [[s-kubernetes-using-crds-v2]]"
msgstr "  [[s-kubernetes-using-crds-v2]]"

#. type: Title ====
#, no-wrap
msgid "Using CustomResourceDefinitions in Operator v2 Deployments"
msgstr "Operator v2 デプロイメントにおける CustomResourceDefinitions の使用"

#. type: Table
msgid ""
"  Within LINSTOR Operator v2 deployments, you can change the cluster state by modifying LINSTOR related Kubernetes `CustomResourceDefinitions` (CRDs) or check the status of a resource. An overview list of these resources follows. Refer to the upstream Piraeus project's API reference (linked for each resource below) for more details.  link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md[`LinstorCluster`]:: This resource "
"controls the state of the LINSTOR cluster and integration with Kubernetes.  link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md[`LinstorSatelliteConfiguration`]:: This resource controls the state of the LINSTOR satellites, optionally applying it to only a subset of nodes.  link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatellite.md[`LinstorSatellite`]:: This "
"resource controls the state of a single LINSTOR satellite. This resource is not intended to be changed directly, rather it is created by the LINSTOR Operator by merging all matching `LinstorSatelliteConfiguration` resources.  link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstornodeconnection.md[`LinstorNodeConnection`]:: This resource controls the state of the LINSTOR node connections.  [[s-kubernetes-next-steps-after-deploying-"
"operator-v2]]"
msgstr ""
"LINSTOR Operator v2のデプロイメント内では、LINSTOR関連のKubernetes `CustomResourceDefinitions` (CRDs) を変更することでクラスターの状態を変更したり、リソースのステータスを確認することができます。これらのリソースの概要リストは以下の通りです。より詳細な情報については、各リソースにリンクされたPiraeusプロジェクトのAPIリファレンスを参照してください。リンク:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md[`LinstorCluster`]:: こ"
"のリソースはLINSTORクラスターの状態とKubernetesとの統合を制御します。リンク:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md[`LinstorSatelliteConfiguration`]:: このリソースは、LINSTORのサテライトの状態を制御し、オプションで一部のノードにのみ適用できます。link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatellite.md[`LinstorSatellite`]:: このリソースは単一のLINSTORサテ"
"ライトの状態を制御します。このリソースは直接変更されることを意図していません。代わりに、すべての一致する`LinstorSatelliteConfiguration`リソースをマージしてLINSTOR Operatorによって作成されます。  link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstornodeconnection.md[`LinstorNodeConnection`]:: このリソースはLINSTORノード接続の状態を制御します。  [[s-kubernetes-next-steps-after-deploying-operator-v2]]"

#. type: Title ====
#, no-wrap
msgid "Next Steps After Deploying LINSTOR Operator v2"
msgstr "LINSTORオペレータv2をデプロイメントした後の次のステップ"

#. type: Table
msgid "  After deploying LINBIT SDS for Kubernetes, you can continue with the <<s-kubernetes-basic-configuration-and-deployment>>, <<s-kubernetes-drbd-module-loader-configuring-v2>>, <<s-kubernetes-drbd-replication-via-host-network-v2>> sections in this chapter, or refer to the available link:https://github.com/piraeusdatastore/piraeus-operator/tree/v2/docs/tutorial[tutorials] in the upstream Piraeus project.  [[s-kubernetes-deploy-linstor-operator-v1]]"
msgstr "LINBIT SDSをKubernetesにデプロイした後、この章の<<s-kubernetes-basic-configuration-and-deployment>>, <<s-kubernetes-drbd-module-loader-configuring-v2>>, <<s-kubernetes-drbd-replication-via-host-network-v2>>セクションに進むか、または上流のPiraeusプロジェクトのリンク：https://github.com/piraeusdatastore/piraeus-operator/tree/v2/docs/tutorial[tutorials]を参照してください。"

#. type: Title ===
#, no-wrap
msgid "Deploying LINSTOR Operator v1"
msgstr "LINSTOR Operator v1のデプロイメント"

#. type: Table
#, no-wrap
msgid ""
"\n"
"IMPORTANT: If you plan to deploy LINSTOR Operator on a new cluster, you should use\n"
"<<s-kubernetes-deploy-linstor-operator-v2, Operator v2>>. If you have already deployed the LINSTOR Operator v2, you can skip this section and proceed to other topics in the chapter, beginning with <<s-kubernetes-deploy-external-controller>>.\n"
"\n"
"The Operator v1 is installed using a Helm v3 chart as follows:\n"
"\n"
"* Create a Kubernetes secret containing your my.linbit.com credentials:\n"
"+\n"
"----\n"
msgstr ""
"\n"
"<<s-kubernetes-deploy-linstor-operator-v2, Operator v2>>. LINSTOR Operator v2 を既にデプロイメント済みの場合は、このセクションをスキップし、<<s-kubernetes-deploy-external-controller>> から始まるこの章の他のトピックに進んでください。\n"
"\n"
"Operator v1 は、次のようにして Helm v3 チャートを使用してインストールされます:\n"
"\n"
"* あなたの my.linbit.com の認証情報を含む Kubernetes シークレットを作成してください。\n"
"+\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \\\n"
#| "  --docker-username=<YOUR_LOGIN> --docker-email=<YOUR_EMAIL> --docker-password=<YOUR_PASSWORD>\n"
msgid ""
"kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \\\n"
"  --docker-username=<YOUR_LOGIN> --docker-email=<YOUR_EMAIL> --docker-password=<YOUR_PASSWORD>\n"
"----\n"
msgstr ""
"kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \\\n"
"  --docker-username=<YOUR_LOGIN> --docker-email=<YOUR_EMAIL> --docker-password=<YOUR_PASSWORD>\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"+\n"
"The name of this secret must match the one specified in the Helm values,\n"
"by default `drbdiocred`.\n"
"\n"
"* Configure the LINSTOR database back end. By default, the chart configures etcd as database\n"
"back end. The Operator can also configure LINSTOR to use\n"
"<<s-kubernetes-linstor-k8s-backend,Kubernetes as datastore>> directly. If you go the etcd\n"
"route, you should configure persistent storage for it:\n"
"** Use an existing storage provisioner with a default `StorageClass`.\n"
"** <<s-kubernetes-etcd-hostpath-persistence,Use `hostPath` volumes>>.\n"
"** Disable persistence, **for basic testing only**. This can be done by adding\n"
"   `--set etcd.persistentVolume.enabled=false` to the `helm install` command below.\n"
"\n"
"* Read <<s-kubernetes-storage, the storage guide>> and configure a basic storage setup for LINSTOR\n"
"\n"
"* Read the <<s-kubernetes-securing-deployment-v1,section on securing the deployment>> and configure as needed.\n"
"\n"
"* Select the appropriate kernel module injector using `--set` with the `helm install` command in the final step.\n"
"\n"
"** Choose the injector according to the distribution you are using. Select the latest version from one of `drbd9-rhel7`, `drbd9-rhel8`, and others, from http://drbd.io/ as appropriate. The `drbd9-rhel8` image should also be used for RHCOS (OpenShift). For the SUSE CaaS Platform use the SLES injector that matches the base system of the CaaS Platform you are using (e.g., `drbd9-sles15sp1`). For example:\n"
"+\n"
"----\n"
msgstr ""
"+\n"
"この秘密の名前は、デフォルトで`drbdiocred`とHelmの値で指定されたものと一致しなければなりません。\n"
"\n"
"* LINSTORのデータベースバックエンドを構成します。デフォルトでは、このチャートはデータベースバックエンドとしてetcdを構成します。オペレータはLINSTORを<<s-kubernetes-linstor-k8s-backend,データストアとしてKubernetesを>>直接使用するようにも設定することができます。etcdの経路を選択する場合は、それに対して永続ストレージを構成する必要があります。\n"
"** 既存のストレージプロビジョナーをデフォルトの `StorageClass` と共に使用してください。\n"
"** <<s-kubernetes-etcd-hostpath-persistence,Use `hostPath` volumes>>.\n"
"** 永続化を無効にしてください。**基本的なテストのみに使用**してください。以下の`helm install`コマンドに `--set etcd.persistentVolume.enabled=false`を追加することで実現できます。\n"
"\n"
"* <<s-kubernetes-storage, ストレージガイド>>を読んで、LINSTORの基本ストレージセットアップを構成してください。\n"
"* <<s-kubernetes-securing-deployment-v1,セキュリティのデプロイメントを保護するセクション>>を読み、必要に応じて構成してください。\n"
"\n"
"* 最終段階で、`helm install`コマンドで`--set`を使用して適切なカーネルモジュールインジェクタを選択します。\n"
"\n"
"** 使用しているディストリビューションに応じてインジェクターを選択してください。適切なバージョンを、`drbd9-rhel7`、`drbd9-rhel8`、その他のいずれかから、http://drbd.io/ で最新バージョンを選択してください。`drbd9-rhel8` イメージは、RHCOS（OpenShift）でも使用する必要があります。SUSE CaaS Platformの場合は、使用しているCaaS Platformのベースシステムに一致するSLESのインジェクターを使用してください（例：`drbd9-sles15sp1`）。例：\n"
"+\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "operator.satelliteSet.kernelModuleInjectionImage=drbd.io/drbd9-rhel8:v9.1.8\n"
msgid ""
"operator.satelliteSet.kernelModuleInjectionImage=drbd.io/drbd9-rhel8:v9.1.8\n"
"----\n"
msgstr ""
"operator.satelliteSet.kernelModuleInjectionImage=drbd.io/drbd9-rhel8:v9.1.8\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"** Only inject modules that are already present on the host machine. If a module is not found, it will be skipped.\n"
"+\n"
"----\n"
msgstr ""
"\n"
"** ホストマシンに既に存在するモジュールのみを挿入します。モジュールが見つからない場合は、スキップされます。.\n"
"+\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "operator.satelliteSet.kernelModuleInjectionMode=DepsOnly\n"
msgid ""
"operator.satelliteSet.kernelModuleInjectionMode=DepsOnly\n"
"----\n"
msgstr ""
"operator.satelliteSet.kernelModuleInjectionMode=DepsOnly\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "Disable kernel module injection if you are installing DRBD by other means. Deprecated by `DepsOnly`"
msgid ""
"\n"
"** Disable kernel module injection if you are installing DRBD by other means. Deprecated by `DepsOnly`\n"
"+\n"
"----\n"
msgstr ""
"\n"
"** D他の手段でDRBDをインストールする場合は、カーネルモジュールのインジェクションを無効にしてください。 `DepsOnly` によって非推奨設定されています。\n"
"+\n"
"----\n"

# , no-wrap
#. type: Table
#, no-wrap
#| msgid "operator.satelliteSet.kernelModuleInjectionMode=None\n"
msgid ""
"operator.satelliteSet.kernelModuleInjectionMode=None\n"
"----\n"
msgstr ""
"operator.satelliteSet.kernelModuleInjectionMode=None\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "Finally create a Helm deployment named `linstor-op` that will set up everything."
msgid ""
"\n"
"* Finally create a Helm deployment named `linstor-op` that will set up everything.\n"
"+\n"
"----\n"
msgstr ""
"\n"
"* 最後にすべてをセットアップする `linstor-op` という名前の Helm デプロイメントを作成します。\n"
"+\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "helm repo add linstor https://charts.linstor.io\n"
#| "helm install linstor-op linstor/linstor\n"
msgid ""
"helm repo add linstor https://charts.linstor.io\n"
"helm install linstor-op linstor/linstor\n"
"----\n"
msgstr ""
"helm repo add linstor https://charts.linstor.io\n"
"helm install linstor-op linstor/linstor\n"
"----\n"

#. type: Table
msgid "Further deployment customization is discussed in the <<s-kubernetes-advanced-deployments,advanced deployment section>> [[s-kubernetes-linstor-k8s-backend]]"
msgstr "デプロイメントのさらなるカスタマイゼーションについては、<<s-kubernetes-advanced-deployments,advanced deployment section>>で議論されています。デプロイメントのさらなるカスタマイゼーションについては、<<s-kubernetes-advanced-deployments,advanced deployment section>>で議論されています。"

#. type: Title ====
#, no-wrap
msgid "Kubernetes Back End for LINSTOR"
msgstr "LINSTOR向けKubernetesバックエンド"

#. type: Table
#, no-wrap
msgid ""
"\n"
"The LINSTOR controller can use the Kubernetes API directly to persist its cluster state. To enable\n"
"this back end, use the following override file during the chart installation:\n"
"\n"
".k8s-backend.yaml\n"
"[source,yaml]\n"
"----\n"
msgstr ""
"\n"
"INSTORコントローラーは、クラスターの状態を永続化するために直接Kubernetes APIを使用することができます。このバックエンドを有効にするには、次のオーバーライドファイルをチャートのインストール中に使用してください。:\n"
"\n"
".k8s-backend.yaml\n"
"[source,yaml]\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "etcd:\n"
#| "  enabled: false\n"
#| "operator:\n"
#| "  controller:\n"
#| "    dbConnectionURL: k8s\n"
msgid ""
"etcd:\n"
"  enabled: false\n"
"operator:\n"
"  controller:\n"
"    dbConnectionURL: k8s\n"
"----\n"
msgstr ""
"etcd:\n"
"  enabled: false\n"
"operator:\n"
"  controller:\n"
"    dbConnectionURL: k8s\n"
"----\n"

#. type: Table
msgid "  TIP: It is possible to migrate an existing cluster that uses an etcd back end to a Kubernetes API back end, by following link:https://charts.linstor.io/migration/1-migrate-db.html[the migration instructions at charts.linstor.io].  [[s-kubernetes-etcd-hostpath-persistence]]"
msgstr "ヒント：既存のetcdバックエンドを使用するクラスタを、Kubernetes APIバックエンドに移行することが可能です。移行手順については、リンク:https://charts.linstor.io/migration/1-migrate-db.html[charts.linstor.io]の移行手順を参照してください。[[s-kubernetes-etcd-hostpath-persistence]]"

#. type: Title ====
#, no-wrap
msgid "Creating Persistent Storage Volumes"
msgstr "永続ストレージボリュームの作成"

#. type: Table
#, no-wrap
msgid ""
"\n"
"You can use the `pv-hostpath` Helm templates to create `hostPath` persistent\n"
"volumes. Create as many PVs as needed to satisfy your configured etcd\n"
"`replicas` (default 1).\n"
"\n"
"Create the `hostPath` persistent volumes, substituting cluster node\n"
"names accordingly in the `nodes=` option:\n"
"\n"
"----\n"
msgstr ""
"\n"
"`pv-hostpath` Helmテンプレートを使用して、`hostPath`永続ボリュームを作成できます。必要なだけ多くのPVを作成して、設定されたetcd `replicas`（デフォルトは1）を満たしてください。\n"
"\n"
"`nodes=`オプションの中で、クラスターノード名に応じて`hostPath`永続ボリュームを作成してください。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "helm repo add linstor https://charts.linstor.io\n"
#| "helm install linstor-etcd linstor/pv-hostpath\n"
msgid ""
"helm repo add linstor https://charts.linstor.io\n"
"helm install linstor-etcd linstor/pv-hostpath\n"
"----\n"
msgstr ""
"helm repo add linstor https://charts.linstor.io\n"
"helm install linstor-etcd linstor/pv-hostpath\n"
"----\n"

#. type: Table
msgid ""
"  By default, a PV is created on every `control-plane` node. You can manually select the storage nodes by passing `--set \"nodes={<NODE0>,<NODE1>,<NODE2>}\"` to the install command.  NOTE: The correct value to reference the node is the value of the `kubernetes.io/hostname` label. You can list the value for all nodes by running `kubectl get nodes -o custom-columns=\"Name:{.metadata.name},NodeName:{.metadata.labels['kubernetes\\.io/hostname']}\"` [[s-kubernetes-"
"existing-database]]"
msgstr ""
"デフォルトでは、`control-plane` ノードごとに PV が作成されます。インストールコマンドに `--set \"nodes={<NODE0>,<NODE1>,<NODE2>}\"` を渡すことで、ストレージノードを手動で選択することができます。注意: ノードを参照する正しい値は、`kubernetes.io/hostname` ラベルの値です。すべてのノードの値をリストアップするには、`kubectl get nodes -o custom-columns=\"Name:{.metadata.name},NodeName:{.metadata.labels['kubernetes\\.io/hostname']}\"` を実行してください。 [[s-kubernetes-"
"existing-database]]"

#. type: Title ====
#, no-wrap
msgid "Using an Existing Database"
msgstr "既存のデータベースの使用"

#. type: Table
#, no-wrap
msgid ""
"\n"
"LINSTOR can connect to an existing PostgreSQL, MariaDB or etcd database. For\n"
"instance, for a PostgreSQL instance with the following configuration:\n"
"\n"
"----\n"
msgstr ""
"\n"
"LINSTORは既存のPostgreSQL、MariaDB、またはetcdデータベースに接続することができます。たとえば、次の構成でPostgreSQLインスタンスに接続する場合:\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "POSTGRES_DB: postgresdb\n"
#| "POSTGRES_USER: postgresadmin\n"
#| "POSTGRES_PASSWORD: admin123\n"
msgid ""
"POSTGRES_DB: postgresdb\n"
"POSTGRES_USER: postgresadmin\n"
"POSTGRES_PASSWORD: admin123\n"
"----\n"
msgstr ""
"POSTGRES_DB: postgresdb\n"
"POSTGRES_USER: postgresadmin\n"
"POSTGRES_PASSWORD: admin123\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "The Helm chart can be configured to use this database rather than deploying an etcd cluster, by adding the following to the Helm install command:"
msgid ""
"\n"
"The Helm chart can be configured to use this database rather than deploying an\n"
"etcd cluster, by adding the following to the Helm install command:\n"
"\n"
"----\n"
msgstr ""
"\n"
"ヘルム・チャートは、次の内容をHelmインストールコマンドに追加することで、このデータベースをデプロイメントするためにetcdクラスターの代わりに構成することができます。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "--set etcd.enabled=false --set \"operator.controller.dbConnectionURL=jdbc:postgresql://postgres/postgresdb?user=postgresadmin&password=admin123\"\n"
msgid ""
"--set etcd.enabled=false --set \"operator.controller.dbConnectionURL=jdbc:postgresql://postgres/postgresdb?user=postgresadmin&password=admin123\"\n"
"----\n"
msgstr ""
"--set etcd.enabled=false --set \"operator.controller.dbConnectionURL=jdbc:postgresql://postgres/postgresdb?user=postgresadmin&password=admin123\"\n"
"----\n"

#. type: Table
msgid "  [[s-kubernetes-configuring-storage-v1]]"
msgstr "ヘルム・チャートは、次の内容をHelmインストールコマンドに追加することで、このデータベースをデプロイメントするためにetcdクラスターの代わりに構成することができます。"

#. type: Title ====
#, no-wrap
msgid "Configuring Storage With Operator v1"
msgstr "Operator v1でのストレージの設定"

#. type: Table
msgid "  The LINSTOR Operator v1 can automate some basic storage set up for LINSTOR."
msgstr "LINSTOR Operator v1は、LINSTORの基本的なストレージ設定を自動化できます。"

#. type: Title =====
#, no-wrap
msgid "Configuring Storage Pool Creation"
msgstr "ストレージプール作成の構成"

#. type: Table
#, no-wrap
#| msgid "The LINSTOR Operator can be used to create LINSTOR storage pools. Creation is under control of the LinstorSatelliteSet resource:"
msgid ""
"\n"
"The LINSTOR Operator can be used to create LINSTOR storage pools. Creation is under control of the\n"
"`LinstorSatelliteSet` resource:\n"
"\n"
"[source]\n"
"----\n"
msgstr ""
"\n"
"LINSTOR OperatorはLINSTORストレージプールを作成するために使用できます。作成は`LinstorSatelliteSet`リソースの制御のもとで行われます。\n"
"\n"
"[source]\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "$ kubectl get LinstorSatelliteSet.linstor.linbit.com linstor-op-ns -o yaml\n"
#| "kind: LinstorSatelliteSet\n"
#| "metadata:\n"
#| "..\n"
#| "spec:\n"
#| "  ..\n"
#| "  storagePools:\n"
#| "    lvmPools:\n"
#| "    - name: lvm-thick\n"
#| "      volumeGroup: drbdpool\n"
#| "    lvmThinPools:\n"
#| "    - name: lvm-thin\n"
#| "      thinVolume: thinpool\n"
#| "      volumeGroup: \"\"\n"
#| "    zfsPools:\n"
#| "    - name: my-linstor-zpool\n"
#| "      zPool: for-linstor\n"
#| "      thin: true\n"
msgid ""
"$ kubectl get LinstorSatelliteSet.linstor.linbit.com linstor-op-ns -o yaml\n"
"kind: LinstorSatelliteSet\n"
"metadata:\n"
"[...]\n"
"spec:\n"
"  [...]\n"
"  storagePools:\n"
"    lvmPools:\n"
"    - name: lvm-thick\n"
"      volumeGroup: drbdpool\n"
"    lvmThinPools:\n"
"    - name: lvm-thin\n"
"      thinVolume: thinpool\n"
"      volumeGroup: \"\"\n"
"    zfsPools:\n"
"    - name: my-linstor-zpool\n"
"      zPool: for-linstor\n"
"      thin: true\n"
"----\n"
msgstr ""
"$ kubectl get LinstorSatelliteSet.linstor.linbit.com linstor-op-ns -o yaml\n"
"kind: LinstorSatelliteSet\n"
"metadata:\n"
"[...]\n"
"spec:\n"
"  [...]\n"
"  storagePools:\n"
"    lvmPools:\n"
"    - name: lvm-thick\n"
"      volumeGroup: drbdpool\n"
"    lvmThinPools:\n"
"    - name: lvm-thin\n"
"      thinVolume: thinpool\n"
"      volumeGroup: \"\"\n"
"    zfsPools:\n"
"    - name: my-linstor-zpool\n"
"      zPool: for-linstor\n"
"      thin: true\n"
"----\n"

#. type: Title =====
#, no-wrap
msgid "Creating Storage Pools at Installation Time"
msgstr "インストール時のストレージプール作成"

#. type: Table
#, no-wrap
#| msgid "At installation time, by setting the value of `operator.satelliteSet.storagePools` when running the `helm install` command."
msgid ""
"\n"
"At installation time, by setting the value of `operator.satelliteSet.storagePools` when running the `helm install` command.\n"
"\n"
"First create a file with the storage configuration such as:\n"
"\n"
"[source,yaml]\n"
"----\n"
msgstr ""
"\n"
"インストール時に、`helm install`コマンドを実行する際に、`operator.satelliteSet.storagePools`の値を設定します。\n"
"\n"
"最初に、ストレージ構成などの情報を含むファイルを作成してください。\n"
"\n"
"[source,yaml]\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "operator:\n"
#| "  satelliteSet:\n"
#| "    storagePools:\n"
#| "      lvmPools:\n"
#| "      - name: lvm-thick\n"
#| "        volumeGroup: drbdpool\n"
msgid ""
"operator:\n"
"  satelliteSet:\n"
"    storagePools:\n"
"      lvmPools:\n"
"      - name: lvm-thick\n"
"        volumeGroup: drbdpool\n"
"----\n"
msgstr ""
"operator:\n"
"  satelliteSet:\n"
"    storagePools:\n"
"      lvmPools:\n"
"      - name: lvm-thick\n"
"        volumeGroup: drbdpool\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"This file can be passed to the Helm installation by entering the following command:\n"
"\n"
"[source]\n"
"----\n"
msgstr ""
"\n"
"このファイルは、次のように helm インストールに渡すことができます:\n"
"\n"
"[source]\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "helm install -f <file> linstor-op linstor/linstor\n"
msgid ""
"helm install -f <file> linstor-op linstor/linstor\n"
"----\n"
msgstr ""
"helm install -f <file> linstor-op linstor/linstor\n"
"----\n"

#. type: Title =====
#, no-wrap
msgid "Creating Storage Pools After Installation"
msgstr "ストレージプール作成の構成"

#. type: Table
#, no-wrap
msgid ""
"\n"
"On a cluster with the operator already configured (that is, after `helm install`),\n"
"you can edit the `LinstorSatelliteSet` configuration by entering the following command:\n"
"\n"
"[source]\n"
"----\n"
msgstr ""
"\n"
"オペレーターがすでに構成されているクラスター（つまり、 `helm install` の後）では、次のようにして LinstorSatelliteSet 構成を編集できます。\n"
"\n"
"[source]\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "$ kubectl edit LinstorSatelliteSet.linstor.linbit.com <satellitesetname>\n"
msgid ""
"$ kubectl edit LinstorSatelliteSet.linstor.linbit.com <satellitesetname>\n"
"----\n"
msgstr ""
"$ kubectl edit LinstorSatelliteSet.linstor.linbit.com <satellitesetname>\n"
"----\n"

#. type: Table
msgid "  The storage pool configuration can be updated as in the example above."
msgstr "ストレージプールの構成は、上記の例のように更新することができます。"

#. type: Title =====
#, no-wrap
msgid "Preparing Physical Devices"
msgstr "物理デバイスの準備"

#. type: Table
#, no-wrap
#| msgid "By default, LINSTOR expects the referenced VolumeGroups, ThinPools and so on to be present. You can use the `devicePaths: []` option to let LINSTOR automatically prepare devices for the pool. Eligible for automatic configuration are block devices that:"
msgid ""
"\n"
"By default, LINSTOR expects the referenced VolumeGroups, ThinPools and so on to be present. You can use the\n"
"`devicePaths: []` option to let LINSTOR automatically prepare devices for the pool. Eligible for automatic configuration\n"
"are block devices that:\n"
"\n"
"* Are a root device (no partition)\n"
"* do not contain partition information\n"
"* have more than 1 GiB\n"
"\n"
"To enable automatic configuration of devices, set the `devicePaths` key on `storagePools` entries:\n"
"\n"
"[source,yaml]\n"
"----\n"
msgstr ""
"\n"
"デフォルトでは、LINSTORは参照されたVolumeGroups、ThinPoolsなどが存在することを想定しています。`devicePaths: []`オプションを使用すると、LINSTORにデバイスをプールのために自動的に準備させることができます。自動構成の対象となるのは、次のようなブロックデバイスです：\n"
"\n"
"* ルートデバイス（パーティションなし）\n"
"* パーティション情報が含まれていない\n"
"* 1ジガバイトを超える容量\n"
"\n"
"デバイスの自動構成を有効にするには、`storagePools`エントリーの`devicePaths`キーを設定します。\n"
"\n"
"[source,yaml]\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "  storagePools:\n"
#| "    lvmPools:\n"
#| "    - name: lvm-thick\n"
#| "      volumeGroup: drbdpool\n"
#| "      devicePaths:\n"
#| "      - /dev/vdb\n"
#| "    lvmThinPools:\n"
#| "    - name: lvm-thin\n"
#| "      thinVolume: thinpool\n"
#| "      volumeGroup: linstor_thinpool\n"
#| "      devicePaths:\n"
#| "      - /dev/vdc\n"
#| "      - /dev/vdd\n"
msgid ""
"  storagePools:\n"
"    lvmPools:\n"
"    - name: lvm-thick\n"
"      volumeGroup: drbdpool\n"
"      devicePaths:\n"
"      - /dev/vdb\n"
"    lvmThinPools:\n"
"    - name: lvm-thin\n"
"      thinVolume: thinpool\n"
"      volumeGroup: linstor_thinpool\n"
"      devicePaths:\n"
"      - /dev/vdc\n"
"      - /dev/vdd\n"
"----\n"
msgstr ""
"  storagePools:\n"
"    lvmPools:\n"
"    - name: lvm-thick\n"
"      volumeGroup: drbdpool\n"
"      devicePaths:\n"
"      - /dev/vdb\n"
"    lvmThinPools:\n"
"    - name: lvm-thin\n"
"      thinVolume: thinpool\n"
"      volumeGroup: linstor_thinpool\n"
"      devicePaths:\n"
"      - /dev/vdc\n"
"      - /dev/vdd\n"
"----\n"

#. type: Table
msgid "  Currently, this method supports creation of LVM and LVMTHIN storage pools."
msgstr "現在、このメソッドは LVM および LVMTHIN ストレージプールの作成をサポートしています。"

#. type: Title =====
#, no-wrap
msgid "Configuring LVM Storage Pools"
msgstr "LVM ストレージプールの設定"

#. type: Table
msgid ""
"  The available keys for `lvmPools` entries are: * `name` name of the LINSTOR storage pool. [Required] * `volumeGroup` name of the VG to create. [Required] * `devicePaths` devices to configure for this pool. Must be empty and >= 1GiB to be recognized. [Optional] * `raidLevel` LVM raid level. [Optional] * `vdo` Enable [VDO] (requires VDO tools in the satellite). [Optional] * `vdoLogicalSizeKib` Size of the created VG (expected to be bigger than the backing devices "
"by using VDO). [Optional] * `vdoSlabSizeKib` Slab size for VDO. [Optional] [VDO]: https://www.redhat.com/en/blog/look-vdo-new-linux-compression-layer"
msgstr ""
"`lvmPools`エントリの利用可能なキーは次の通りです。* `name` LINSTORストレージプールの名前。[必須] * `volumeGroup` 作成するVGの名前。[必須] * `devicePaths` このプールに構成するデバイス。認識されるには空であり、1GiB以上でなければなりません。[オプション] * `raidLevel` LVMレイドレベル。[オプション] * `vdo` VDOの有効化（サテライトにVDOツールが必要）。[オプション] * `vdoLogicalSizeKib` 作成されるVGのサイズ（VDOを使用してバッキングデバイスより大きいことが予想されます）。"
"[オプション] * `vdoSlabSizeKib` VDOのスラブサイズ。[オプション] [VDO]: https://www.redhat.com/en/blog/look-vdo-new-linux-compression-layer"

#. type: Title =====
#, no-wrap
msgid "Configuring LVM Thin Pools"
msgstr "LVM Thin Poolの設定"

#. type: Table
msgid ""
"  * `name` name of the LINSTOR storage pool. [Required] * `volumeGroup` VG to use for the thin pool. If you want to use `devicePaths`, you must set this to `\"\"`. This is required because LINSTOR does not allow configuration of the VG name when preparing devices. `thinVolume` name of the thin pool. [Required] * `devicePaths` devices to configure for this pool. Must be empty and >= 1GiB to be recognized. [Optional] * `raidLevel` LVM raid level. [Optional] NOTE: "
"The volume group created by LINSTOR for LVM thin pools will always follow the scheme \"linstor_$THINPOOL\"."
msgstr ""
"* `name`: LINSTOR ストレージプールの名前。[必須] * `volumeGroup`: シンプールに使用するVG。`devicePaths` を使用する場合は、これを`\"\"`に設定する必要があります。これは、デバイスの準備時には、LINSTORがVG名の構成を許可していないためです。[必須] * `thinVolume`: シンプールの名前。[必須] * `devicePaths`: このプールに構成するデバイス。認識されるには空であり、1GiB以上である必要があります。[オプション] * `raidLevel`: LVMのRAIDレベル。[オプション] 注意: LINSTORがLVMシンプー"
"ル用に作成するボリュームグループは常に\"linstor_$THINPOOL\"の形式になります。"

#. type: Title =====
#, no-wrap
msgid "Configuring ZFS Storage Pools"
msgstr "ZFS ストレージプールの設定"

#. type: Table
msgid "  * `name` name of the LINSTOR storage pool. [Required] * `zPool` name of the `zpool` to use. Must already be present on all machines. [Required] * `thin` `true` to use thin provisioning, `false` otherwise. [Required]"
msgstr ""
"* `name` LINSTORストレージプールの名前。[必須]\n"
"* `zPool` 使用する`zpool`の名前。すべてのマシンにすでに存在している必要があります。[必須]\n"
"* `thin` スリンプロビジョニングを使用する場合は`true`、それ以外は`false`。[必須]"

#. type: Title =====
#, no-wrap
msgid "Automatic Storage Type Provisioning (DEPRECATED)"
msgstr "`automaticStorageType` の使用（非推奨）"

#. type: Table
msgid ""
"  _ALL_ eligible devices will be prepared according to the value of `operator.satelliteSet.automaticStorageType`, unless they are already prepared using the `storagePools` section. Devices are added to a storage pool based on the device name (that is, all `/dev/nvme1` devices will be part of the pool `autopool-nvme1`)  The possible values for `operator.satelliteSet.automaticStorageType`: * `None` no automatic set up (default)  * `LVM` create a LVM (thick) storage "
"pool * `LVMTHIN` create a LVM thin storage pool * `ZFS` create a ZFS based storage pool (**UNTESTED**)  [[s-kubernetes-securing-deployment-v1]]"
msgstr ""
"すべての適格なデバイスは、すでに`storagePools`セクションを使用して準備されていない限り、`operator.satelliteSet.automaticStorageType`の値に従って準備されます。デバイスはデバイス名に基づいてストレージプールに追加されます（つまり、すべての `/dev/nvme1` デバイスは `autopool-nvme1` プールの一部となります）。`operator.satelliteSet.automaticStorageType`の可能な値は以下の通りです：\n"
"* `None`: 自動設定なし（デフォルト）\n"
"* `LVM`: LVM（厚い）ストレージプールを作成\n"
"* `LVMTHIN`: LVMスリム型ストレージプールを作成\n"
"* `ZFS`: ZFSベースのストレージプールを作成（**未検証**）  \n"
"[[s-kubernetes-securing-deployment-v1]]"

#  no-wrap
#. type: Title ====
#, no-wrap
msgid "Securing Operator v1 Deployment"
msgstr "オペレーターv1デプロイメントのセキュリティ確保"

#. type: Table
msgid "  This section describes the different options for enabling security features available when using a LINSTOR Operator v1 deployment (<<s-kubernetes-deploy-linstor-operator-v1,using Helm>>) in Kubernetes."
msgstr "このセクションでは、KubernetesでHelmを使用してLINSTOR Operator v1をデプロイ（<<s-kubernetes-deploy-linstor-operator-v1,using Helm>>) する際に利用可能なセキュリティ機能を有効にするためのさまざまなオプションについて説明します。"

#. type: Title =====
#, no-wrap
msgid "Secure Communication with an Existing etcd Instance"
msgstr "既存の etcd インスタンスとの安全な通信"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Secure communication to an `etcd` instance can be enabled by providing a CA certificate to the operator in form of a\n"
"Kubernetes secret. The secret has to contain the key `ca.pem` with the PEM encoded CA certificate as value.\n"
"\n"
"The secret can then be passed to the controller by passing the following argument to `helm install`\n"
"\n"
"----\n"
msgstr ""
"\n"
"`etcd`インスタンスへの安全な通信は、Kubernetesシークレットの形でCA証明書をオペレータに提供することで有効になります。シークレットには、PEMエンコードされたCA証明書を値とする`ca.pem`キーを含める必要があります。\n"
"\n"
"その秘密は、次の引数を`helm install`に渡すことでコントローラーに渡すことができます。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "--set operator.controller.dbCertSecret=<secret name>\n"
msgid ""
"--set operator.controller.dbCertSecret=<secret name>\n"
"----\n"
msgstr ""
"--set operator.controller.dbCertSecret=<secret name>\n"
"----\n"

#. type: Title =====
#, no-wrap
msgid "Authentication with `etcd` Using Certificates"
msgstr "証明書を使用した `etcd` での認証"

#. type: Table
#, no-wrap
msgid ""
"\n"
"If you want to use TLS certificates to authenticate with an `etcd` database, you need to set the following option on\n"
"Helm install:\n"
"\n"
"----\n"
msgstr ""
"\n"
"`etcd`データベースでTLS証明書を使用して認証する場合は、Helmのインストール時に次のオプションを設定する必要があります:\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "--set operator.controller.dbUseClientCert=true\n"
msgid ""
"--set operator.controller.dbUseClientCert=true\n"
"----\n"
msgstr ""
"--set operator.controller.dbUseClientCert=true\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"If this option is active, the secret specified in the above section must contain two additional keys:\n"
"\n"
"* `client.cert` PEM formatted certificate presented to `etcd` for authentication\n"
"* `client.key` private key **in PKCS8 format**, matching the above client certificate.\n"
"\n"
"Keys can be converted into PKCS8 format using `openssl`:\n"
"\n"
"----\n"
msgstr ""
"\n"
"もし、このオプションが有効になっている場合、上記セクションで指定されたシークレットには、2つの追加キーを含める必要があります。\n"
"\n"
"* `client.cert`は、認証のために`etcd`に提示されるPEM形式の証明書です。\n"
"* `client.key`は上記のクライアント証明書に対応する、**PKCS8形式の**プライベートキーです。\n"
"\n"
"鍵は `openssl` を使用して PKCS8 形式に変換できます。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "openssl pkcs8 -topk8 -nocrypt -in client-key.pem -out client-key.pkcs8\n"
msgid ""
"openssl pkcs8 -topk8 -nocrypt -in client-key.pem -out client-key.pkcs8\n"
"----\n"
msgstr ""
"openssl pkcs8 -topk8 -nocrypt -in client-key.pem -out client-key.pkcs8\n"
"----\n"

#. type: Table
msgid "  [[s-kubenetes-secure-communication-between-linstor-components-v1]]"
msgstr "  [[s-kubenetes-secure-communication-between-linstor-components-v1]]"

#. type: Title ====
#, no-wrap
msgid "Configuring Secure Communication Between LINSTOR Components in Operator v1 Deployments"
msgstr "オペレーターv1デプロイメントにおけるLINSTORコンポーネント間のセキュアな通信の設定"

#. type: Table
msgid "  The default communication between LINSTOR components is not secured by TLS. If this is needed for your setup, choose one of three methods: // \"cert-manager\" is a product name so keep the original case"
msgstr "LINSTORのコンポーネント間のデフォルト通信はTLSでセキュリティ保護されていません。これがセットアップに必要な場合は、次の3つの方法のいずれかを選択してください: // \"cert-manager\"は製品名なので元のケースを保持します"

#. type: Title =====
#, no-wrap
msgid "Generating Keys and Certificates Using cert-manager"
msgstr "cert-managerを用いた鍵、証明書の生成"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Requires https://cert-manager.io/docs/[cert-manager] to be installed in your cluster.\n"
"\n"
"Set the following options in your Helm override file:\n"
"\n"
"[source,yaml]\n"
"----\n"
msgstr ""
"\n"
"クラスターにインストールするには、[cert-manager] (https://cert-manager.io/docs/) が必要です。\n"
"\n"
"Helmのオーバーライドファイルで以下のオプションを設定します:\n"
"\n"
"[source,yaml]\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"linstorSslMethod: cert-manager\n"
"linstorHttpsMethod: cert-manager\n"
"----\n"
msgstr ""
"linstorSslMethod: cert-manager\n"
"linstorHttpsMethod: cert-manager\n"
"----\n"

#. type: Title =====
#, no-wrap
msgid "Generate Keys and Certificates Using Helm"
msgstr "Helmを使用してキーと証明書を生成"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Set the following options in your Helm override file:\n"
"\n"
"[source,yaml]\n"
"----\n"
msgstr ""
"\n"
"以下のオプションをHelmのオーバーライドファイルに設定してください。\n"
"\n"
"[source,yaml]\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "linstorSslMethod: helm\n"
#| "linstorHttpsMethod: helm\n"
msgid ""
"linstorSslMethod: helm\n"
"linstorHttpsMethod: helm\n"
"----\n"
msgstr ""
"linstorSslMethod: helm\n"
"linstorHttpsMethod: helm\n"
"----\n"

#. type: Title =====
#, no-wrap
msgid "Generating Keys and Certificates Manually"
msgstr "鍵や証明書の手動生成"

#. type: Table
#, no-wrap
#| msgid "Create a private key and self-signed certificate for your certificate authorities:"
msgid ""
"\n"
"Create a private key and self-signed certificate for your certificate authorities:\n"
"\n"
"----\n"
msgstr ""
"\n"
"証明機関のための秘密鍵と自己署名証明書を作成してください。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "openssl req -new -newkey rsa:2048 -days 5000 -nodes -x509 -keyout ca.key \\\n"
#| "  -out ca.crt -subj \"/CN=linstor-system\"\n"
#| "openssl req -new -newkey rsa:2048 -days 5000 -nodes -x509 -keyout client-ca.key \\\n"
#| "  -out client-ca.crt -subj \"/CN=linstor-client-ca\"\n"
msgid ""
"openssl req -new -newkey rsa:2048 -days 5000 -nodes -x509 -keyout ca.key \\\n"
"  -out ca.crt -subj \"/CN=linstor-system\"\n"
"openssl req -new -newkey rsa:2048 -days 5000 -nodes -x509 -keyout client-ca.key \\\n"
"  -out client-ca.crt -subj \"/CN=linstor-client-ca\"\n"
"----\n"
msgstr ""
"openssl req -new -newkey rsa:2048 -days 5000 -nodes -x509 -keyout ca.key \\\n"
"  -out ca.crt -subj \"/CN=linstor-system\"\n"
"openssl req -new -newkey rsa:2048 -days 5000 -nodes -x509 -keyout client-ca.key \\\n"
"  -out client-ca.crt -subj \"/CN=linstor-client-ca\"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "Create private keys, two for the controller, one for all nodes and one for all clients:"
msgid ""
"\n"
"Create private keys, two for the controller, one for all nodes and one for all clients:\n"
"\n"
"----\n"
msgstr ""
"\n"
"コントローラー用に2つ、全てのノード用に1つ、全てのクライアント用に1つプライベートキーを作成してください。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "openssl genrsa -out linstor-control.key 2048\n"
#| "openssl genrsa -out linstor-satellite.key 2048\n"
#| "openssl genrsa -out linstor-client.key 2048\n"
#| "openssl genrsa -out linstor-api.key 2048\n"
msgid ""
"openssl genrsa -out linstor-control.key 2048\n"
"openssl genrsa -out linstor-satellite.key 2048\n"
"openssl genrsa -out linstor-client.key 2048\n"
"openssl genrsa -out linstor-api.key 2048\n"
"----\n"
msgstr ""
"openssl genrsa -out linstor-control.key 2048\n"
"openssl genrsa -out linstor-satellite.key 2048\n"
"openssl genrsa -out linstor-client.key 2048\n"
"openssl genrsa -out linstor-api.key 2048\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "Create trusted certificates for controller and nodes:"
msgid ""
"\n"
"Create trusted certificates for controller and nodes:\n"
"\n"
"----\n"
msgstr ""
"\n"
"コントローラーとノード用の信頼できる証明書を作成してください。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "openssl req -new -sha256 -key linstor-control.key -subj \"/CN=system:control\" \\\n"
#| "  -out linstor-control.csr\n"
#| "openssl req -new -sha256 -key linstor-satellite.key -subj \"/CN=system:node\" \\\n"
#| "  -out linstor-satellite.csr\n"
#| "openssl req -new -sha256 -key linstor-client.key -subj \"/CN=linstor-client\" \\\n"
#| "  -out linstor-client.csr\n"
#| "openssl req -new -sha256 -key linstor-api.key -subj \"/CN=linstor-controller\" \\\n"
#| "  -out  linstor-api.csr\n"
#| "openssl x509 -req -in linstor-control.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\\n"
#| "  -out linstor-control.crt -days 5000 -sha256\n"
#| "openssl x509 -req -in linstor-satellite.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\\n"
#| "  -out linstor-satellite.crt -days 5000 -sha256\n"
#| "openssl x509 -req -in linstor-client.csr -CA client-ca.crt -CAkey client-ca.key \\\n"
#| "  -CAcreateserial -out linstor-client.crt -days 5000 -sha256\n"
#| "openssl x509 -req -in linstor-api.csr -CA client-ca.crt -CAkey client-ca.key \\\n"
#| "  -CAcreateserial -out linstor-api.crt -days 5000 -sha256 -extensions 'v3_req' \\\n"
#| "  -extfile <(printf '%s\\n' '[v3_req]' extendedKeyUsage=serverAuth \\\n"
#| "  subjectAltName=DNS:linstor-op-cs.default.svc)\n"
msgid ""
"openssl req -new -sha256 -key linstor-control.key -subj \"/CN=system:control\" \\\n"
"  -out linstor-control.csr\n"
"openssl req -new -sha256 -key linstor-satellite.key -subj \"/CN=system:node\" \\\n"
"  -out linstor-satellite.csr\n"
"openssl req -new -sha256 -key linstor-client.key -subj \"/CN=linstor-client\" \\\n"
"  -out linstor-client.csr\n"
"openssl req -new -sha256 -key linstor-api.key -subj \"/CN=linstor-controller\" \\\n"
"  -out  linstor-api.csr\n"
"openssl x509 -req -in linstor-control.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\\n"
"  -out linstor-control.crt -days 5000 -sha256\n"
"openssl x509 -req -in linstor-satellite.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\\n"
"  -out linstor-satellite.crt -days 5000 -sha256\n"
"openssl x509 -req -in linstor-client.csr -CA client-ca.crt -CAkey client-ca.key \\\n"
"  -CAcreateserial -out linstor-client.crt -days 5000 -sha256\n"
"openssl x509 -req -in linstor-api.csr -CA client-ca.crt -CAkey client-ca.key \\\n"
"  -CAcreateserial -out linstor-api.crt -days 5000 -sha256 -extensions 'v3_req' \\\n"
"  -extfile <(printf '%s\\n' '[v3_req]' extendedKeyUsage=serverAuth \\\n"
"  subjectAltName=DNS:linstor-op-cs.default.svc)\n"
"----\n"
msgstr ""
"openssl req -new -sha256 -key linstor-control.key -subj \"/CN=system:control\" \\\n"
"  -out linstor-control.csr\n"
"openssl req -new -sha256 -key linstor-satellite.key -subj \"/CN=system:node\" \\\n"
"  -out linstor-satellite.csr\n"
"openssl req -new -sha256 -key linstor-client.key -subj \"/CN=linstor-client\" \\\n"
"  -out linstor-client.csr\n"
"openssl req -new -sha256 -key linstor-api.key -subj \"/CN=linstor-controller\" \\\n"
"  -out  linstor-api.csr\n"
"openssl x509 -req -in linstor-control.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\\n"
"  -out linstor-control.crt -days 5000 -sha256\n"
"openssl x509 -req -in linstor-satellite.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\\n"
"  -out linstor-satellite.crt -days 5000 -sha256\n"
"openssl x509 -req -in linstor-client.csr -CA client-ca.crt -CAkey client-ca.key \\\n"
"  -CAcreateserial -out linstor-client.crt -days 5000 -sha256\n"
"openssl x509 -req -in linstor-api.csr -CA client-ca.crt -CAkey client-ca.key \\\n"
"  -CAcreateserial -out linstor-api.crt -days 5000 -sha256 -extensions 'v3_req' \\\n"
"  -extfile <(printf '%s\\n' '[v3_req]' extendedKeyUsage=serverAuth \\\n"
"  subjectAltName=DNS:linstor-op-cs.default.svc)\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"NOTE: `linstor-op-cs.default.svc` in the last command needs to match create service name. With Helm, this is always\n"
"`<release-name>-cs.<namespace>.svc`.\n"
"\n"
"Create Kubernetes secrets that can be passed to the controller and node pods:\n"
"\n"
"----\n"
msgstr ""
"\n"
"注意：最後のコマンドの中の `linstor-op-cs.default.svc` は、作成するサービス名と一致する必要があります。Helmを使用する場合、常に `<リリース名>-cs.<ネームスペース>.svc` となります。\n"
"\n"
"コントローラーとノードのポッドに渡すことができるKubernetesのシークレットを作成してください。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "kubectl create secret generic linstor-control --type=kubernetes.io/tls \\\n"
#| "  --from-file=ca.crt=ca.crt --from-file=tls.crt=linstor-control.crt \\\n"
#| "  --from-file=tls.key=linstor-control.key\n"
#| "kubectl create secret generic linstor-satellite --type=kubernetes.io/tls \\\n"
#| "  --from-file=ca.crt=ca.crt --from-file=tls.crt=linstor-satellite.crt \\\n"
#| "  --from-file=tls.key=linstor-satellite.key\n"
#| "kubectl create secret generic linstor-api --type=kubernetes.io/tls \\\n"
#| "  --from-file=ca.crt=client-ca.crt --from-file=tls.crt=linstor-api.crt \\\n"
#| "  --from-file=tls.key=linstor-api.key\n"
#| "kubectl create secret generic linstor-client --type=kubernetes.io/tls \\\n"
#| "  --from-file=ca.crt=client-ca.crt --from-file=tls.crt=linstor-client.crt \\\n"
#| "  --from-file=tls.key=linstor-client.key\n"
msgid ""
"kubectl create secret generic linstor-control --type=kubernetes.io/tls \\\n"
"  --from-file=ca.crt=ca.crt --from-file=tls.crt=linstor-control.crt \\\n"
"  --from-file=tls.key=linstor-control.key\n"
"kubectl create secret generic linstor-satellite --type=kubernetes.io/tls \\\n"
"  --from-file=ca.crt=ca.crt --from-file=tls.crt=linstor-satellite.crt \\\n"
"  --from-file=tls.key=linstor-satellite.key\n"
"kubectl create secret generic linstor-api --type=kubernetes.io/tls \\\n"
"  --from-file=ca.crt=client-ca.crt --from-file=tls.crt=linstor-api.crt \\\n"
"  --from-file=tls.key=linstor-api.key\n"
"kubectl create secret generic linstor-client --type=kubernetes.io/tls \\\n"
"  --from-file=ca.crt=client-ca.crt --from-file=tls.crt=linstor-client.crt \\\n"
"  --from-file=tls.key=linstor-client.key\n"
"----\n"
msgstr ""
"kubectl create secret generic linstor-control --type=kubernetes.io/tls \\\n"
"  --from-file=ca.crt=ca.crt --from-file=tls.crt=linstor-control.crt \\\n"
"  --from-file=tls.key=linstor-control.key\n"
"kubectl create secret generic linstor-satellite --type=kubernetes.io/tls \\\n"
"  --from-file=ca.crt=ca.crt --from-file=tls.crt=linstor-satellite.crt \\\n"
"  --from-file=tls.key=linstor-satellite.key\n"
"kubectl create secret generic linstor-api --type=kubernetes.io/tls \\\n"
"  --from-file=ca.crt=client-ca.crt --from-file=tls.crt=linstor-api.crt \\\n"
"  --from-file=tls.key=linstor-api.key\n"
"kubectl create secret generic linstor-client --type=kubernetes.io/tls \\\n"
"  --from-file=ca.crt=client-ca.crt --from-file=tls.crt=linstor-client.crt \\\n"
"  --from-file=tls.key=linstor-client.key\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Pass the names of the created secrets to `helm install`:\n"
"\n"
"[source,yaml]\n"
"----\n"
msgstr ""
"\n"
"作成したシークレットの名前を `helm install` に渡します。\n"
"\n"
"[source,yaml]\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "linstorHttpsControllerSecret: linstor-api\n"
#| "linstorHttpsClientSecret: linstor-client\n"
#| "operator:\n"
#| "  controller:\n"
#| "    sslSecret: linstor-control\n"
#| "  satelliteSet:\n"
#| "    sslSecret: linstor-satellite\n"
msgid ""
"linstorHttpsControllerSecret: linstor-api\n"
"linstorHttpsClientSecret: linstor-client\n"
"operator:\n"
"  controller:\n"
"    sslSecret: linstor-control\n"
"  satelliteSet:\n"
"    sslSecret: linstor-satellite\n"
"----\n"
msgstr ""
"linstorHttpsControllerSecret: linstor-api\n"
"linstorHttpsClientSecret: linstor-client\n"
"operator:\n"
"  controller:\n"
"    sslSecret: linstor-control\n"
"  satelliteSet:\n"
"    sslSecret: linstor-satellite\n"
"----\n"

#. type: Table
msgid "  [[s-kubernetes-linstor-master-passphrase-v1]]"
msgstr "作成したシークレットの名前を `helm install` に渡します。"

#. type: Title =====
#, no-wrap
msgid "Automatically Set the Passphrase for LINSTOR"
msgstr "LINSTOR のパスフレーズを自動設定"

#. type: Table
#, no-wrap
msgid ""
"\n"
"LINSTOR needs to store confidential data to support encrypted information. This data is protected by a master\n"
"passphrase. A passphrase is automatically generated on the first chart install.\n"
"\n"
"If you want to use a custom passphrase, store it in a secret:\n"
"\n"
"----\n"
msgstr ""
"\n"
"LINSTORは暗号化された情報をサポートするために機密データを保存する必要があります。このデータはマスターパスフレーズによって保護されています。パスフレーズは最初のチャートインストール時に自動的に生成されます。\n"
"\n"
"カスタムのパスフレーズを使用したい場合は、それを秘密情報に保存してください。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "kubectl create secret generic linstor-pass --from-literal=MASTER_PASSPHRASE=<password>\n"
msgid ""
"kubectl create secret generic linstor-pass --from-literal=MASTER_PASSPHRASE=<password>\n"
"----\n"
msgstr ""
"kubectl create secret generic linstor-pass --from-literal=MASTER_PASSPHRASE=<password>\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"On install, add the following arguments to the Helm command:\n"
"\n"
"----\n"
msgstr ""
"\n"
"インストール時に、次の引数を helm コマンドに追加します:\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "--set operator.controller.luksSecret=linstor-pass\n"
msgid ""
"--set operator.controller.luksSecret=linstor-pass\n"
"----\n"
msgstr ""
"--set operator.controller.luksSecret=linstor-pass\n"
"----\n"

#. type: Table
msgid "  [[s-kubernetes-helm-install-examples-v1]]"
msgstr "  [[s-kubernetes-helm-install-examples-v1]]"

#. type: Title ====
#, no-wrap
msgid "Helm Installation Examples for Operator v1"
msgstr "Operator v1に対するHelmインストールの例"

#. type: Table
#, no-wrap
msgid ""
"\n"
"All the below examples use the following `sp-values.yaml` file. Feel\n"
"free to adjust this for your uses and environment. See <<Configuring storage pool creation>>\n"
"for further details.\n"
"\n"
"----\n"
msgstr ""
"\n"
"以下の例はすべて、次の `sp-values.yaml` ファイルを使用しています。お使いの環境や使用目的に合わせて自由に調整してください。詳細については、<<ストレージプールの作成の構成>> を参照してください。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "operator:\n"
#| "  satelliteSet:\n"
#| "    storagePools:\n"
#| "      lvmThinPools:\n"
#| "      - name: lvm-thin\n"
#| "        thinVolume: thinpool\n"
#| "        volumeGroup: \"\"\n"
#| "        devicePaths:\n"
#| "        - /dev/sdb\n"
msgid ""
"operator:\n"
"  satelliteSet:\n"
"    storagePools:\n"
"      lvmThinPools:\n"
"      - name: lvm-thin\n"
"        thinVolume: thinpool\n"
"        volumeGroup: \"\"\n"
"        devicePaths:\n"
"        - /dev/sdb\n"
"----\n"
msgstr ""
"operator:\n"
"  satelliteSet:\n"
"    storagePools:\n"
"      lvmThinPools:\n"
"      - name: lvm-thin\n"
"        thinVolume: thinpool\n"
"        volumeGroup: \"\"\n"
"        devicePaths:\n"
"        - /dev/sdb\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"NOTE: Default install. This does not setup any persistence for\n"
"the backing etcd key-value store.\n"
"\n"
"WARNING: This is not suggested for any use outside of testing.\n"
"\n"
"----\n"
msgstr ""
"\n"
"注意: デフォルトのインストール。これは、バックエンディングのetcdキー値ストアに対する永続化を設定しません。\n"
"\n"
"警告: これはテスト以外での使用を推奨しません。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \\\n"
#| "  --docker-username=<YOUR_LOGIN> --docker-password=<YOUR_PASSWORD>\n"
#| "helm repo add linstor https://charts.linstor.io\n"
#| "helm install linstor-op linstor/linstor\n"
msgid ""
"kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \\\n"
"  --docker-username=<YOUR_LOGIN> --docker-password=<YOUR_PASSWORD>\n"
"helm repo add linstor https://charts.linstor.io\n"
"helm install linstor-op linstor/linstor\n"
"----\n"
msgstr ""
"kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \\\n"
"  --docker-username=<YOUR_LOGIN> --docker-password=<YOUR_PASSWORD>\n"
"helm repo add linstor https://charts.linstor.io\n"
"helm install linstor-op linstor/linstor\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"IMPORTANT: LINBIT's container image repository (http://drbd.io), used in the previous and\n"
"upcoming `kubectl create` commands, is only available to LINBIT customers or through LINBIT\n"
"customer trial accounts. link:https://linbit.com/contact-us/[Contact LINBIT for information on\n"
"pricing or to begin a trial]. Alternatively, you can use the LINSTOR SDS upstream project named\n"
"link:https://github.com/piraeusdatastore/piraeus-operator[Piraeus], without being a LINBIT\n"
"customer.\n"
"\n"
"Install with LINSTOR storage-pools defined at install through\n"
"`sp-values.yaml`, persistent `hostPath` volumes, three etcd replicas, and by\n"
"compiling the DRBD kernel modules for the host kernels.\n"
"\n"
"This should be adequate for most basic deployments. Note that\n"
"this deployment is not using the pre-compiled DRBD kernel modules just\n"
"to make this command more portable. Using the pre-compiled binaries\n"
"will make for a much faster install and deployment. Using the\n"
"`Compile` option would not be suggested for use in a large Kubernetes clusters.\n"
"\n"
"----\n"
msgstr ""
"\n"
"重要: LINBITのコンテナイメージリポジトリ (http://drbd.io) は、前述のおよび今後の `kubectl create` コマンドで使用されますが、LINBITのお客様またはLINBITの顧客試用アカウントを持っている方のみ利用可能です。 link:https://linbit.com/contact-us/[価格情報の確認やトライアルの開始についてはLINBITにお問い合わせください]。 また、LINBITのお客様でなくても、LINSTOR SDSアップストリームプロジェクトであるlink:https://github.com/piraeusdatastore/piraeus-operator[Piraeus]を利用することもできます。\n"
"\n"
"`sp-values.yaml`を介してインストール時に定義されたLINSTORストレージプール、永続的な `hostPath` ボリューム、3つのetcdレプリカ、ホストカーネルのためにDRBDカーネルモジュールをコンパイルしてインストールします。\n"
"\n"
"これはほとんどの基本的なデプロイメントに適しているはずです。 このデプロイメントでは、このコマンドをよりポータブルにするために、事前にコンパイルされたDRBDカーネルモジュールを使用していません。 事前にコンパイルされたバイナリを使用すると、より高速なインストールとデプロイメントが可能です。 `Compile`オプションを使用することは、大規模なKubernetesクラスターでの使用をお勧めしません。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \\\n"
#| "  --docker-username=<YOUR_LOGIN> --docker-password=<YOUR_PASSWORD>\n"
#| "helm repo add linstor https://charts.linstor.io\n"
#| "helm install linstor-etcd linstor/pv-hostpath --set \"nodes={<NODE0>,<NODE1>,<NODE2>}\"\n"
#| "helm install -f sp-values.yaml linstor-op linstor/linstor --set etcd.replicas=3 \\\n"
#| "  --set operator.satelliteSet.kernelModuleInjectionMode=Compile\n"
msgid ""
"kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \\\n"
"  --docker-username=<YOUR_LOGIN> --docker-password=<YOUR_PASSWORD>\n"
"helm repo add linstor https://charts.linstor.io\n"
"helm install linstor-etcd linstor/pv-hostpath --set \"nodes={<NODE0>,<NODE1>,<NODE2>}\"\n"
"helm install -f sp-values.yaml linstor-op linstor/linstor --set etcd.replicas=3 \\\n"
"  --set operator.satelliteSet.kernelModuleInjectionMode=Compile\n"
"----\n"
msgstr ""
"kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \\\n"
"  --docker-username=<YOUR_LOGIN> --docker-password=<YOUR_PASSWORD>\n"
"helm repo add linstor https://charts.linstor.io\n"
"helm install linstor-etcd linstor/pv-hostpath --set \"nodes={<NODE0>,<NODE1>,<NODE2>}\"\n"
"helm install -f sp-values.yaml linstor-op linstor/linstor --set etcd.replicas=3 \\\n"
"  --set operator.satelliteSet.kernelModuleInjectionMode=Compile\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Install with LINSTOR storage-pools defined at install through\n"
"`sp-values.yaml`, use an already created PostgreSQL DB (preferably\n"
"clustered), rather than etcd, and use already compiled kernel modules for\n"
"DRBD.\n"
"\n"
"The PostgreSQL database in this particular example is reachable through a\n"
"service endpoint named `postgres`. PostgreSQL itself is configured with\n"
"`POSTGRES_DB=postgresdb`, `POSTGRES_USER=postgresadmin`, and\n"
"`POSTGRES_PASSWORD=admin123`\n"
"\n"
"----\n"
msgstr ""
"\n"
"`sp-values.yaml`を使用してインストール時に定義されたLINSTORストレージプールでインストールを行い、既に作成されたPostgreSQL DB（できればクラスタ化されたもの）を使用し、etcdの代わりに既にコンパイルされたDRBDのカーネルモジュールを使用してください。\n"
"\n"
"この特定の例のPostgreSQLデータベースは、`postgres`というサービスエンドポイントを介してアクセス可能です。PostgreSQL自体は、`POSTGRES_DB=postgresdb`、`POSTGRES_USER=postgresadmin`、`POSTGRES_PASSWORD=admin123`として構成されています。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \\\n"
#| "  --docker-username=<YOUR_LOGIN> --docker-email=<YOUR_EMAIL> --docker-password=<YOUR_PASSWORD>\n"
#| "helm repo add linstor https://charts.linstor.io\n"
#| "helm install -f sp-values.yaml linstor-op linstor/linstor --set etcd.enabled=false \\\n"
#| "  --set \"operator.controller.dbConnectionURL=jdbc:postgresql://postgres/postgresdb?user=postgresadmin&password=admin123\"\n"
msgid ""
"kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \\\n"
"  --docker-username=<YOUR_LOGIN> --docker-email=<YOUR_EMAIL> --docker-password=<YOUR_PASSWORD>\n"
"helm repo add linstor https://charts.linstor.io\n"
"helm install -f sp-values.yaml linstor-op linstor/linstor --set etcd.enabled=false \\\n"
"  --set \"operator.controller.dbConnectionURL=jdbc:postgresql://postgres/postgresdb?user=postgresadmin&password=admin123\"\n"
"----\n"
msgstr ""
"kubectl create secret docker-registry drbdiocred --docker-server=drbd.io \\\n"
"  --docker-username=<YOUR_LOGIN> --docker-email=<YOUR_EMAIL> --docker-password=<YOUR_PASSWORD>\n"
"helm repo add linstor https://charts.linstor.io\n"
"helm install -f sp-values.yaml linstor-op linstor/linstor --set etcd.enabled=false \\\n"
"  --set \"operator.controller.dbConnectionURL=jdbc:postgresql://postgres/postgresdb?user=postgresadmin&password=admin123\"\n"
"----\n"

#. type: Table
msgid "  [[s-kubernetes-helm-terminate]]"
msgstr "  [[s-kubernetes-helm-terminate]]"

#. type: Title ====
#, no-wrap
msgid "Terminating Helm Deployment"
msgstr "Helm デプロイメントの終了"

#. type: Table
#, no-wrap
msgid ""
"\n"
"To protect the storage infrastructure of the cluster from accidentally deleting vital components, it is necessary to perform some manual steps before deleting a Helm deployment.\n"
"\n"
"1. Delete all volume claims managed by LINSTOR components. You can use the following command to get a list of volume claims managed by LINSTOR. After checking that none of the listed volumes still hold needed data, you can delete them using the generated `kubectl delete` command.\n"
"+\n"
"----\n"
msgstr ""
"\n"
"クラスターのストレージインフラストラクチャを誤って重要なコンポーネントを削除することを防ぐために、Helmのデプロイメントを削除する前にいくつかの手動手順を実行する必要があります。\n"
"\n"
"1.LINSTORコンポーネントで管理されているすべてのボリュームクレームを削除します。LINSTORによって管理されているボリュームクレームのリストを取得するためには、次のコマンドを使用できます。リストされたボリュームのいずれも必要なデータを保持していないことを確認した後、生成された`kubectl delete`コマンドを使用して削除できます。\n"
"+\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "$ kubectl get pvc --all-namespaces -o=jsonpath='{range .items[?(@.metadata.annotations.volume\\.beta\\.kubernetes\\.io/storage-provisioner==\"linstor.csi.linbit.com\")]}kubectl delete pvc --namespace {.metadata.namespace} {.metadata.name}{\"\\n\"}{end}'\n"
#| "kubectl delete pvc --namespace default data-mysql-0\n"
#| "kubectl delete pvc --namespace default data-mysql-1\n"
#| "kubectl delete pvc --namespace default data-mysql-2\n"
msgid ""
"$ kubectl get pvc --all-namespaces -o=jsonpath='{range .items[?(@.metadata.annotations.volume\\.beta\\.kubernetes\\.io/storage-provisioner==\"linstor.csi.linbit.com\")]}kubectl delete pvc --namespace {.metadata.namespace} {.metadata.name}{\"\\n\"}{end}'\n"
"kubectl delete pvc --namespace default data-mysql-0\n"
"kubectl delete pvc --namespace default data-mysql-1\n"
"kubectl delete pvc --namespace default data-mysql-2\n"
"----\n"
msgstr ""
"$ kubectl get pvc --all-namespaces -o=jsonpath='{range .items[?(@.metadata.annotations.volume\\.beta\\.kubernetes\\.io/storage-provisioner==\"linstor.csi.linbit.com\")]}kubectl delete pvc --namespace {.metadata.namespace} {.metadata.name}{\"\\n\"}{end}'\n"
"kubectl delete pvc --namespace default data-mysql-0\n"
"kubectl delete pvc --namespace default data-mysql-1\n"
"kubectl delete pvc --namespace default data-mysql-2\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"+\n"
"WARNING: These volumes, once deleted, cannot be recovered.\n"
"\n"
"2. Delete the LINSTOR controller and satellite resources.\n"
"+\n"
"Deployment of LINSTOR satellite and controller is controlled by the `LinstorSatelliteSet` and `LinstorController` resources. You can delete the resources associated with your deployment by using `kubectl`\n"
"+\n"
"----\n"
msgstr ""
"+\n"
"警告: これらのボリュームを一旦削除すると、回復できません。\n"
"\n"
"2. LINSTORコントローラーとサテライトリソースを削除してください。\n"
"+\n"
"LINSTORのサテライトやコントローラの展開は、`LinstorSatelliteSet`リソースと`LinstorController`リソースで制御されます。デプロイメントに関連するリソースは、`kubectl`を使用して削除することができます。\n"
"+\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "kubectl delete linstorcontroller <helm-deploy-name>-cs\n"
#| "kubectl delete linstorsatelliteset <helm-deploy-name>-ns\n"
msgid ""
"kubectl delete linstorcontroller <helm-deploy-name>-cs\n"
"kubectl delete linstorsatelliteset <helm-deploy-name>-ns\n"
"----\n"
msgstr ""
"kubectl delete linstorcontroller <helm-deploy-name>-cs\n"
"kubectl delete linstorsatelliteset <helm-deploy-name>-ns\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"+\n"
"After a short wait, the controller and satellite pods should terminate. If they continue to run, you can check the above resources for errors (they are only removed after all associated pods have terminated).\n"
"\n"
"3. Delete the Helm deployment.\n"
"+\n"
"If you removed all PVCs and all LINSTOR pods have terminated, you can uninstall the Helm deployment\n"
"+\n"
"----\n"
msgstr ""
"+\n"
"短い待機後、コントローラーとサテライトポッドは終了すべきです。実行が継続する場合は、エラーの確認が可能です（それらは関連するすべてのポッドが終了した後にのみ削除されます）。\n"
"\n"
"3. Helmデプロイメントを削除してください。\n"
"+\n"
"すべての PVC を削除し、すべての LINSTOR ポッドが終了した場合、Helm デプロイメントをアンインストールできます。\n"
"+\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "helm uninstall linstor-op\n"
msgid ""
"helm uninstall linstor-op\n"
"----\n"
msgstr ""
"helm uninstall linstor-op\n"
"----\n"

#. type: Table
msgid "+ NOTE: Due to the Helm's current policy, the Custom Resource Definitions named `LinstorController` and `LinstorSatelliteSet` will not be deleted by the command.  More information regarding Helm's current position on CRDs can be found https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#method-1-let-helm-do-it-for-you[here].  [[s-kubernetes-advanced-deployments-v1]]"
msgstr "+ 注意: ヘルムの現行ポリシーにより、`LinstorController`と`LinstorSatelliteSet`という名前のカスタムリソース定義は、このコマンドでは削 除されません。 ヘルムがCRDに関する現在の立場に関する詳細情報は、[こちら](https://helm.sh/docs/chart_best_practices/custom_resource_definitions/#method-1-let-helm-do-it-for-you) で確認できます。 [[s-kubernetes-advanced-deployments-v1]]"

#. type: Title ====
#, no-wrap
msgid "Advanced Deployment Options for Operator v1"
msgstr "Operator v1のための高度なデプロイメントオプション"

#. type: Table
#, no-wrap
msgid ""
"\n"
"The Helm charts provide a set of further customization options for advanced use cases.\n"
"\n"
"IMPORTANT: LINBIT's container image repository (http://drbd.io), used in the Helm chart below, is only available to LINBIT customers or through LINBIT customer trial accounts. link:https://linbit.com/contact-us/[Contact LINBIT for information on pricing or to begin a trial]. Alternatively, you can use the LINSTOR SDS upstream project named link:https://github.com/piraeusdatastore/piraeus-operator[Piraeus], without being a LINBIT customer.\n"
"\n"
"[source,yaml]\n"
"----\n"
msgstr ""
"\n"
"Helmチャートは、高度なユースケースのためのさらなるカスタマイズオプションを提供します。\n"
"\n"
"重要: LINBITのコンテナイメージリポジトリ（http://drbd.io）は、以下のHelmチャートで使用されており、LINBITのお客様またはLINBITの顧客試用アカウントを通じてのみ利用可能です。 link:https://linbit.com/contact-us/[価格情報や試用を始めるためのLINBITへのお問い合わせ]。あるいは、LINBITのお客様でなくても使用できる、LINSTOR SDSのアップストリームプロジェクトであるlink:https://github.com/piraeusdatastore/piraeus-operator[Piraeus]を使用することもできます。\n"
"\n"
"[source,yaml]\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "global:\n"
#| "  imagePullPolicy: IfNotPresent # empty pull policy means k8s default is used (\"always\" if tag == \":latest\", \"ifnotpresent\" else) <1>\n"
#| "  setSecurityContext: true # Force non-privileged containers to run as non-root users\n"
#| "# Dependency charts\n"
#| "etcd:\n"
#| "  enabled: true\n"
#| "  persistentVolume:\n"
#| "    enabled: true\n"
#| "    storage: 1Gi\n"
#| "  replicas: 1 # How many instances of etcd will be added to the initial cluster. <2>\n"
#| "  resources: {} # resource requirements for etcd containers <3>\n"
#| "  image:\n"
#| "    repository: gcr.io/etcd-development/etcd\n"
#| "    tag: v3.4.15\n"
#| "stork:\n"
#| "  enabled: false\n"
#| "  storkImage: docker.io/openstorage/stork:2.8.2\n"
#| "  schedulerImage: k8s.gcr.io/kube-scheduler-amd64\n"
#| "  schedulerTag: \"\"\n"
#| "  replicas: 1 <2>\n"
#| "  storkResources: {} # resources requirements for the stork plug-in containers <3>\n"
#| "  schedulerResources: {} # resource requirements for the kube-scheduler containers <3>\n"
#| "  podsecuritycontext: {}\n"
#| "csi:\n"
#| "  enabled: true\n"
#| "  pluginImage: \"drbd.io/linstor-csi:v0.20.0\"\n"
#| "  csiAttacherImage: k8s.gcr.io/sig-storage/csi-attacher:v4.0.0\n"
#| "  csiLivenessProbeImage: k8s.gcr.io/sig-storage/livenessprobe:v2.7.0\n"
#| "  csiNodeDriverRegistrarImage: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.1\n"
#| "  csiProvisionerImage: k8s.gcr.io/sig-storage/csi-provisioner:v3.2.1\n"
#| "  csiSnapshotterImage: k8s.gcr.io/sig-storage/csi-snapshotter:v6.0.1\n"
#| "  csiResizerImage: k8s.gcr.io/sig-storage/csi-resizer:v1.6.0\n"
#| "  csiAttacherWorkerThreads: 10 <9>\n"
#| "  csiProvisionerWorkerThreads: 10 <9>\n"
#| "  csiSnapshotterWorkerThreads: 10 <9>\n"
#| "  csiResizerWorkerThreads: 10 <9>\n"
#| "  controllerReplicas: 1 <2>\n"
#| "  nodeAffinity: {} <4>\n"
#| "  nodeTolerations: [] <4>\n"
#| "  controllerAffinity: {} <4>\n"
#| "  controllerTolerations: [] <4>\n"
#| "  enableTopology: true\n"
#| "  resources: {} <3>\n"
#| "  customLabels: {}\n"
#| "  customAnnotations: {}\n"
#| "  kubeletPath: /var/lib/kubelet <7>\n"
#| "  controllerSidecars: []\n"
#| "  controllerExtraVolumes: []\n"
#| "  nodeSidecars: []\n"
#| "  nodeExtraVolumes: []\n"
#| "priorityClassName: \"\"\n"
#| "drbdRepoCred: drbdiocred\n"
#| "linstorSslMethod: \"manual\" # <- If set to 'helm' or 'cert-manager' the certificates will be generated automatically\n"
#| "linstorHttpsMethod: \"manual\" # <- If set to 'helm' or 'cert-manager' the certificates will be generated automatically\n"
#| "linstorHttpsControllerSecret: \"\" # <- name of secret containing linstor server certificates+key. See docs/security.md\n"
#| "linstorHttpsClientSecret: \"\" # <- name of secret containing linstor client certificates+key. See docs/security.md\n"
#| "controllerEndpoint: \"\" # <- override to the generated controller endpoint. use if controller is not deployed via operator\n"
#| "psp:\n"
#| "  privilegedRole: \"\"\n"
#| "  unprivilegedRole: \"\"\n"
#| "operator:\n"
#| "  replicas: 1 # <- number of replicas for the operator deployment <2>\n"
#| "  image: \"drbd.io/linstor-operator:v1.10.0\"\n"
#| "  affinity: {} <4>\n"
#| "  tolerations: [] <4>\n"
#| "  resources: {} <3>\n"
#| "  customLabels: {}\n"
#| "  customAnnotations: {}\n"
#| "  podsecuritycontext: {}\n"
#| "  args:\n"
#| "    createBackups: true\n"
#| "    createMonitoring: true\n"
#| "  sidecars: []\n"
#| "  extraVolumes: []\n"
#| "  controller:\n"
#| "    enabled: true\n"
#| "    controllerImage: \"drbd.io/linstor-controller:v1.20.0\"\n"
#| "    dbConnectionURL: \"\"\n"
#| "    luksSecret: \"\"\n"
#| "    dbCertSecret: \"\"\n"
#| "    dbUseClientCert: false\n"
#| "    sslSecret: \"\"\n"
#| "    affinity: {} <4>\n"
#| "    httpBindAddress: \"\"\n"
#| "    httpsBindAddress: \"\"\n"
#| "    tolerations: <4>\n"
#| "      - key: node-role.kubernetes.io/master\n"
#| "        operator: Exists\n"
#| "        effect: NoSchedule\n"
#| "      - key: node-role.kubernetes.io/control-plane\n"
#| "        operator: Exists\n"
#| "        effect: NoSchedule\n"
#| "    resources: {} <3>\n"
#| "    replicas: 1 <2>\n"
#| "    additionalEnv: [] <5>\n"
#| "    additionalProperties: {} <6>\n"
#| "    sidecars: []\n"
#| "    extraVolumes: []\n"
#| "    customLabels: {}\n"
#| "    customAnnotations: {}\n"
#| "  satelliteSet:\n"
#| "    enabled: true\n"
#| "    satelliteImage: \"drbd.io/linstor-satellite:v1.20.0\"\n"
#| "    storagePools: {}\n"
#| "    sslSecret: \"\"\n"
#| "    automaticStorageType: None\n"
#| "    affinity: {} <4>\n"
#| "    tolerations: [] <4>\n"
#| "    resources: {} <3>\n"
#| "    monitoringImage: \"drbd.io/drbd-reactor:v0.9.0\"\n"
#| "    monitoringBindAddress: \"\"\n"
#| "    mountDrbdResourceDirectoriesFromHost: false <10>\n"
#| "    kernelModuleInjectionImage: \"drbd.io/drbd9-rhel7:v9.1.11\"\n"
#| "    kernelModuleInjectionMode: ShippedModules\n"
#| "    kernelModuleInjectionAdditionalSourceDirectory: \"\" <8>\n"
#| "    kernelModuleInjectionResources: {} <3>\n"
#| "    kernelModuleInjectionExtraVolumeMounts: []\n"
#| "    additionalEnv: [] <5>\n"
#| "    sidecars: []\n"
#| "    extraVolumes: []\n"
#| "    customLabels: {}\n"
#| "    customAnnotations: {}\n"
#| "haController:\n"
#| "  enabled: false\n"
#| "  image: drbd.io/linstor-k8s-ha-controller:v0.3.0\n"
#| "  affinity: {} <4>\n"
#| "  tolerations: [] <4>\n"
#| "  resources: {} <3>\n"
#| "  replicas: 1 <2>\n"
#| "  customLabels: {}\n"
#| "  customAnnotations: {}\n"
msgid ""
"global:\n"
"  imagePullPolicy: IfNotPresent # empty pull policy means k8s default is used (\"always\" if tag == \":latest\", \"ifnotpresent\" else) <1>\n"
"  setSecurityContext: true # Force non-privileged containers to run as non-root users\n"
"# Dependency charts\n"
"etcd:\n"
"  enabled: true\n"
"  persistentVolume:\n"
"    enabled: true\n"
"    storage: 1Gi\n"
"  replicas: 1 # How many instances of etcd will be added to the initial cluster. <2>\n"
"  resources: {} # resource requirements for etcd containers <3>\n"
"  image:\n"
"    repository: gcr.io/etcd-development/etcd\n"
"    tag: v3.4.15\n"
"stork:\n"
"  enabled: false\n"
"  storkImage: docker.io/openstorage/stork:2.8.2\n"
"  schedulerImage: registry.k8s.io/kube-scheduler\n"
"  schedulerTag: \"\"\n"
"  replicas: 1 <2>\n"
"  storkResources: {} # resources requirements for the stork plugin containers <3>\n"
"  schedulerResources: {} # resource requirements for the kube-scheduler containers <3>\n"
"  podsecuritycontext: {}\n"
"csi:\n"
"  enabled: true\n"
"  pluginImage: \"drbd.io/linstor-csi:v1.1.0\"\n"
"  csiAttacherImage: registry.k8s.io/sig-storage/csi-attacher:v4.3.0\n"
"  csiLivenessProbeImage: registry.k8s.io/sig-storage/livenessprobe:v2.10.0\n"
"  csiNodeDriverRegistrarImage: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0\n"
"  csiProvisionerImage: registry.k8s.io/sig-storage/csi-provisioner:v3.5.0\n"
"  csiSnapshotterImage: registry.k8s.io/sig-storage/csi-snapshotter:v6.2.1\n"
"  csiResizerImage: registry.k8s.io/sig-storage/csi-resizer:v1.8.0\n"
"  csiAttacherWorkerThreads: 10 <9>\n"
"  csiProvisionerWorkerThreads: 10 <9>\n"
"  csiSnapshotterWorkerThreads: 10 <9>\n"
"  csiResizerWorkerThreads: 10 <9>\n"
"  controllerReplicas: 1 <2>\n"
"  nodeAffinity: {} <4>\n"
"  nodeTolerations: [] <4>\n"
"  controllerAffinity: {} <4>\n"
"  controllerTolerations: [] <4>\n"
"  enableTopology: true\n"
"  resources: {} <3>\n"
"  customLabels: {}\n"
"  customAnnotations: {}\n"
"  kubeletPath: /var/lib/kubelet <7>\n"
"  controllerSidecars: []\n"
"  controllerExtraVolumes: []\n"
"  nodeSidecars: []\n"
"  nodeExtraVolumes: []\n"
"priorityClassName: \"\"\n"
"drbdRepoCred: drbdiocred\n"
"linstorSslMethod: \"manual\" # <- If set to 'helm' or 'cert-manager' the certificates will be generated automatically\n"
"linstorHttpsMethod: \"manual\" # <- If set to 'helm' or 'cert-manager' the certificates will be generated automatically\n"
"linstorHttpsControllerSecret: \"\" # <- name of secret containing linstor server certificates+key. See docs/security.md\n"
"linstorHttpsClientSecret: \"\" # <- name of secret containing linstor client certificates+key. See docs/security.md\n"
"controllerEndpoint: \"\" # <- override to the generated controller endpoint. use if controller is not deployed via operator\n"
"psp:\n"
"  privilegedRole: \"\"\n"
"  unprivilegedRole: \"\"\n"
"operator:\n"
"  replicas: 1 # <- number of replicas for the operator deployment <2>\n"
"  image: \"drbd.io/linstor-operator:v1.10.4\"\n"
"  affinity: {} <4>\n"
"  tolerations: [] <4>\n"
"  resources: {} <3>\n"
"  customLabels: {}\n"
"  customAnnotations: {}\n"
"  podsecuritycontext: {}\n"
"  args:\n"
"    createBackups: true\n"
"    createMonitoring: true\n"
"  sidecars: []\n"
"  extraVolumes: []\n"
"  controller:\n"
"    enabled: true\n"
"    controllerImage: \"drbd.io/linstor-controller:v1.23.0\"\n"
"    dbConnectionURL: \"\"\n"
"    luksSecret: \"\"\n"
"    dbCertSecret: \"\"\n"
"    dbUseClientCert: false\n"
"    sslSecret: \"\"\n"
"    affinity: {} <4>\n"
"    httpBindAddress: \"\"\n"
"    httpsBindAddress: \"\"\n"
"    tolerations: <4>\n"
"      - key: node-role.kubernetes.io/master\n"
"        operator: Exists\n"
"        effect: NoSchedule\n"
"      - key: node-role.kubernetes.io/control-plane\n"
"        operator: Exists\n"
"        effect: NoSchedule\n"
"    resources: {} <3>\n"
"    replicas: 1 <2>\n"
"    additionalEnv: [] <5>\n"
"    additionalProperties: {} <6>\n"
"    sidecars: []\n"
"    extraVolumes: []\n"
"    customLabels: {}\n"
"    customAnnotations: {}\n"
"  satelliteSet:\n"
"    enabled: true\n"
"    satelliteImage: \"drbd.io/linstor-satellite:v1.23.0\"\n"
"    storagePools: {}\n"
"    sslSecret: \"\"\n"
"    automaticStorageType: None\n"
"    affinity: {} <4>\n"
"    tolerations: [] <4>\n"
"    resources: {} <3>\n"
"    monitoringImage: \"drbd.io/drbd-reactor:v1.2.0\"\n"
"    monitoringBindAddress: \"\"\n"
"    kernelModuleInjectionImage: \"drbd.io/drbd9-rhel7:v9.1.14\"\n"
"    kernelModuleInjectionMode: ShippedModules\n"
"    kernelModuleInjectionAdditionalSourceDirectory: \"\" <8>\n"
"    kernelModuleInjectionResources: {} <3>\n"
"    kernelModuleInjectionExtraVolumeMounts: []\n"
"    mountDrbdResourceDirectoriesFromHost: \"\" <10>\n"
"    additionalEnv: [] <5>\n"
"    sidecars: []\n"
"    extraVolumes: []\n"
"    customLabels: {}\n"
"    customAnnotations: {}\n"
"haController:\n"
"  enabled: false\n"
"  image: drbd.io/linstor-k8s-ha-controller:v0.3.0\n"
"  affinity: {} <4>\n"
"  tolerations: [] <4>\n"
"  resources: {} <3>\n"
"  replicas: 1 <2>\n"
"  customLabels: {}\n"
"  customAnnotations: {}\n"
"----\n"
msgstr ""
"global:\n"
"  imagePullPolicy: IfNotPresent # empty pull policy means k8s default is used (\"always\" if tag == \":latest\", \"ifnotpresent\" else) <1>\n"
"  setSecurityContext: true # Force non-privileged containers to run as non-root users\n"
"# Dependency charts\n"
"etcd:\n"
"  enabled: true\n"
"  persistentVolume:\n"
"    enabled: true\n"
"    storage: 1Gi\n"
"  replicas: 1 # How many instances of etcd will be added to the initial cluster. <2>\n"
"  resources: {} # resource requirements for etcd containers <3>\n"
"  image:\n"
"    repository: gcr.io/etcd-development/etcd\n"
"    tag: v3.4.15\n"
"stork:\n"
"  enabled: false\n"
"  storkImage: docker.io/openstorage/stork:2.8.2\n"
"  schedulerImage: registry.k8s.io/kube-scheduler\n"
"  schedulerTag: \"\"\n"
"  replicas: 1 <2>\n"
"  storkResources: {} # resources requirements for the stork plugin containers <3>\n"
"  schedulerResources: {} # resource requirements for the kube-scheduler containers <3>\n"
"  podsecuritycontext: {}\n"
"csi:\n"
"  enabled: true\n"
"  pluginImage: \"drbd.io/linstor-csi:v1.1.0\"\n"
"  csiAttacherImage: registry.k8s.io/sig-storage/csi-attacher:v4.3.0\n"
"  csiLivenessProbeImage: registry.k8s.io/sig-storage/livenessprobe:v2.10.0\n"
"  csiNodeDriverRegistrarImage: registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0\n"
"  csiProvisionerImage: registry.k8s.io/sig-storage/csi-provisioner:v3.5.0\n"
"  csiSnapshotterImage: registry.k8s.io/sig-storage/csi-snapshotter:v6.2.1\n"
"  csiResizerImage: registry.k8s.io/sig-storage/csi-resizer:v1.8.0\n"
"  csiAttacherWorkerThreads: 10 <9>\n"
"  csiProvisionerWorkerThreads: 10 <9>\n"
"  csiSnapshotterWorkerThreads: 10 <9>\n"
"  csiResizerWorkerThreads: 10 <9>\n"
"  controllerReplicas: 1 <2>\n"
"  nodeAffinity: {} <4>\n"
"  nodeTolerations: [] <4>\n"
"  controllerAffinity: {} <4>\n"
"  controllerTolerations: [] <4>\n"
"  enableTopology: true\n"
"  resources: {} <3>\n"
"  customLabels: {}\n"
"  customAnnotations: {}\n"
"  kubeletPath: /var/lib/kubelet <7>\n"
"  controllerSidecars: []\n"
"  controllerExtraVolumes: []\n"
"  nodeSidecars: []\n"
"  nodeExtraVolumes: []\n"
"priorityClassName: \"\"\n"
"drbdRepoCred: drbdiocred\n"
"linstorSslMethod: \"manual\" # <- If set to 'helm' or 'cert-manager' the certificates will be generated automatically\n"
"linstorHttpsMethod: \"manual\" # <- If set to 'helm' or 'cert-manager' the certificates will be generated automatically\n"
"linstorHttpsControllerSecret: \"\" # <- name of secret containing linstor server certificates+key. See docs/security.md\n"
"linstorHttpsClientSecret: \"\" # <- name of secret containing linstor client certificates+key. See docs/security.md\n"
"controllerEndpoint: \"\" # <- override to the generated controller endpoint. use if controller is not deployed via operator\n"
"psp:\n"
"  privilegedRole: \"\"\n"
"  unprivilegedRole: \"\"\n"
"operator:\n"
"  replicas: 1 # <- number of replicas for the operator deployment <2>\n"
"  image: \"drbd.io/linstor-operator:v1.10.4\"\n"
"  affinity: {} <4>\n"
"  tolerations: [] <4>\n"
"  resources: {} <3>\n"
"  customLabels: {}\n"
"  customAnnotations: {}\n"
"  podsecuritycontext: {}\n"
"  args:\n"
"    createBackups: true\n"
"    createMonitoring: true\n"
"  sidecars: []\n"
"  extraVolumes: []\n"
"  controller:\n"
"    enabled: true\n"
"    controllerImage: \"drbd.io/linstor-controller:v1.23.0\"\n"
"    dbConnectionURL: \"\"\n"
"    luksSecret: \"\"\n"
"    dbCertSecret: \"\"\n"
"    dbUseClientCert: false\n"
"    sslSecret: \"\"\n"
"    affinity: {} <4>\n"
"    httpBindAddress: \"\"\n"
"    httpsBindAddress: \"\"\n"
"    tolerations: <4>\n"
"      - key: node-role.kubernetes.io/master\n"
"        operator: Exists\n"
"        effect: NoSchedule\n"
"      - key: node-role.kubernetes.io/control-plane\n"
"        operator: Exists\n"
"        effect: NoSchedule\n"
"    resources: {} <3>\n"
"    replicas: 1 <2>\n"
"    additionalEnv: [] <5>\n"
"    additionalProperties: {} <6>\n"
"    sidecars: []\n"
"    extraVolumes: []\n"
"    customLabels: {}\n"
"    customAnnotations: {}\n"
"  satelliteSet:\n"
"    enabled: true\n"
"    satelliteImage: \"drbd.io/linstor-satellite:v1.23.0\"\n"
"    storagePools: {}\n"
"    sslSecret: \"\"\n"
"    automaticStorageType: None\n"
"    affinity: {} <4>\n"
"    tolerations: [] <4>\n"
"    resources: {} <3>\n"
"    monitoringImage: \"drbd.io/drbd-reactor:v1.2.0\"\n"
"    monitoringBindAddress: \"\"\n"
"    kernelModuleInjectionImage: \"drbd.io/drbd9-rhel7:v9.1.14\"\n"
"    kernelModuleInjectionMode: ShippedModules\n"
"    kernelModuleInjectionAdditionalSourceDirectory: \"\" <8>\n"
"    kernelModuleInjectionResources: {} <3>\n"
"    kernelModuleInjectionExtraVolumeMounts: []\n"
"    mountDrbdResourceDirectoriesFromHost: \"\" <10>\n"
"    additionalEnv: [] <5>\n"
"    sidecars: []\n"
"    extraVolumes: []\n"
"    customLabels: {}\n"
"    customAnnotations: {}\n"
"haController:\n"
"  enabled: false\n"
"  image: drbd.io/linstor-k8s-ha-controller:v0.3.0\n"
"  affinity: {} <4>\n"
"  tolerations: [] <4>\n"
"  resources: {} <3>\n"
"  replicas: 1 <2>\n"
"  customLabels: {}\n"
"  customAnnotations: {}\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"<1> Sets the pull policy for all images.\n"
"\n"
"<2> Controls the number of replicas for each component.\n"
"\n"
"<3> Set container resource requests and limits. See https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/[the Kubernetes docs].\n"
" Most containers need a minimal amount of resources, except for:\n"
"    * `etcd.resources` See the https://etcd.io/docs/v3.4.0/op-guide/hardware/[etcd docs]\n"
"    * `operator.controller.resources` Around `700MiB` memory is required\n"
"    * `operater.satelliteSet.resources` Around `700MiB` memory is required\n"
"    * `operator.satelliteSet.kernelModuleInjectionResources` If kernel modules are compiled,\n"
"1GiB of memory is required.\n"
"\n"
"<4> Affinity and toleration determine where pods are scheduled on the cluster. See the\n"
"https://kubernetes.io/docs/concepts/scheduling-eviction/[Kubernetes docs on affinity and\n"
"toleration]. This might be especially important for the `operator.satelliteSet` and `csi.node*`\n"
"values. To schedule a pod using a LINSTOR persistent volume, the node requires a running\n"
"LINSTOR satellite and LINSTOR CSI pod.\n"
"\n"
"<5> Sets additional environments variables to pass to the LINSTOR controller and satellites.\n"
"Uses the same format as https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/[the\n"
"`env` value of a container]\n"
"\n"
"<6> Sets additional properties on the LINSTOR controller. Expects a simple mapping of `<property-key>: <value>`.\n"
"\n"
"<7> kubelet expects every CSI plugin to mount volumes under a specific subdirectory of its own state directory. By default, this state directory is `/var/lib/kubelet`. Some Kubernetes distributions use a different directory:\n"
"\n"
"* microk8s: `/var/snap/microk8s/common/var/lib/kubelet`\n"
"\n"
"<8> Directory on the host that is required for building kernel modules. Only needed if using the `Compile` injection method. Defaults to `/usr/src`, which is where the actual kernel sources are stored on most distributions. Use `\"none\"` to not mount any additional directories.\n"
"\n"
"<9> Set the number of worker threads used by the CSI driver. Higher values put more load on the LINSTOR controller, which might lead to instability when creating many volumes at once.\n"
"\n"
"<10> If set to true, the satellite containers will have the following files and directories mounted from the host operating system:\n"
"+\n"
"* `/etc/drbd/drbd.conf` (file)\n"
"* `/etc/drbd.d` (directory)\n"
"* `/var/lib/drbd` (directory)\n"
"* `/var/lib/linstor.d` (directory)\n"
"+\n"
"All files and directories must already exist on the host.\n"
"\n"
"[[s-kubernetes-ha-deployment]]\n"
msgstr ""
"<1> すべてのイメージに対してプルポリシーを設定します。\n"
"\n"
"<2> 各コンポーネントのレプリカ数を制御します。\n"
"\n"
"<3> コンテナのリソース要求と制限を設定します。詳細は[「kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource」(Kubernetesのドキュメント)]を参照してください。\n"
" ほとんどのコンテナは、最小限のリソースが必要ですが、例外があります: \n"
"    * `etcd.resources` See the https://etcd.io/docs/v3.4.0/op-guide/hardware/[etcd docs]\n"
"    * `operator.controller.resources` Around `700MiB` memory is required\n"
"    * `operater.satelliteSet.resources` Around `700MiB` memory is required\n"
"    * `operator.satelliteSet.kernelModuleInjectionResources` カーネルモジュールがコンパイルされると、1GiBのメモリが必要です。\n"
"\n"
"<4> アフィニティと寛容度は、ポッドがクラスター上でスケジュールされる場所を決定します。アフィニティと寛容度に関する Kubernetes ドキュメントは、https://kubernetes.io/docs/concepts/scheduling-eviction/[Kubernetes docs on affinity and toleration]を参照してください。これは、`operator.satelliteSet`および`csi.node*`の値に特に重要かもしれません。LINSTOR永続ボリュームを使用してポッドをスケジュールする場合、ノードには実行中の LINSTOR サテライトと LINSTOR CSI ポッドが必要です。\n"
"\n"
"<5> LINSTOR コントローラおよび <<satellites>> に渡す追加の環境変数を設定します。\n"
"https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/の「コンテナの`env`値」に同じ形式を使用します。\n"
"\n"
"<6> LINSTORコントローラーに追加のプロパティを設定します。`<property-key>: <value>` の単純なマッピングを期待しています。\n"
"\n"
"<7> kubeletは、すべてのCSIプラグインが、独自の状態ディレクトリの特定のサブディレクトリにボリュームをマウントすることを期待しています。デフォルトでは、この状態ディレクトリは`/var/lib/kubelet`です。一部のKubernetesディストリビューションは異なるディレクトリを使用しています。\n"
"\n"
"* microk8s: `/var/snap/microk8s/common/var/lib/kubelet`\n"
"\n"
"<8> デプロイメント方法として`Compile`インジェクションを使用する場合に必要な、カーネルモジュールをビルドするためのホスト上のディレクトリ。ほとんどのディストリビューションで実際のカーネルソースが保存されている`/usr/src`にデフォルトで設定されています。追加のディレクトリをマウントしない場合は`\"none\"`を使用してください。\n"
"\n"
"<9> CSI ドライバーが使用するワーカースレッドの数を設定します。より高い値は、一度に多くのボリュームを作成するときに不安定になる可能性がある LINSTOR コントローラーにより多くの負荷をかけます。\n"
"\n"
"<10> もしtrueに設定された場合、サテライトコンテナはホストオペレーティングシステムから以下のファイルやディレクトリをマウントされます:\n"
"+\n"
"* `/etc/drbd/drbd.conf` (file)\n"
"* `/etc/drbd.d` (directory)\n"
"* `/var/lib/drbd` (directory)\n"
"* `/var/lib/linstor.d` (directory)\n"
"+\n"
"すべてのファイルとディレクトリは、ホスト上にすでに存在している必要があります。\n"
"\n"
"[[s-kubernetes-ha-deployment]]\n"

#. type: Title ====
#, no-wrap
msgid "High-Availability Deployment in Operator v1"
msgstr "Operator v1における高可用性デプロイメント"

#. type: Table
msgid ""
"  To create a high-availability deployment of all components within a LINSTOR Operator v1 deployment, consult the https://github.com/piraeusdatastore/piraeus-operator/blob/b00fd34/doc/scheduling.md[upstream guide] The default values are chosen so that scaling the components to multiple replicas ensures that the replicas are placed on different nodes. This ensures that a single node failures will not interrupt the service.  NOTE: If you have deployed LINBIT SDS in "
"Kubernetes by using the LINSTOR Operator v2, high availability is built into the deployment by default.  [[s-kubernetes-ha-controller-v1]]"
msgstr ""
"LINSTOR Operator v1 デプロイメント内のすべてのコンポーネントのハイアベイラビリティデプロイメントを作成するには、https://github.com/piraeusdatastore/piraeus-operator/blob/b00fd34/doc/scheduling.md[アップストリームガイド]を参照してください。デフォルト値は、コンポーネントを複数のレプリカにスケーリングすることで、レプリカが異なるノードに配置されるように選択されています。これにより、単一ノードの障害がサービスを中断しないようになっています。注意: LINBIT SDSをLINSTOR "
"Operator v2 を使用してKubernetesに展開した場合、ハイアベイラビリティがデフォルトでデプロイメントに組み込まれています。"

#. type: Title =====
#, no-wrap
msgid "Fast Workload Failover Using the High Availability Controller"
msgstr "高可用性コントローラーを使用した高速ワークロードフェイルオーバー"

#. type: Table
#, no-wrap
msgid ""
"\n"
"When node failures occur, Kubernetes is very conservative in rescheduling stateful workloads. This means it can\n"
"take more than 15 minutes for Pods to be moved from unreachable nodes. With the information available to DRBD and\n"
"LINSTOR, this process can be sped up significantly.\n"
"\n"
"The LINSTOR High Availability Controller (HA Controller) speeds up the failover process for stateful workloads using\n"
"LINSTOR for storage. It monitors and manages any Pod that is attached to at least one DRBD resource.\n"
"\n"
"For the HA Controller to work properly, you need quorum, that is at least three replicas (or two replicas + one diskless\n"
"tiebreaker). If using lower replica counts, attached Pods will be ignored and are not eligible for faster failover.\n"
"\n"
"The HA Controller is packaged as a Helm chart, and can be deployed using:\n"
"\n"
"----\n"
msgstr ""
"\n"
"ノードの障害が発生すると、Kubernetes はステートフルなワークロードの再スケジューリングに非常に慎重です。これは、ポッドが到達不能なノードから移動するのに15分以上かかる可能性があることを意味します。DRBD と LINSTOR が利用可能な情報を活用することで、このプロセスを大幅に高速化することが可能です。\n"
"\n"
"LINSTOR ハイアベイラビリティ コントローラ（HA コントローラ）は、LINSTOR を使用してストレージを利用する状態を持つワークロードのフェイルオーバー プロセスを高速化します。少なくとも1つの DRBD リソースにアタッチされた任意の Pod を監視および管理します。\n"
"\n"
"HAコントローラーが正しく機能するためには、少なくとも3つのレプリカ（または2つのレプリカ+1つのディスクレス・タイブレーカー）が必要です。レプリカ数を低く設定している場合、関連するポッドは無視され、より迅速なフェイルオーバーの対象とはなりません。\n"
"\n"
"HAコントローラーはHelmチャートとしてパッケージ化されており、以下を使用してデプロイメントできます:\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "$ helm repo update\n"
#| "$ helm install linstor-ha-controller linstor/linstor-ha-controller\n"
msgid ""
"$ helm repo update\n"
"$ helm install linstor-ha-controller linstor/linstor-ha-controller\n"
"----\n"
msgstr ""
"$ helm repo update\n"
"$ helm install linstor-ha-controller linstor/linstor-ha-controller\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"If you are using the HA Controller in your cluster you can set additional parameters in all StorageClasses. These\n"
"parameters ensure that the volume is not accidentally remounted as read-only, leading to degraded Pods.\n"
"\n"
"[source,yaml]\n"
"----\n"
msgstr ""
"\n"
"クラスターでHAコントローラーを使用している場合、すべてのStorageClassesに追加パラメータを設定できます。これらのパラメータにより、ボリュームが誤って読み取り専用で再マウントされることが防がれ、Podが低下することがありません。\n"
"\n"
"[source,yaml]\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "parameters:\n"
#| "  property.linstor.csi.linbit.com/DrbdOptions/auto-quorum: suspend-io\n"
#| "  property.linstor.csi.linbit.com/DrbdOptions/Resource/on-no-data-accessible: suspend-io\n"
#| "  property.linstor.csi.linbit.com/DrbdOptions/Resource/on-suspended-primary-outdated: force-secondary\n"
#| "  property.linstor.csi.linbit.com/DrbdOptions/Net/rr-conflict: retry-connect\n"
msgid ""
"parameters:\n"
"  property.linstor.csi.linbit.com/DrbdOptions/auto-quorum: suspend-io\n"
"  property.linstor.csi.linbit.com/DrbdOptions/Resource/on-no-data-accessible: suspend-io\n"
"  property.linstor.csi.linbit.com/DrbdOptions/Resource/on-suspended-primary-outdated: force-secondary\n"
"  property.linstor.csi.linbit.com/DrbdOptions/Net/rr-conflict: retry-connect\n"
"----\n"
msgstr ""
"parameters:\n"
"  property.linstor.csi.linbit.com/DrbdOptions/auto-quorum: suspend-io\n"
"  property.linstor.csi.linbit.com/DrbdOptions/Resource/on-no-data-accessible: suspend-io\n"
"  property.linstor.csi.linbit.com/DrbdOptions/Resource/on-suspended-primary-outdated: force-secondary\n"
"  property.linstor.csi.linbit.com/DrbdOptions/Net/rr-conflict: retry-connect\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"To exempt a Pod from management by the HA Controller, add the following annotation to the Pod:\n"
"\n"
"----\n"
msgstr ""
"\n"
"HA コントローラーによるポッドの管理を除外するには、次の注釈をポッドに追加します:\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "$ kubectl annotate pod <podname> drbd.linbit.com/ignore-fail-over=\"\"\n"
msgid ""
"$ kubectl annotate pod <podname> drbd.linbit.com/ignore-fail-over=\"\"\n"
"----\n"
msgstr ""
"$ kubectl annotate pod <podname> drbd.linbit.com/ignore-fail-over=\"\"\n"
"----\n"

#. type: Table
msgid "  [[s-kubernetes-etcd-backup]]"
msgstr "  [[s-kubernetes-etcd-backup]]"

#. type: Title ====
#, no-wrap
msgid "Backing up the etcd Database"
msgstr "etcdデータベースのバックアップ"

#. type: Table
#, no-wrap
msgid ""
"\n"
"To create a backup of the etcd database (in LINSTOR Operator v1 deployments) and store it on your control host, enter the following commands:\n"
"\n"
"[source]\n"
"----\n"
msgstr ""
"\n"
"etcdデータベースのバックアップ（LINSTOR Operator v1のDeploymentにおいて）を作成し、それを制御ホストに保存するには、以下のコマンドを入力してください:\n"
"\n"
"[source]\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"kubectl exec linstor-op-etcd-0 -- etcdctl snapshot save /tmp/save.db\n"
"kubectl cp linstor-op-etcd-0:/tmp/save.db save.db\n"
"----\n"
msgstr ""
"kubectl exec linstor-op-etcd-0 -- etcdctl snapshot save /tmp/save.db\n"
"kubectl cp linstor-op-etcd-0:/tmp/save.db save.db\n"
"----\n"

#. type: Table
msgid "  These commands will create a file `save.db` on the machine you are running `kubectl` from.  [[s-kubernetes-deploy-external-controller]]"
msgstr "これらのコマンドは、`kubectl`を実行しているマシンに`save.db`というファイルを作成します。[[s-kubernetes-deploy-external-controller]]"

#. type: Title ===
#, no-wrap
msgid "Deploying with an External LINSTOR Controller"
msgstr "LINSTORオペレーターを使用したデプロイ"

#. type: Table
msgid "  The Operator can configure the satellites and CSI plugin to use an existing LINSTOR setup. This can be useful in cases where the storage infrastructure is separate from the Kubernetes cluster. Volumes can be provisioned in diskless mode on the Kubernetes nodes while the storage nodes will provide the backing disk storage.  [[s-kubernetes-external-linstor-controller-deployment-v2]]"
msgstr "オペレーターは、既存のLINSTORセットアップを使用するようにサテライトとCSIプラグインを構成できます。これは、ストレージインフラがKubernetesクラスタとは別である場合に役立ちます。ボリュームは、Kubernetesノード上でディスクレスモードでプロビジョニングされ、ストレージノードはバッキングディスクストレージを提供します。[[s-kubernetes-external-linstor-controller-deployment-v2]]"

#. type: Title ====
#, no-wrap
msgid "Operator v2 Deployment with an External LINSTOR Controller"
msgstr "外部LINSTORコントローラを使用したOperator v2のデプロイメント"

#. type: Table
msgid ""
"// see this GL issue: // https://gitlab.at.linbit.com/linbit/linbit-documentation/-/issues/88 The instructions in this section describe how you can connect an Operator v2 LINBIT SDS deployment to an existing LINBIST SDS cluster that you manage outside Kubernetes.  To follow the steps in this section you should be familiar with editing link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md[`LinstorCluster`] resources.  "
"[[s-kubernetes-external-linstor-controller-deployment-configuring-linstorcluster-v2]]"
msgstr ""
"// see this GL issue: // https://gitlab.at.linbit.com/linbit/linbit-documentation/-/issues/88 The instructions in this section describe how you can connect an Operator v2 LINBIT SDS deployment to an existing LINBIST SDS cluster that you manage outside Kubernetes.  To follow the steps in this section you should be familiar with editing link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md[`LinstorCluster`] resources.  "
"[[s-kubernetes-external-linstor-controller-deployment-configuring-linstorcluster-v2]]"

#. type: Title =====
#, no-wrap
msgid "Configuring the `LinstorCluster` Resource"
msgstr "`LinstorCluster`リソースの設定"

#. type: Table
#, no-wrap
msgid ""
"\n"
"To use an externally managed LINSTOR cluster, specify the URL of the LINSTOR controller in the `LinstorCluster` resource in a YAML configuration and apply it to your deployment. In the following example, the LINSTOR controller is reachable at `http://linstor-controller.example.com:3370`.\n"
"\n"
"----\n"
msgstr ""
"\n"
"外部で管理されたLINSTORクラスタを使用するには、YAML構成の`LinstorCluster`リソースにLINSTORコントローラのURLを指定し、デプロイメントに適用してください。次の例では、LINSTORコントローラは`http://linstor-controller.example.com:3370`でアクセス可能です。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec:\n"
"  externalController:\n"
"    url: http://linstor-controller.example.com:3370\n"
"----\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec:\n"
"  externalController:\n"
"    url: http://linstor-controller.example.com:3370\n"
"----\n"

#. type: Table
msgid "  NOTE: You can also specify an IP address rather than a hostname and domain for the controller.  [[s-kubernetes-external-linstor-controller-deployment-configuring-host-networking-v2]]"
msgstr "注意：コントローラーにホスト名とドメインの代わりにIPアドレスを指定することもできます。 [[s-kubernetes-external-linstor-controller-deployment-configuring-host-networking-v2]]"

#. type: Title =====
#, no-wrap
msgid "Configuring Host Networking for LINSTOR Satellites"
msgstr "LINSTOR サテライトのホストネットワーキングを構成する"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Normally the pod network is not reachable from outside the Kubernetes cluster. In this case the external LINSTOR controller would not be able to communicate with the satellites in the Kubernetes cluster. For this reason, you need to configure your satellites to use host networking.\n"
"\n"
"To use host networking, deploy a `LinstorSatelliteConfiguration` resource by applying the following YAML configuration to your deployment:\n"
"\n"
"----\n"
msgstr ""
"\n"
"通常、ポッドネットワークはKubernetesクラスターの外部から到達できません。この場合、外部のLINSTORコントローラーはKubernetesクラスター内のサテライトと通信できません。そのため、サテライトがホストネットワーキングを使用するように設定する必要があります。\n"
"\n"
"ホストネットワーキングを使用するには、次のYAML構成をデプロイメントに適用して、`LinstorSatelliteConfiguration`リソースをデプロイしてください:\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: host-network\n"
"spec:\n"
"  patches:\n"
"    - target:\n"
"        kind: Pod\n"
"        name: satellite\n"
"      patch: |\n"
"        apiVersion: v1\n"
"        kind: Pod\n"
"        metadata:\n"
"          name: satellite\n"
"        spec:\n"
"          hostNetwork: true\n"
"----\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: host-network\n"
"spec:\n"
"  patches:\n"
"    - target:\n"
"        kind: Pod\n"
"        name: satellite\n"
"      patch: |\n"
"        apiVersion: v1\n"
"        kind: Pod\n"
"        metadata:\n"
"          name: satellite\n"
"        spec:\n"
"          hostNetwork: true\n"
"----\n"

#. type: Table
msgid "  [[s-kubernetes-external-linstor-controller-deployment-verifying-v2]]"
msgstr "  [[s-kubernetes-external-linstor-controller-deployment-verifying-v2]]"

#. type: Title =====
#, no-wrap
msgid "Verifying an External LINSTOR Controller Configuration"
msgstr "外部のLINSTORコントローラー構成を検証"

#. type: Table
#, no-wrap
msgid ""
"\n"
"You can verify that you have correctly configured your Kubernetes deployment to use an external LINSTOR controller by verifying the following:\n"
"\n"
"- The `Available` condition on the `LinstorCluster` resource reports the expected URL for the\n"
"  external LINSTOR controller:\n"
"+\n"
"[%autofit]\n"
"----\n"
msgstr ""
"\n"
"あなたがKubernetesデプロイメントを外部のLINSTORコントローラを使用するように正しく設定したかどうかを確認する方法は、以下を確認することによって行うことができます:\n"
"\n"
"- `LinstorCluster`リソースの`Available`条件は、外部LINSTORコントローラーの予想されるURLを報告します。\n"
"+\n"
"[%autofit]\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"$ kubectl get LinstorCluster -ojsonpath='{.items[].status.conditions[?(@.type==\"Available\")].message}{\"\\n\"}'\n"
"Controller 1.20.3 (API: 1.16.0, Git: 8d19a891df018f6e3d40538d809904f024bfe361) reachable at 'http://linstor-controller.example.com:3370'\n"
"----\n"
msgstr ""
"$ kubectl get LinstorCluster -ojsonpath='{.items[].status.conditions[?(@.type==\"Available\")].message}{\"\\n\"}'\n"
"Controller 1.20.3 (API: 1.16.0, Git: 8d19a891df018f6e3d40538d809904f024bfe361) reachable at 'http://linstor-controller.example.com:3370'\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"- The `linstor-csi-controller` deployment uses the expected URL:\n"
"+\n"
"[%autofit]\n"
"----\n"
msgstr ""
"\n"
"- `linstor-csi-controller`デプロイメントは期待されるURLを使用します:\n"
"+\n"
"[%autofit]\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"$ kubectl get -n linbit-sds deployment linstor-csi-controller -ojsonpath='{.spec.template.spec.containers[?(@.name==\"linstor-csi\")].env[?(@.name==\"LS_CONTROLLERS\")].value}{\"\\n\"}'\n"
"http://linstor-controller.example.com:3370\n"
"----\n"
msgstr ""
"$ kubectl get -n linbit-sds deployment linstor-csi-controller -ojsonpath='{.spec.template.spec.containers[?(@.name==\"linstor-csi\")].env[?(@.name==\"LS_CONTROLLERS\")].value}{\"\\n\"}'\n"
"http://linstor-controller.example.com:3370\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"- The `linstor-csi-node` deployment uses the expected URL:\n"
"+\n"
"[%autofit]\n"
"----\n"
msgstr ""
"\n"
"- `linstor-csi-node`デプロイメントは期待されるURLを使用します。\n"
"+\n"
"[%autofit]\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"$ kubectl get -n linbit-sds daemonset linstor-csi-node -ojsonpath='{.spec.template.spec.containers[?(@.name==\"linstor-csi\")].env[?(@.name==\"LS_CONTROLLERS\")].value}{\"\\n\"}'\n"
"http://linstor-controller.example.com:3370\n"
"----\n"
msgstr ""
"$ kubectl get -n linbit-sds daemonset linstor-csi-node -ojsonpath='{.spec.template.spec.containers[?(@.name==\"linstor-csi\")].env[?(@.name==\"LS_CONTROLLERS\")].value}{\"\\n\"}'\n"
"http://linstor-controller.example.com:3370\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"- The Kubernetes nodes are registered as satellite nodes on the LINSTOR controller:\n"
"+\n"
"[%autofit]\n"
"----\n"
msgstr ""
"\n"
"- KubernetesノードはLINSTORコントローラー上でサテライトノードとして登録されています: \n"
"+\n"
"[%autofit]\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"$ kubectl get nodes -owide\n"
"NAME               STATUS   ROLES           AGE   VERSION   INTERNAL-IP      [...]\n"
"k8s-1-26-10.test   Ready    control-plane   22m   v1.26.3   192.168.122.10   [...]\n"
"[...]\n"
"----\n"
msgstr ""
"$ kubectl get nodes -owide\n"
"NAME               STATUS   ROLES           AGE   VERSION   INTERNAL-IP      [...]\n"
"k8s-1-26-10.test   Ready    control-plane   22m   v1.26.3   192.168.122.10   [...]\n"
"[...]\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"+\n"
"After getting the node names from the output of the above command, verify that the node names are also LINSTOR satellites by entering a LINSTOR `node list` command on your LINSTOR controller node.\n"
"+\n"
"----\n"
msgstr ""
"+\n"
"上記コマンドの出力からノード名を取得した後、LINSTORコントローラーノードでLINSTOR `node list`コマンドを入力して、ノード名がLINSTORサテライトであることも確認してください。\n"
"+\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "$ kubectl linstor node list\n"
#| "╭────────────────────────────────────────────────────────────────────────────────────╮\n"
#| "┊ Node                           ┊ NodeType   ┊ Addresses                   ┊ State  ┊\n"
#| "╞════════════════════════════════════════════════════════════════════════════════════╡\n"
#| "┊ kube-node-01.test              ┊ SATELLITE  ┊ 10.43.224.26:3366 (PLAIN)   ┊ Online ┊\n"
#| "┊ kube-node-02.test              ┊ SATELLITE  ┊ 10.43.224.27:3366 (PLAIN)   ┊ Online ┊\n"
#| "┊ kube-node-03.test              ┊ SATELLITE  ┊ 10.43.224.28:3366 (PLAIN)   ┊ Online ┊\n"
#| "┊ linstor-op-cs-controller-[...] ┊ CONTROLLER ┊ 172.24.116.114:3366 (PLAIN) ┊ Online ┊\n"
#| "╰────────────────────────────────────────────────────────────────────────────────────╯\n"
msgid ""
"$ linstor node list\n"
"╭─────────────────────────────────────────────────────────────────────╮\n"
"┊ Node             ┊ NodeType  ┊ Addresses                   ┊ State  ┊\n"
"╞═════════════════════════════════════════════════════════════════════╡\n"
"┊ k8s-1-26-10.test ┊ SATELLITE ┊ 192.168.122.10:3366 (PLAIN) ┊ Online ┊\n"
"[...]\n"
"----\n"
msgstr ""
"$ linstor node list\n"
"╭─────────────────────────────────────────────────────────────────────╮\n"
"┊ Node             ┊ NodeType  ┊ Addresses                   ┊ State  ┊\n"
"╞═════════════════════════════════════════════════════════════════════╡\n"
"┊ k8s-1-26-10.test ┊ SATELLITE ┊ 192.168.122.10:3366 (PLAIN) ┊ Online ┊\n"
"[...]\n"
"----\n"

#. type: Table
msgid "  [[s-kubernetes-external-linstor-controller-deployment-v1]]"
msgstr "  [[s-kubernetes-external-linstor-controller-deployment-v1]]"

#. type: Title ====
#, no-wrap
msgid "Operator v1 Deployment with an External LINSTOR Controller"
msgstr "外部のLINSTORコントローラーを使用した Operator v1 のデプロイメント"

#. type: Table
#, no-wrap
msgid ""
"\n"
"To skip the creation of a LINSTOR controller deployment and configure the other components to use your existing LINSTOR\n"
"controller, use the following options when running `helm install`:\n"
"\n"
"* `operator.controller.enabled=false` This disables creation of the `LinstorController`\n"
"resource\n"
"* `operator.etcd.enabled=false` Since no LINSTOR controller will run on Kubernetes, no\n"
"database is required.\n"
"* `controllerEndpoint=<url-of-linstor-controller>` The HTTP endpoint of the existing LINSTOR\n"
"controller. For example: `http://linstor.storage.cluster:3370/`\n"
"\n"
"After all pods are ready, you should see the Kubernetes cluster nodes as satellites in your LINSTOR setup.\n"
"\n"
"IMPORTANT: Your Kubernetes nodes must be reachable using their IP by the controller and storage nodes.\n"
"\n"
"Create a storage class referencing an existing storage pool on your storage nodes.\n"
"\n"
"[source,yaml]\n"
"----\n"
msgstr ""
"\n"
"`helm install`を実行する際に、LINSTORコントローラーデプロイメントの作成をスキップし、他のコンポーネントを既存のLINSTORコントローラーを使用するように設定するには、次のオプションを使用してください。\n"
"\n"
"* `operator.controller.enabled=false` This disables creation of the `LinstorController` resource\n"
"* `operator.etcd.enabled=false` Since no LINSTOR controller will run on Kubernetes, no database is required.\n"
"* `controllerEndpoint=<url-of-linstor-controller>` The HTTP endpoint of the existing LINSTOR controller. For example: http://linstor.storage.cluster:3370/`\n"
"\n"
"すべてのポッドが準備完了になると、LINSTORセットアップでKubernetesクラスターノードがサテライトとして表示されるはずです。\n"
"\n"
"重要: コントローラーとストレージノードは、KubernetesノードがそれらのIPを使用して到達可能である必要があります。\n"
"\n"
"ストレージノード上で既存のストレージプールを参照するストレージクラスを作成してください。\n"
"\n"
"[source,yaml]\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: storage.k8s.io/v1\n"
"kind: StorageClass\n"
"metadata:\n"
"  name: linstor-on-k8s\n"
"provisioner: linstor.csi.linbit.com\n"
"parameters:\n"
"  autoPlace: \"3\"\n"
"  storagePool: existing-storage-pool\n"
"  resourceGroup: linstor-on-k8s\n"
"----\n"
msgstr ""
"apiVersion: storage.k8s.io/v1\n"
"kind: StorageClass\n"
"metadata:\n"
"  name: linstor-on-k8s\n"
"provisioner: linstor.csi.linbit.com\n"
"parameters:\n"
"  autoPlace: \"3\"\n"
"  storagePool: existing-storage-pool\n"
"  resourceGroup: linstor-on-k8s\n"
"----\n"

#. type: Table
msgid "  You can provision new volumes by creating PVCs using your storage class. The volumes will first be placed only on nodes with the given storage pool, that is, your storage infrastructure. Once you want to use the volume in a pod, LINSTOR CSI will create a diskless resource on the Kubernetes node and attach over the network to the diskful resource.  [[s-kubernetes-linstor-interacting]]"
msgstr "ストレージクラスを使用してPVCを作成することで、新しいボリュームをプロビジョニングできます。ボリュームはまず、与えられたストレージプール（つまり、ストレージインフラストラクチャ）を持つノードにのみ配置されます。ポッドでボリュームを使用したい場合は、LINSTOR CSIがKubernetesノード上にディスクレスリソースを作成し、ネットワークを介してディスクフルリソースにアタッチします。[[s-kubernetes-linstor-interacting]]"

#. type: Title ===
#, no-wrap
msgid "Interacting with LINSTOR in Kubernetes"
msgstr "Kubernetes で LINSTOR の操作"

#. type: Table
#, no-wrap
msgid ""
"\n"
"The controller pod includes a LINSTOR Client, making it easy to interact directly with LINSTOR.\n"
"For example:\n"
"\n"
"----\n"
msgstr ""
"\n"
"コントローラーポッドにはLINSTORクライアントが含まれており、LINSTORと直接やり取りすることが簡単になります。\n"
"実行例:\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "kubectl exec deployment/linstor-op-cs-controller -- linstor storage-pool list\n"
msgid ""
"kubectl exec deployment/linstor-op-cs-controller -- linstor storage-pool list\n"
"----\n"
msgstr ""
"kubectl exec deployment/linstor-op-cs-controller -- linstor storage-pool list\n"
"----\n"

#. type: Table
msgid "  [[s-kubernetes-kubectl-linstor-utility]]"
msgstr "  [[s-kubernetes-kubectl-linstor-utility]]"

#. type: Title ====
#, no-wrap
msgid "Simplifying LINSTOR Client Command Entry"
msgstr "LINSTORクライアントコマンドエントリの簡略化"

#. type: Table
#, no-wrap
msgid ""
"\n"
"To simplify entering LINSTOR client commands within a Kubernetes deployment, you can use the\n"
"`kubectl-linstor` utility. This utility is available from the upstream Piraeus datastore\n"
"project. To download it, enter the following commands on your Kubernetes control plane node:\n"
"\n"
"[%autofit]\n"
"----\n"
msgstr ""
"\n"
"Kubernetesデプロイメント内でLINSTORクライアントコマンドを簡略化するために、`kubectl-linstor`ユーティリティを使用できます。このユーティリティは、上流のPiraeusデータストアプロジェクトから利用できます。それをダウンロードするには、次のコマンドをあなたのKubernetesコントロールプレーンノードで入力してください:\n"
"\n"
"[%autofit]\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"# KL_VERS=0.3.0 <1>\n"
"# KL_ARCH=linux_amd64 <2>\n"
"# curl -L -O \\\n"
"https://github.com/piraeusdatastore/kubectl-linstor/releases/download/v$KL_VERS/kubectl-linstor_v\"$KL_VERS\"_$KL_ARCH.tar.gz\n"
"----\n"
msgstr ""
"# KL_VERS=0.3.0 <1>\n"
"# KL_ARCH=linux_amd64 <2>\n"
"# curl -L -O \\\n"
"https://github.com/piraeusdatastore/kubectl-linstor/releases/download/v$KL_VERS/kubectl-linstor_v\"$KL_VERS\"_$KL_ARCH.tar.gz\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"<1> Set the shell variable `KL_VERS` to the latest release version of the `kubectl-linstor`\n"
"utility, as shown on the\n"
"https://github.com/piraeusdatastore/kubectl-linstor/releases[`kubectl-linstor` releases page].\n"
"<2> Set the shell variable `KL_ARCH` to the architecture appropriate to your deployment and\n"
"supported by the utility's available releases.\n"
"\n"
"IMPORTANT: If your deployment uses the LINSTOR Operator v2, you must use version 0.2.0 or higher\n"
"of the `kubectl-linstor` utility.\n"
"\n"
"NOTE: It is possible that the tar archive asset name could change over time. If you have issues downloading the asset by using the commands shown above, verify the naming convention of the asset that you want on the link:https://github.com/piraeusdatastore/kubectl-linstor/releases[`kubectl-linstor` releases page] or else manually download the asset\n"
"that you want from the releases page.\n"
"\n"
"To install the utility, first extract it and then move the extracted executable file to a\n"
"directory in your `$PATH`, for example, `/usr/bin`. Then you can use `kubectl-linstor` to get\n"
"access to the complete LINSTOR CLI.\n"
"\n"
"----\n"
msgstr ""
"\n"
"<1> [こちら](https://github.com/piraeusdatastore/kubectl-linstor/releases)にある`kubectl-linstor`のリリースページ。\n"
"\n"
"<2> シェル変数`KL_ARCH`を、デプロイメントに適していて、ユーティリティの利用可能なリリースでサポートされているアーキテクチャに設定してください。\n"
"\n"
"I重要：もし貴方のデプロイメントがLINSTOR Operator v2を使用している場合、`kubectl-linstor`ユーティリティのバージョンを0.2.0以上にする必要があります。\n"
"\n"
"注意: tarアーカイブのアセット名は時間と共に変更される可能性があります。上記のコマンドを使用してアセットをダウンロードする際に問題が発生した場合は、リンク:https://github.com/piraeusdatastore/kubectl-linstor/releases[`kubectl-linstor`リリースページ]で欲しいアセットの命名規則を確認するか、それ以外の場合はリリースページから欲しいアセットを手動でダウンロードしてください。\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "$ kubectl linstor node list\n"
#| "╭────────────────────────────────────────────────────────────────────────────────────╮\n"
#| "┊ Node                           ┊ NodeType   ┊ Addresses                   ┊ State  ┊\n"
#| "╞════════════════════════════════════════════════════════════════════════════════════╡\n"
#| "┊ kube-node-01.test              ┊ SATELLITE  ┊ 10.43.224.26:3366 (PLAIN)   ┊ Online ┊\n"
#| "┊ kube-node-02.test              ┊ SATELLITE  ┊ 10.43.224.27:3366 (PLAIN)   ┊ Online ┊\n"
#| "┊ kube-node-03.test              ┊ SATELLITE  ┊ 10.43.224.28:3366 (PLAIN)   ┊ Online ┊\n"
#| "┊ linstor-op-cs-controller-[...] ┊ CONTROLLER ┊ 172.24.116.114:3366 (PLAIN) ┊ Online ┊\n"
#| "╰────────────────────────────────────────────────────────────────────────────────────╯\n"
msgid ""
"$ kubectl linstor node list\n"
"╭────────────────────────────────────────────────────────────────────────────────────╮\n"
"┊ Node                           ┊ NodeType   ┊ Addresses                   ┊ State  ┊\n"
"╞════════════════════════════════════════════════════════════════════════════════════╡\n"
"┊ kube-node-01.test              ┊ SATELLITE  ┊ 10.43.224.26:3366 (PLAIN)   ┊ Online ┊\n"
"┊ kube-node-02.test              ┊ SATELLITE  ┊ 10.43.224.27:3366 (PLAIN)   ┊ Online ┊\n"
"┊ kube-node-03.test              ┊ SATELLITE  ┊ 10.43.224.28:3366 (PLAIN)   ┊ Online ┊\n"
"┊ linstor-op-cs-controller-[...] ┊ CONTROLLER ┊ 172.24.116.114:3366 (PLAIN) ┊ Online ┊\n"
"╰────────────────────────────────────────────────────────────────────────────────────╯\n"
"----\n"
msgstr ""
"$ kubectl linstor node list\n"
"╭────────────────────────────────────────────────────────────────────────────────────╮\n"
"┊ Node                           ┊ NodeType   ┊ Addresses                   ┊ State  ┊\n"
"╞════════════════════════════════════════════════════════════════════════════════════╡\n"
"┊ kube-node-01.test              ┊ SATELLITE  ┊ 10.43.224.26:3366 (PLAIN)   ┊ Online ┊\n"
"┊ kube-node-02.test              ┊ SATELLITE  ┊ 10.43.224.27:3366 (PLAIN)   ┊ Online ┊\n"
"┊ kube-node-03.test              ┊ SATELLITE  ┊ 10.43.224.28:3366 (PLAIN)   ┊ Online ┊\n"
"┊ linstor-op-cs-controller-[...] ┊ CONTROLLER ┊ 172.24.116.114:3366 (PLAIN) ┊ Online ┊\n"
"╰────────────────────────────────────────────────────────────────────────────────────╯\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "It also expands references to PVCs to the matching LINSTOR resource"
msgid ""
"\n"
"It also expands references to PVCs to the matching LINSTOR resource.\n"
"\n"
"----\n"
msgstr ""
"\n"
"それはPVCへの参照も対応するLINSTORリソースに拡張します。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "$ kubectl linstor resource list -r pvc:my-namespace/demo-pvc-1 --all\n"
#| "pvc:my-namespace/demo-pvc-1 -> pvc-2f982fb4-bc05-4ee5-b15b-688b696c8526\n"
#| "╭─────────────────────────────────────────────────────────────────────────────────────────────╮\n"
#| "┊ ResourceName ┊ Node              ┊ Port ┊ Usage  ┊ Conns ┊    State   ┊ CreatedOn           ┊\n"
#| "╞═════════════════════════════════════════════════════════════════════════════════════════════╡\n"
#| "┊ pvc-[...]    ┊ kube-node-01.test ┊ 7000 ┊ Unused ┊ Ok    ┊   UpToDate ┊ 2021-02-05 09:16:09 ┊\n"
#| "┊ pvc-[...]    ┊ kube-node-02.test ┊ 7000 ┊ Unused ┊ Ok    ┊ TieBreaker ┊ 2021-02-05 09:16:08 ┊\n"
#| "┊ pvc-[...]    ┊ kube-node-03.test ┊ 7000 ┊ InUse  ┊ Ok    ┊   UpToDate ┊ 2021-02-05 09:16:09 ┊\n"
#| "╰─────────────────────────────────────────────────────────────────────────────────────────────╯\n"
msgid ""
"$ kubectl linstor resource list -r pvc:my-namespace/demo-pvc-1 --all\n"
"pvc:my-namespace/demo-pvc-1 -> pvc-2f982fb4-bc05-4ee5-b15b-688b696c8526\n"
"╭─────────────────────────────────────────────────────────────────────────────────────────────╮\n"
"┊ ResourceName ┊ Node              ┊ Port ┊ Usage  ┊ Conns ┊    State   ┊ CreatedOn           ┊\n"
"╞═════════════════════════════════════════════════════════════════════════════════════════════╡\n"
"┊ pvc-[...]    ┊ kube-node-01.test ┊ 7000 ┊ Unused ┊ Ok    ┊   UpToDate ┊ 2021-02-05 09:16:09 ┊\n"
"┊ pvc-[...]    ┊ kube-node-02.test ┊ 7000 ┊ Unused ┊ Ok    ┊ TieBreaker ┊ 2021-02-05 09:16:08 ┊\n"
"┊ pvc-[...]    ┊ kube-node-03.test ┊ 7000 ┊ InUse  ┊ Ok    ┊   UpToDate ┊ 2021-02-05 09:16:09 ┊\n"
"╰─────────────────────────────────────────────────────────────────────────────────────────────╯\n"
"----\n"
msgstr ""
"$ kubectl linstor resource list -r pvc:my-namespace/demo-pvc-1 --all\n"
"pvc:my-namespace/demo-pvc-1 -> pvc-2f982fb4-bc05-4ee5-b15b-688b696c8526\n"
"╭─────────────────────────────────────────────────────────────────────────────────────────────╮\n"
"┊ ResourceName ┊ Node              ┊ Port ┊ Usage  ┊ Conns ┊    State   ┊ CreatedOn           ┊\n"
"╞═════════════════════════════════════════════════════════════════════════════════════════════╡\n"
"┊ pvc-[...]    ┊ kube-node-01.test ┊ 7000 ┊ Unused ┊ Ok    ┊   UpToDate ┊ 2021-02-05 09:16:09 ┊\n"
"┊ pvc-[...]    ┊ kube-node-02.test ┊ 7000 ┊ Unused ┊ Ok    ┊ TieBreaker ┊ 2021-02-05 09:16:08 ┊\n"
"┊ pvc-[...]    ┊ kube-node-03.test ┊ 7000 ┊ InUse  ┊ Ok    ┊   UpToDate ┊ 2021-02-05 09:16:09 ┊\n"
"╰─────────────────────────────────────────────────────────────────────────────────────────────╯\n"
"----\n"

#. type: Table
msgid "  It also expands references of the form `pod:[<namespace>/]<podname>` into a list resources in use by the pod.  This should only be necessary for investigating problems and accessing advanced functionality.  Regular operation such as creating volumes should be achieved through the <<s-kubernetes-basic-configuration-and-deployment,Kubernetes integration>>.  [[s-kubernetes-basic-configuration-and-deployment]]"
msgstr "`pod:[<namespace>/]<podname>`形式の参照を、そのポッドで使用されているリソースのリストに展開することも可能です。この機能は、問題の調査や高度な機能へのアクセスが必要な場合にのみ使用するべきです。ボリュームの作成などの通常の操作は、<<s-kubernetes-basic-configuration-and-deployment,Kubernetesの統合>>を介して行うべきです。"

#. type: Title ===
#, no-wrap
msgid "Getting Started with LINBIT SDS Storage in Kubernetes"
msgstr "KubernetesでのLINBIT SDSストレージの始め方"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Once all linstor-csi __Pod__s are up and running, you can provision volumes\n"
"using the usual Kubernetes workflows.\n"
"\n"
"Configuring the behavior and properties of LINSTOR volumes deployed through Kubernetes\n"
"is accomplished using link:https://kubernetes.io/docs/concepts/storage/storage-classes/[Kubernetes __StorageClass__] objects.\n"
"\n"
"IMPORTANT: The `resourceGroup` parameter is mandatory. Because Kubernetes StorageClass objects have a one-to-one correspondence with LINSTOR resource groups, you usually want the `resourceGroup` parameter to be unique and the same as the storage class name.\n"
"\n"
"Here below is the simplest practical _StorageClass_ that can be used to deploy volumes:\n"
"\n"
".linstor-basic-sc.yaml\n"
"[source,yaml]\n"
"----\n"
msgstr ""
"\n"
"全ての linstor-csi __Pod__ が稼働していると、通常の Kubernetes ワークフローを使用してボリュームをプロビジョニングすることができます。.\n"
"\n"
"Kubernetesを通じてデプロイされたLINSTORボリュームの挙動やプロパティを設定するには、link:https://kubernetes.io/docs/concepts/storage/storage-classes/[Kubernetes __StorageClass__]オブジェクトを使用します。\n"
"\n"
"重要: `resourceGroup` パラメータは必須です。なぜなら、KubernetesのStorageClassオブジェクトはLINSTORリソースグループと一対一の対応関係があるため、通常は `resourceGroup` パラメータを一意であり、ストレージクラスの名前と同じにしたいと思うでしょう。\n"
"\n"
"ここ以下に、ボリュームをデプロイするのに使用できる最もシンプルな実用的な_StorageClass_が示されています。\n"
"\n"
".linstor-basic-sc.yaml\n"
"[source,yaml]\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: storage.k8s.io/v1\n"
"kind: StorageClass\n"
"metadata:\n"
"  # The name used to identify this StorageClass.\n"
"  name: linstor-basic-storage-class\n"
"  # The name used to match this StorageClass with a provisioner.\n"
"  # linstor.csi.linbit.com is the name that the LINSTOR CSI plugin uses to identify itself\n"
"provisioner: linstor.csi.linbit.com\n"
"volumeBindingMode: WaitForFirstConsumer\n"
"parameters:\n"
"  # LINSTOR will provision volumes from the drbdpool storage pool configured\n"
"  # On the satellite nodes in the LINSTOR cluster specified in the plugin's deployment\n"
"  storagePool: \"lvm-thin\"\n"
"  resourceGroup: \"linstor-basic-storage-class\"\n"
"  # Setting a fstype is required for \"fsGroup\" permissions to work correctly.\n"
"  # Currently supported: xfs/ext4\n"
"  csi.storage.k8s.io/fstype: xfs\n"
"----\n"
msgstr ""
"apiVersion: storage.k8s.io/v1\n"
"kind: StorageClass\n"
"metadata:\n"
"  # The name used to identify this StorageClass.\n"
"  name: linstor-basic-storage-class\n"
"  # The name used to match this StorageClass with a provisioner.\n"
"  # linstor.csi.linbit.com is the name that the LINSTOR CSI plugin uses to identify itself\n"
"provisioner: linstor.csi.linbit.com\n"
"volumeBindingMode: WaitForFirstConsumer\n"
"parameters:\n"
"  # LINSTOR will provision volumes from the drbdpool storage pool configured\n"
"  # On the satellite nodes in the LINSTOR cluster specified in the plugin's deployment\n"
"  storagePool: \"lvm-thin\"\n"
"  resourceGroup: \"linstor-basic-storage-class\"\n"
"  # Setting a fstype is required for \"fsGroup\" permissions to work correctly.\n"
"  # Currently supported: xfs/ext4\n"
"  csi.storage.k8s.io/fstype: xfs\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"IMPORTANT: The `storagePool` value, `lvm-thin` in the example YAML configuration file above, must match an available LINSTOR _StoragePool_. You can list storage pool information using the `linstor storage-pool list` command, executed within the running `linstor-op-cs-controller` pod, or by using the `kubectl linstor storage-pool list` command if you have installed the <<s-kubernetes-kubectl-linstor-utility,`kubectl-linstor` utility>>.\n"
"\n"
"You can create the storage class with the following command:\n"
"\n"
"----\n"
msgstr ""
"\n"
"重要: 上記のYAML設定ファイルの例で`storagePool`の値が`lvm-thin`である必要があります。利用可能なLINSTORの_StoragePool_ に一致するようにしてください。実行中の`linstor-op-cs-controller`ポッド内で`linstor storage-pool list`コマンドを使用するか、<<s-kubernetes-kubectl-linstor-utility,`kubectl-linstor` ユーティリティ>>をインストールしている場合は`kubectl linstor storage-pool list`コマンドを使用して、ストレージプールの情報を一覧表示することができます。\n"
"\n"
"次のコマンドを使用して、ストレージクラスを作成できます: \n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "kubectl create -f linstor-basic-sc.yaml\n"
msgid ""
"kubectl create -f linstor-basic-sc.yaml\n"
"----\n"
msgstr ""
"kubectl create -f linstor-basic-sc.yaml\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Now that your storage class is created, you can now create a persistent volume claim (PVC)\n"
"which can be used to provision volumes known both to Kubernetes and LINSTOR:\n"
"\n"
".my-first-linstor-volume-pvc.yaml\n"
"[source,yaml]\n"
"----\n"
msgstr ""
"\n"
"今回、ストレージクラスが作成されたので、KubernetesとLINSTORの両方で知られているボリュームをプロビジョニングするために使用できる永続ボリュームクレーム（PVC）を作成できます。\n"
"\n"
".my-first-linstor-volume-pvc.yaml\n"
"[source,yaml]\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "kind: PersistentVolumeClaim\n"
#| "apiVersion: v1\n"
#| "metadata:\n"
#| "  name: my-first-linstor-volume\n"
#| "spec:\n"
#| "  storageClassName: linstor-basic-storage-class\n"
#| "  accessModes:\n"
#| "    - ReadWriteOnce\n"
#| "  resources:\n"
#| "    requests:\n"
#| "      storage: 500Mi\n"
msgid ""
"kind: PersistentVolumeClaim\n"
"apiVersion: v1\n"
"metadata:\n"
"  name: my-first-linstor-volume\n"
"spec:\n"
"  storageClassName: linstor-basic-storage-class\n"
"  accessModes:\n"
"    - ReadWriteOnce\n"
"  resources:\n"
"    requests:\n"
"      storage: 500Mi\n"
"----\n"
msgstr ""
"kind: PersistentVolumeClaim\n"
"apiVersion: v1\n"
"metadata:\n"
"  name: my-first-linstor-volume\n"
"spec:\n"
"  storageClassName: linstor-basic-storage-class\n"
"  accessModes:\n"
"    - ReadWriteOnce\n"
"  resources:\n"
"    requests:\n"
"      storage: 500Mi\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"You can create the _PersistentVolumeClaim_ with the following command:\n"
"\n"
"----\n"
msgstr ""
"\n"
"次のコマンドで _PersistentVolumeClaim_ を作成できます。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "kubectl create -f my-first-linstor-volume-pvc.yaml\n"
msgid ""
"kubectl create -f my-first-linstor-volume-pvc.yaml\n"
"----\n"
msgstr ""
"kubectl create -f my-first-linstor-volume-pvc.yaml\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"This will create a _PersistentVolumeClaim_, but no volume will be created just yet.\n"
"The storage class we used specified `volumeBindingMode: WaitForFirstConsumer`, which\n"
"means that the volume is only created once a workload starts using it. This ensures\n"
"that the volume is placed on the same node as the workload.\n"
"\n"
"For our example, we create a simple Pod, which mounts or volume by referencing the\n"
"_PersistentVolumeClaim_.\n"
".my-first-linstor-volume-pod.yaml\n"
"[source,yaml]\n"
"----\n"
msgstr ""
"\n"
"これにより、_PersistentVolumeClaim_ が作成されますが、その時点ではボリュームは作成されません。 使用したストレージクラスは `volumeBindingMode: WaitForFirstConsumer` を指定していました。これは、ボリュームが作業負荷を開始するときにのみ作成されることを意味します。これにより、ボリュームが作業負荷と同じノードに配置されることが保証されます。\n"
"\n"
"私たちの例では、_PersistentVolumeClaim_ を参照してボリュームをマウントする単純な Pod を作成します。\n"
".my-first-linstor-volume-pod.yaml\n"
"[source,yaml]\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "apiVersion: v1\n"
#| "kind: Pod\n"
#| "metadata:\n"
#| "  name: fedora\n"
#| "  namespace: default\n"
#| "spec:\n"
#| "  containers:\n"
#| "  - name: fedora\n"
#| "    image: fedora\n"
#| "    command: [/bin/bash]\n"
#| "    args: [\"-c\", \"while true; do sleep 10; done\"]\n"
#| "    volumeMounts:\n"
#| "    - name: my-first-linstor-volume\n"
#| "      mountPath: /data\n"
#| "    ports:\n"
#| "    - containerPort: 80\n"
#| "  volumes:\n"
#| "  - name: my-first-linstor-volume\n"
#| "    persistentVolumeClaim:\n"
#| "      claimName: \"my-first-linstor-volume\"\n"
msgid ""
"apiVersion: v1\n"
"kind: Pod\n"
"metadata:\n"
"  name: fedora\n"
"  namespace: default\n"
"spec:\n"
"  containers:\n"
"  - name: fedora\n"
"    image: fedora\n"
"    command: [/bin/bash]\n"
"    args: [\"-c\", \"while true; do sleep 10; done\"]\n"
"    volumeMounts:\n"
"    - name: my-first-linstor-volume\n"
"      mountPath: /data\n"
"    ports:\n"
"    - containerPort: 80\n"
"  volumes:\n"
"  - name: my-first-linstor-volume\n"
"    persistentVolumeClaim:\n"
"      claimName: \"my-first-linstor-volume\"\n"
"----\n"
msgstr ""
"apiVersion: v1\n"
"kind: Pod\n"
"metadata:\n"
"  name: fedora\n"
"  namespace: default\n"
"spec:\n"
"  containers:\n"
"  - name: fedora\n"
"    image: fedora\n"
"    command: [/bin/bash]\n"
"    args: [\"-c\", \"while true; do sleep 10; done\"]\n"
"    volumeMounts:\n"
"    - name: my-first-linstor-volume\n"
"      mountPath: /data\n"
"    ports:\n"
"    - containerPort: 80\n"
"  volumes:\n"
"  - name: my-first-linstor-volume\n"
"    persistentVolumeClaim:\n"
"      claimName: \"my-first-linstor-volume\"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"You can create the _Pod_ with the following command:\n"
"\n"
"----\n"
msgstr ""
"\n"
"次のコマンドで _Pod_ を作成できます。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "kubectl create -f my-first-linstor-volume-pod.yaml\n"
msgid ""
"kubectl create -f my-first-linstor-volume-pod.yaml\n"
"----\n"
msgstr ""
"kubectl create -f my-first-linstor-volume-pod.yaml\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Running `kubectl describe pod fedora` can be used to confirm that _Pod_\n"
"scheduling and volume attachment succeeded. Examining the _PersistentVolumeClaim_,\n"
"we can see that it is now bound to a volume.\n"
"\n"
"To remove a volume, verify that no pod is using it and then delete the\n"
"_PersistentVolumeClaim_ using the `kubectl` command. For example, to remove the volume that we\n"
"just made, run the following two commands, noting that the _Pod_ must be\n"
"unscheduled before the _PersistentVolumeClaim_ will be removed:\n"
"\n"
"----\n"
msgstr ""
"\n"
"`kubectl describe pod fedora`を実行することで、<<Pod_scheduling,ポッドのスケジューリング>>と<<volume attachment,ボリュームのアタッチメント>>が成功したことを確認できます。_PersistentVolumeClaim_を調査すると、今やボリュームにバインドされていることがわかります。\n"
"\n"
"ボリュームを削除するには、いかなるポッドもそれを使用していないことを確認し、`kubectl`コマンドを使用して _PersistentVolumeClaim_ を削除します。たとえば、作成したばかりのボリュームを削除するには、以下の2つのコマンドを実行します。なお、_PersistentVolumeClaim_ を削除する前に、_Pod_ がスケジュールされていない必要があります。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "kubectl delete pvc my-first-linstor-volume # remove the PersistentVolumeClaim, the PersistentVolume, and the LINSTOR Volume.\n"
msgid ""
"kubectl delete pod fedora # unschedule the pod.\n"
"\n"
"kubectl get pod -w # wait for pod to be unscheduled\n"
"\n"
"kubectl delete pvc my-first-linstor-volume # remove the PersistentVolumeClaim, the PersistentVolume, and the LINSTOR Volume.\n"
"----\n"
msgstr ""
"kubectl delete pod fedora # unschedule the pod.\n"
"\n"
"kubectl get pod -w # wait for pod to be unscheduled\n"
"\n"
"kubectl delete pvc my-first-linstor-volume # remove the PersistentVolumeClaim, the PersistentVolume, and the LINSTOR Volume.\n"
"----\n"

#. type: Table
msgid "  [[s-kubernetes-sc-parameters]]"
msgstr "  [[s-kubernetes-sc-parameters]]"

#. type: Title ====
#, no-wrap
msgid "Available Parameters in a Storage Class"
msgstr "StorageClass で使用可能なパラメータ"

#. type: Table
#, no-wrap
msgid ""
"\n"
"The following storage class contains all currently available parameters to configure the provisioned storage.\n"
"\n"
"NOTE: `linstor.csi.linbit.com/` is an optional, but recommended prefix for LINSTOR CSI specific parameters.\n"
"\n"
"[source,yaml]\n"
"----\n"
msgstr ""
"\n"
"次のストレージクラスには、現在利用可能なすべてのパラメータが含まれています。.\n"
"\n"
"注意: `linstor.csi.linbit.com/` は、LINSTOR CSI 特有のパラメーターに対してオプションですが、推奨されています。\n"
"\n"
"[source,yaml]\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: storage.k8s.io/v1\n"
"kind: StorageClass\n"
"metadata:\n"
"  name: full-example\n"
"provisioner: linstor.csi.linbit.com\n"
"parameters:\n"
"  # CSI related parameters\n"
"  csi.storage.k8s.io/fstype: xfs\n"
"  # LINSTOR parameters\n"
"  linstor.csi.linbit.com/autoPlace: \"2\"\n"
"  linstor.csi.linbit.com/placementCount: \"2\"\n"
"  linstor.csi.linbit.com/resourceGroup: \"full-example\"\n"
"  linstor.csi.linbit.com/storagePool: \"my-storage-pool\"\n"
"  linstor.csi.linbit.com/disklessStoragePool: \"DfltDisklessStorPool\"\n"
"  linstor.csi.linbit.com/layerList: \"drbd storage\"\n"
"  linstor.csi.linbit.com/placementPolicy: \"AutoPlaceTopology\"\n"
"  linstor.csi.linbit.com/allowRemoteVolumeAccess: \"true\"\n"
"  linstor.csi.linbit.com/encryption: \"true\"\n"
"  linstor.csi.linbit.com/nodeList: \"diskful-a diskful-b\"\n"
"  linstor.csi.linbit.com/clientList: \"diskless-a diskless-b\"\n"
"  linstor.csi.linbit.com/replicasOnSame: \"zone=a\"\n"
"  linstor.csi.linbit.com/replicasOnDifferent: \"rack\"\n"
"  linstor.csi.linbit.com/disklessOnRemaining: \"false\"\n"
"  linstor.csi.linbit.com/doNotPlaceWithRegex: \"tainted.*\"\n"
"  linstor.csi.linbit.com/fsOpts: \"-E nodiscard\"\n"
"  linstor.csi.linbit.com/mountOpts: \"noatime\"\n"
"  linstor.csi.linbit.com/postMountXfsOpts: \"extsize 2m\"\n"
"  # Linstor properties\n"
"  property.linstor.csi.linbit.com/*: <x>\n"
"  # DRBD parameters\n"
"  DrbdOptions/*: <x>\n"
"----\n"
msgstr ""
"apiVersion: storage.k8s.io/v1\n"
"kind: StorageClass\n"
"metadata:\n"
"  name: full-example\n"
"provisioner: linstor.csi.linbit.com\n"
"parameters:\n"
"  # CSI related parameters\n"
"  csi.storage.k8s.io/fstype: xfs\n"
"  # LINSTOR parameters\n"
"  linstor.csi.linbit.com/autoPlace: \"2\"\n"
"  linstor.csi.linbit.com/placementCount: \"2\"\n"
"  linstor.csi.linbit.com/resourceGroup: \"full-example\"\n"
"  linstor.csi.linbit.com/storagePool: \"my-storage-pool\"\n"
"  linstor.csi.linbit.com/disklessStoragePool: \"DfltDisklessStorPool\"\n"
"  linstor.csi.linbit.com/layerList: \"drbd storage\"\n"
"  linstor.csi.linbit.com/placementPolicy: \"AutoPlaceTopology\"\n"
"  linstor.csi.linbit.com/allowRemoteVolumeAccess: \"true\"\n"
"  linstor.csi.linbit.com/encryption: \"true\"\n"
"  linstor.csi.linbit.com/nodeList: \"diskful-a diskful-b\"\n"
"  linstor.csi.linbit.com/clientList: \"diskless-a diskless-b\"\n"
"  linstor.csi.linbit.com/replicasOnSame: \"zone=a\"\n"
"  linstor.csi.linbit.com/replicasOnDifferent: \"rack\"\n"
"  linstor.csi.linbit.com/disklessOnRemaining: \"false\"\n"
"  linstor.csi.linbit.com/doNotPlaceWithRegex: \"tainted.*\"\n"
"  linstor.csi.linbit.com/fsOpts: \"-E nodiscard\"\n"
"  linstor.csi.linbit.com/mountOpts: \"noatime\"\n"
"  linstor.csi.linbit.com/postMountXfsOpts: \"extsize 2m\"\n"
"  # Linstor properties\n"
"  property.linstor.csi.linbit.com/*: <x>\n"
"  # DRBD parameters\n"
"  DrbdOptions/*: <x>\n"
"----\n"

#. type: Table
msgid "  [[s-kubernetes-drbd-options-setting]]"
msgstr "  [[s-kubernetes-drbd-options-setting]]"

#. type: Title ====
#, no-wrap
msgid "Setting DRBD Options for Storage Resources in Kubernetes"
msgstr "KubernetesでストレージリソースのためのDRBDオプションを設定する"

#. type: Table
#, no-wrap
msgid ""
"\n"
"As shown in the <<s-kubernetes-sc-parameters,>> section, you can set DRBD options within a\n"
"storage class configuration. Because of the one-to-one correspondence between a StorageClass\n"
"Kubernetes object and its named LINSTOR resource group (`resourceGroup` parameter), setting DRBD\n"
"options within a storage class configuration is similar to setting DRBD options on the LINSTOR\n"
"resource group.\n"
"\n"
"There are some differences. If you set DRBD options within a storage class configuration, these\n"
"options will only affect new volumes that are created from the storage class. The options will\n"
"not affect existing volumes. Furthermore, because you cannot just update the storage class, you\n"
"will have to delete it and create it again if you add DRBD options to the storage class's\n"
"configuration. If you set DRBD options on the LINSTOR resource group object (`linstor\n"
"resource-group set-property <rg-name> DrbdOptions/<option-name>`), changes will affect future\n"
"and existing volumes belonging to the resource group. \n"
"\n"
"WARNING: If you set DRBD options on a LINSTOR resource group that also corresponds to a\n"
"Kubernetes storage class, the next time a volume is created, the CSI driver will revert changes\n"
"to DRBD options that are only in the resource group, unless you also configure the DRBD options\n"
"in the storage class. \n"
"\n"
"Because of the potential pitfalls here, it is simpler to set DRBD options on the LINSTOR\n"
"controller object. DRBD options set on the controller will apply to all resources groups,\n"
"resources, and volumes, unless you have also set the same options on any of those LINSTOR\n"
"objects. Refer to the <<s-linstor-introduction-linstor-object-hierarchy,>> section for more\n"
"details about LINSTOR object hierarchy.\n"
"\n"
"You can list the properties, including DRBD options, that you can set on the LINSTOR controller\n"
"object by entering the following command:\n"
"\n"
"----\n"
msgstr ""
"\n"
"<<s-kubernetes-sc-parameters,>>セクションに示されているように、ストレージクラスの構成内でDRBDオプションを設定できます。ストレージクラスのKubernetesオブジェクトとその名前付きLINSTORリソースグループ（`resourceGroup`パラメータ）との一対一の対応関係のため、ストレージクラスの構成内でDRBDオプションを設定することは、LINSTORリソースグループに対するDRBDオプションの設定と似ています。\n"
"\n"
"いくつかの違いがあります。ストレージクラスの構成内でDRBDオプションを設定すると、これらのオプションはストレージクラスから作成される新しいボリュームにのみ影響します。これらのオプションは既存のボリュームには影響しません。さらに、ストレージクラスを単純に更新することはできないため、ストレージクラスの構成にDRBDオプションを追加する場合は、そのストレージクラスを削除して再作成する必要があります。また、LINSTORリソースグループオブジェクトでDRBDオプションを設定すると（`linstor resource-group set-property <rg-name> DrbdOptions/<option-name>`）、変更は将来のボリュームおよびリソースグループに属する既存のボリュームに影響します。\n"
"\n"
"警告: LINSTORリソースグループにDRBDオプションを設定し、それがKubernetesストレージクラスにも対応している場合、次にボリュームが作成されるとき、CSIドライバーは、リソースグループ内にのみあるDRBDオプションの変更を元に戻します。この問題を解決するには、ストレージクラスでもDRBDオプションを設定する必要があります。\n"
"\n"
"潜在的な落とし穴があるため、LINSTORコントローラーオブジェクトにDRBDオプションを設定する方が簡単です。コントローラーに設定されたDRBDオプションは、すべてのリソースグループ、リソース、およびボリュームに適用されます。これらのLINSTORオブジェクトのいずれかに同じオプションが設定されていない限りです。LINSTORオブジェクト階層に関する詳細については、<<s-linstor-introduction-linstor-object-hierarchy,>>セクションを参照してください。\n"
"\n"
"以下のコマンドを入力することで、LINSTORコントローラオブジェクトに設定できるプロパティ（DRBDオプションを含む）をリストアップできます:\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "kubectl exec deployment/linstor-op-cs-controller -- linstor storage-pool list\n"
msgid ""
"# kubectl exec -n linbit-sds deployment/linstor-controller -- \\\n"
"linstor controller set-property --help\n"
"----\n"
msgstr ""
"# kubectl exec -n linbit-sds deployment/linstor-controller -- \\\n"
"linstor controller set-property --help\n"
"----\n"

#. type: Table
msgid "  [[s-kubernetes-drbd-options-setting-on-controller]]"
msgstr "  [[s-kubernetes-drbd-options-setting-on-controller]]"

#. type: Title =====
#, no-wrap
msgid "Setting DRBD Options on the LINSTOR Controller in Kubernetes"
msgstr "Kubernetes内のLINSTORコントローラーにDRBDオプションを設定する"

#. type: Table
#, no-wrap
msgid ""
"\n"
"To set DRBD options on the LINSTOR controller in a Kubernetes deployment, edit the\n"
"link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md#specproperties[`LinstorCluster`\n"
"configuration]. For example, to set transport encryption for all DRBD traffic: \n"
"\n"
"----\n"
msgstr ""
"\n"
"Kubernetesデプロイメントにおいて、LINSTORコントローラ上のDRBDオプションを設定するには、link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorcluster.md#specproperties[`LinstorCluster`configuration]を編集します。例えば、すべてのDRBDトラフィックに対して転送暗号化を設定するには、次のようにします：\n"
"\n"
"----\n"
"\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec:\n"
"  properties:\n"
"    # This one will be set on the controller, verify with: linstor controller list-properties\n"
"    # Enable TLS for all resources by default\n"
"    - name: \"DrbdOptions/Net/tls\"\n"
"      value: \"yes\"\n"
"----\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorCluster\n"
"metadata:\n"
"  name: linstorcluster\n"
"spec:\n"
"  properties:\n"
"    # This one will be set on the controller, verify with: linstor controller list-properties\n"
"    # Enable TLS for all resources by default\n"
"    - name: \"DrbdOptions/Net/tls\"\n"
"      value: \"yes\"\n"
"----\n"

#. type: Table
msgid "  After editing the `LinstorCluster` configuration, apply it to your deployment by entering `kubectl apply -f <configuration-file>`.  [[s-kubernetes-drbd-options-setting-on-node-connection]]"
msgstr "`LinstorCluster`構成を編集した後、`kubectl apply -f <configuration-file>`を入力してデプロイメントに適用してください。"

#. type: Title =====
#, no-wrap
msgid "Setting DRBD Options on a LINSTOR Node Connection in Kubernetes"
msgstr "KubernetesにおけるLINSTORノード接続におけるDRBDオプションの設定"

#. type: Table
#, no-wrap
msgid ""
"\n"
"You can set DRBD options on LINSTOR node connections in Kubernetes, by editing the Kubernetes\n"
"link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstornodeconnection.md#specproperties[`LinstorNodeConnection`]\n"
"configuration. Instructions are similar for editing and applying a `LinstorCluster`\n"
"configuration, described in the previous section.\n"
"\n"
"DRBD options set at the node connection level will take precedence over DRBD options set at the\n"
"controller level and satellite node levels.\n"
"\n"
"The following is an example node connection configuration that will do two things:\n"
"\n"
". Select pairs of nodes (a node connection by definition connects two nodes) that are in\n"
"  different geographical regions, for example, two data centers.\n"
". Set a DRBD option to make DRBD use\n"
"  link:file:///home/michael/linbit-documentation/UG9/en/output-html/drbd-users-guide-without-css.html#s-replication-protocols[protocol\n"
"  A] (asynchronous replication) on the node level connection between nodes that match the\n"
"  selection criterion.\n"
"\n"
"----\n"
msgstr ""
"\n"
"Kubernetes上で、LINSTORノード接続に対するDRBDオプションを設定することができます。これは、link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstornodeconnection.md#specproperties[`LinstorNodeConnection`]構成を編集することで行います。`LinstorCluster`構成の編集と適用手順は前のセクションで説明されているものと似ています。\n"
"\n"
"ノード接続レベルで設定されたDRBDオプションは、コントローラーレベルやサテライトノードレベルで設定されたDRBDオプションよりも優先されます。ノード接続レベルで設定されたDRBDオプションは、コントローラーレベルやサテライトノードレベルで設定されたDRBDオプションよりも優先されます。\n"
"\n"
"次は、2つのことを行う例となるノード接続構成の例です。\n"
"\n"
". 異なる地理的領域、例えば、2つのデータセンターにあるノードのペアを選択してください。\n"
". DRBDが選択基準を満たすノード間のノードレベル接続でlink:file:///home/michael/linbit-documentation/UG9/en/output-html/drbd-users-guide-without-css.html#s-replication-protocols[protocol A]（非同期レプリケーション）を使用するためのDRBDオプションを設定してください。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorNodeConnection\n"
"metadata:\n"
"  name: cross-region\n"
"spec:\n"
"  selector:\n"
"    # Select pairs of nodes (A, B) where A is in a different region than node B.\n"
"    - matchLabels:\n"
"        - key: topology.kubernetes.io/region\n"
"          op: NotSame\n"
"  properties:\n"
"    # This property will be set on the node connection, verify with:\n"
"    # linstor node-connection list-properties <node1> <node2>\n"
"    # Configure DRBD protocol A between regions for reduced latency\n"
"    - name: DrbdOptions/Net/protocol\n"
"      value: A\n"
"----\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorNodeConnection\n"
"metadata:\n"
"  name: cross-region\n"
"spec:\n"
"  selector:\n"
"    # Select pairs of nodes (A, B) where A is in a different region than node B.\n"
"    - matchLabels:\n"
"        - key: topology.kubernetes.io/region\n"
"          op: NotSame\n"
"  properties:\n"
"    # This property will be set on the node connection, verify with:\n"
"    # linstor node-connection list-properties <node1> <node2>\n"
"    # Configure DRBD protocol A between regions for reduced latency\n"
"    - name: DrbdOptions/Net/protocol\n"
"      value: A\n"
"----\n"

#. type: Table
msgid "  NOTE: Refer to link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstornodeconnection.md#specselector[documentation within the upstream Piraeus project] for more details about the node connection `selector` specification.  [[s-kubernetes-drbd-options-setting-on-satellite]]"
msgstr "  NOTE: ノード接続`selector`の仕様についての詳細については、<<https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstornodeconnection.md#specselector[upstream Piraeus project]>>内のドキュメントを参照してください。  [[s-kubernetes-drbd-options-setting-on-satellite]]"

#. type: Title =====
#, no-wrap
msgid "Setting DRBD Options on LINSTOR Satellite Nodes in Kubernetes"
msgstr "KubernetesにおけるLINSTORサテライトノードでのDRBDオプションの設定"

#. type: Table
#, no-wrap
msgid ""
"\n"
"You can set DRBD options on LINSTOR satellite nodes in Kubernetes, by editing the Kubernetes\n"
"`LinstorSatelliteConfiguration`] configuration. Instructions are similar for editing and\n"
"applying a `LinstorCluster` or `LinstorNodeConnection` configuration, described in the previous\n"
"sections.\n"
"\n"
"DRBD options set at the satellite node level will take precedence over DRBD options set at the\n"
"controller level.\n"
"\n"
"To set a DRBD option that would prevent LINSTOR from automatically evicting a node, you could\n"
"use the following configuration file, provided that you apply an `under-maintenance` label to\n"
"the node that you want to disable the automatic eviction feature for. This might be useful\n"
"during a system maintenance window on a node.\n"
"\n"
"----\n"
msgstr ""
"\n"
"Kubernetes内のLINSTORサテライトノードでDRBDオプションを設定することができます。これは、Kubernetes `LinstorSatelliteConfiguration`設定を編集することで行います。`LinstorCluster`または`LinstorNodeConnection`設定を編集および適用する手順は、前のセクションで説明されているものと似ています。\n"
"\n"
"サテライトノードレベルで設定されたDRBDオプションは、コントローラーレベルで設定されたDRBDオプションよりも優先されます。\n"
"\n"
" DRBDのオプションを設定して、LINSTORがノードを自動的に追い出さないようにするには、次の設定ファイルを使用することができます。ただし、自動追い出し機能を無効にしたいノードに 'under-maintenance' ラベルを適用する必要があります。これはノードのシステムメンテナンスウィンドウ中に便利です。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: nodes-under-maintenance\n"
"spec:\n"
"  nodeSelector:\n"
"    under-maintenance: \"yes\"\n"
"  properties:\n"
"    - name: DrbdOptions/AutoEvictAllowEviction\n"
"      value: \"false\"\n"
"----\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: nodes-under-maintenance\n"
"spec:\n"
"  nodeSelector:\n"
"    under-maintenance: \"yes\"\n"
"  properties:\n"
"    - name: DrbdOptions/AutoEvictAllowEviction\n"
"      value: \"false\"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"[[s-kubernetes-linstor-properties-setting-on-storage-pools]]\n"
"====== Setting LINSTOR Properties on LINSTOR Storage Pools in Kubernetes\n"
"\n"
"Additionaly, you can specify LINSTOR properties (not DRBD options) on LINSTOR storage pools that\n"
"might exist on LINSTOR satellite nodes, as shown in the example configuration that follows.\n"
"\n"
"The example configuration would apply to all LINSTOR satellite nodes in your Kubernetes\n"
"deployment. However, it is possible to select only certain nodes within a configuration, similar\n"
"to the configuration example in <<s-kubernetes-drbd-options-setting-on-satellite,>>. Refer to\n"
"link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md#linstorsatelliteconfiguration[documentation\n"
"in the upstream Piraeus project] for details.\n"
"\n"
"----\n"
msgstr ""
"\n"
"[[s-kubernetes-linstor-properties-setting-on-storage-pools]]\n"
"====== KubernetesにおけるLINSTORストレージプールのLINSTORプロパティの設定\n"
"\n"
"さらに、次に続く例の構成に示されているように、LINSTORサテライトノード上に存在するLINSTORストレージプールに対して、LINSTORプロパティ（DRBDオプションではない）を指定することもできます。\n"
"\n"
"この例の設定は、あなたのKubernetesデプロイメント内のすべてのLINSTORサテライトノードに適用されます。ただし、<<s-kubernetes-drbd-options-setting-on-satellite,>>内の設定例と同様に、特定のノードのみを選択することも可能です。詳細については、アップストリームPiraeusプロジェクトのドキュメント[documentation in the upstream Piraeus project] (https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md#linstorsatelliteconfiguration) を参照してください。\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"spec:\n"
"  storagePools:\n"
"    - name: pool1\n"
"      lvmThinPool: {}\n"
"      properties:\n"
"        # This one will be set on the storage pool, verify with:\n"
"        # linstor storage-pool list-properties <node> <pool>\n"
"        # Set the oversubscription ratio on the storage pool to 1, i.e. no oversubscription.\n"
"        - name: MaxOversubscriptionRatio\n"
"          value: \"1\"\n"
"----\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"spec:\n"
"  storagePools:\n"
"    - name: pool1\n"
"      lvmThinPool: {}\n"
"      properties:\n"
"        # This one will be set on the storage pool, verify with:\n"
"        # linstor storage-pool list-properties <node> <pool>\n"
"        # Set the oversubscription ratio on the storage pool to 1, i.e. no oversubscription.\n"
"        - name: MaxOversubscriptionRatio\n"
"          value: \"1\"\n"
"----\n"

#. type: Table
msgid "  [[s-kubernetes-file-system]]"
msgstr "  [[s-kubernetes-file-system]]"

#. type: Title ====
#, no-wrap
#| msgid "csi.storage.k8s.io/fstype"
msgid "`csi.storage.k8s.io/fstype`"
msgstr "`csi.storage.k8s.io/fstype`"

#. type: Table
msgid "  The `csi.storage.k8s.io/fstype` parameter sets the file system type to create for `volumeMode: FileSystem` PVCs. Currently supported are: * `ext4` (default)  * `xfs` [[s-kubernetes-autoplace]]"
msgstr "`csi.storage.k8s.io/fstype`パラメータは、`volumeMode: FileSystem` PVCsの作成に使用するファイルシステムタイプを設定します。現在サポートされているのは以下の通りです：* `ext4`（デフォルト）* `xfs` [[s-kubernetes-autoplace]]"

#. type: Title ====
#, no-wrap
#| msgid "autoPlace"
msgid "`autoPlace`"
msgstr "`autoPlace`"

#. type: Table
msgid ""
"  `autoPlace` is an integer that determines the amount of replicas a volume of this _StorageClass_ will have. For example, `autoPlace: \"3\"` will produce volumes with three-way replication. If neither `autoPlace` nor `nodeList` are set, volumes will be <<s-autoplace-linstor,automatically placed>> on one node.  IMPORTANT: If you use this option, you must not use <<s-kubernetes-nodelist,`nodeList`>>.  IMPORTANT: You have to use quotes, otherwise Kubernetes will "
"complain about a malformed _StorageClass_.  TIP: This option (and all options which affect auto-placement behavior) modifies the number of LINSTOR nodes on which the underlying storage for volumes will be provisioned and is orthogonal to which _kubelets_ those volumes will be accessible from."
msgstr ""
"`autoPlace` は、この _StorageClass_のボリュームにレプリカがいくつ作成されるかを決定する整数です。たとえば `autoPlace: \"3\"` は、3つの複製を持つボリュームを生成します。`autoPlace` も `nodeList` も設定されていない場合、ボリュームは1つのノードに<<s-autoplace-linstor,自動的に配置>>されます。重要: このオプションを使用する場合、<<s-kubernetes-nodelist,`nodeList`>> を使用しないでください。重要: ダブルクオーテーションを使用する必要があります。そうしないと Kubernetes は不"
"正な _StorageClass_ としてエラーを出力します。ヒント: このオプション（および自動配置動作に影響を与えるすべてのオプション）は、ボリュームの基礎ストレージがプロビジョニングされる LINSTOR ノードの数を変更し、どの _kubelets_ からそのボリュームにアクセスできるかには直交しています。"

#. type: Title ====
#, no-wrap
#| msgid "placementCount"
msgid "`placementCount`"
msgstr "`placementCount`"

#. type: Table
msgid "  `placementCount` is an alias for <<s-kubernetes-autoplace,`autoPlace`>>"
msgstr "`placementCount` は<<s-kubernetes-autoplace,`autoPlace`>> のエイリアス"

#. type: Title ====
#, no-wrap
#| msgid "resourceGroup"
msgid "`resourceGroup`"
msgstr "`resourceGroup`"

#. type: Table
msgid "  The <<s-linstor-resource-groups, LINSTOR Resource Group (RG)>> to associate with this StorageClass. If not set, a new RG will be created for each new PVC.  [[s-kubernetes-storagepool]]"
msgstr "このStorageClassに関連付ける<<s-linstor-resource-groups, LINSTORリソースグループ（RG）>>。設定されていない場合、新しいPVCごとに新しいRGが作成されます。[[s-kubernetes-storagepool]]"

#. type: Title ====
#, no-wrap
#| msgid "storagePool"
msgid "`storagePool`"
msgstr "`storagePool`"

#. type: Table
msgid ""
"  `storagePool` is the name of the LINSTOR <<s-storage_pools,storage pool>> that will be used to provide storage to the newly-created volumes.  CAUTION: Only nodes configured with this same _storage pool_ with be considered for <<s-kubernetes-autoplace,auto-placement>>. Likewise, for _StorageClasses_ using <<s-kubernetes-nodelist,`nodeList`>> all nodes specified in that list must have this _storage pool_ configured on them.  [[s-kubernetes-disklessstoragepool]]"
msgstr "`storagePool` は、新しく作成されるボリュームにストレージを提供するために使用されるLINSTORの<<s-storage_pools,ストレージプール>>の名前です。 注意: この同じ _storage pool_ で構成されたノードのみが <<s-kubernetes-autoplace,自動配置>> の対象となります。同様に、`nodeList` を使用する _StorageClasses_ の場合、そのリストに指定されたすべてのノードにこの _storage pool_ が構成されている必要があります。[[s-kubernetes-disklessstoragepool]]"

#. type: Title ====
#, no-wrap
#| msgid "disklessStoragePool"
msgid "`disklessStoragePool`"
msgstr "`disklessStoragePool`"

#. type: Table
msgid "  `disklessStoragePool` is an optional parameter that only affects LINSTOR volumes that are assigned as \"diskless\" to _kubelets_, that is, as clients. If you have a custom diskless storage pool defined in LINSTOR, you will specify that here."
msgstr "`disklessStoragePool`は、\"ディスクレス\"として_linetets_に割り当てられたLINSTORボリュームにのみ影響するオプションパラメータです。LINSTORでカスタムディスクレスストレージプールが定義されている場合、ここで指定します。"

#. type: Title ====
#, no-wrap
#| msgid "layerList"
msgid "`layerList`"
msgstr "`layerList`"

#. type: Table
msgid "  A comma-separated list of layers to use for the created volumes. The available layers and their order are described towards the end of <<s-linstor-without-drbd, this section>>. Defaults to `drbd,storage` [[s-kubernetes-placementpolicy]]"
msgstr "作成されたボリュームに使用するレイヤーのカンマ区切りのリスト。使用可能なレイヤーとその順序は、<<s-linstor-without-drbd,このセクション>>の最後に記載されています。デフォルトは `drbd,storage` [[s-kubernetes-placementpolicy]]。"

#. type: Title ====
#, no-wrap
#| msgid "placementPolicy"
msgid "`placementPolicy`"
msgstr "`placementPolicy`"

#. type: Table
msgid ""
"  Select from one of the available volume schedulers: * `AutoPlaceTopology`, the default: Use topology information from Kubernetes together with user provided constraints (see <<s-kubernetes-replicasonsame>> and <<s-kubernetes-replicasondifferent>>).  * `AutoPlace` Use the LINSTOR auto-placement feature, influenced by <<s-kubernetes-replicasonsame>> and <<s-kubernetes-replicasondifferent>> * `FollowTopology`: Use CSI Topology information to place at least one "
"volume in each \"preferred\" zone. Only usable if CSI Topology is enabled.  * `Manual`: Use only the nodes listed in `nodeList` and `clientList`.  * `Balanced`: **EXPERIMENTAL** Place volumes across failure domains, using the least used storage pool on each selected node.  [[s-kubernetes-params-allow-remote-volume-access]]"
msgstr ""
"利用可能なボリュームスケジューラのいずれかを選択します:   * `AutoPlaceTopology`、デフォルト: Kubernetesからのトポロジ情報とユーザー定義の制約条件を使用します（<<s-kubernetes-replicasonsame>> および <<s-kubernetes-replicasondifferent>> を参照）。  * `AutoPlace` LINSTORの自動配置機能を使用し、<<s-kubernetes-replicasonsame>> および <<s-kubernetes-replicasondifferent>> の影響を受けます。  * `FollowTopology`: CSIトポロジ情報を使用して、各 \"preferred\" ゾーンに少なくと"
"も1つのボリュームを配置します。CSIトポロジが有効な場合のみ使用可能です。  * `Manual`: `nodeList` と `clientList`にリストされているノードのみを使用します。  * `Balanced`: **実験的** 障害ドメイン全体にわたってボリュームを配置し、各選択されたノード上で最も使用されていないストレージプールを使用します。 [[s-kubernetes-params-allow-remote-volume-access]]"

#. type: Title ====
#, no-wrap
#| msgid "allowRemoteVolumeAccess"
msgid "`allowRemoteVolumeAccess`"
msgstr "`allowRemoteVolumeAccess`"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Control on which nodes a volume is accessible. The value for this option can take two different forms:\n"
"\n"
"- A simple `\"true\"` or `\"false\"` allows access from all nodes, or only those nodes with\n"
"  diskful resources.\n"
"\n"
"- Advanced rules, which allow more granular rules on which nodes can access the volume.\n"
"+\n"
"The current implementation can grant access to the volume for nodes that share the same labels. For example, if you want\n"
"to allow access from all nodes in the same region and zone as a diskful resource, you could use:\n"
"+\n"
"[source,yaml]\n"
"----\n"
msgstr ""
"\n"
"ボリュームにアクセス可能なノードを制御します。このオプションの値には、2つの異なる形式があります。\n"
"\n"
"- シンプルな`\"true\"`または`\"false\"`は、すべてのノードからのアクセスを許可するか、ディスクリソースを持つノードのみを許可するかを決定します。\n"
"\n"
"- より細かいルールを許可し、どのノードがボリュームにアクセスできるかを制御できる高度なルール。\n"
"+\n"
"現在の実装では、同じラベルを共有するノードに対してボリュームへのアクセスを許可することができます。たとえば、ディスクフルリソースと同じ地域およびゾーンのすべてのノードからのアクセスを許可したい場合は、次のようにします:\n"
"+\n"
"[source,yaml]\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "parameters:\n"
#| "  linstor.csi.linbit.com/allowRemoteVolumeAccess: |\n"
#| "    - fromSame:\n"
#| "      - topology.kubernetes.io/region\n"
#| "      - topology.kubernetes.io/zone\n"
msgid ""
"parameters:\n"
"  linstor.csi.linbit.com/allowRemoteVolumeAccess: |\n"
"    - fromSame:\n"
"      - topology.kubernetes.io/region\n"
"      - topology.kubernetes.io/zone\n"
"----\n"
msgstr ""
"parameters:\n"
"  linstor.csi.linbit.com/allowRemoteVolumeAccess: |\n"
"    - fromSame:\n"
"      - topology.kubernetes.io/region\n"
"      - topology.kubernetes.io/zone\n"
"----\n"

#. type: Table
msgid "+ You can specify multiple rules. The rules are additive, a node only need to match one rule to be assignable.  [[s-kubernetes-encryption]]"
msgstr "+ 複数のルールを指定することができます。ルールは追加され、ノードは1つのルールに一致すれば割り当てられます。[[s-kubernetes-encryption]]"

#. type: Title ====
#, no-wrap
#| msgid "encryption"
msgid "`encryption`"
msgstr "`encryption`"

#. type: Table
msgid "  `encryption` is an optional parameter that determines whether to encrypt volumes. LINSTOR must be <<s-linstor-encrypted-volumes,configured for encryption>> for this to work properly.  [[s-kubernetes-nodelist]]"
msgstr "`encryption` は、ボリュームを暗号化するかどうかを決定するオプションパラメータです。 これが正常に機能するには、LINSTORはデプロイメントが<<s-linstor-encrypted-volumes,暗号化に設定>>されている必要があります。 [[s-kubernetes-nodelist]]"

#. type: Title ====
#, no-wrap
#| msgid "nodeList"
msgid "`nodeList`"
msgstr "`nodeList`"

#. type: Table
msgid ""
"  `nodeList` is a list of nodes for volumes to be assigned to. This will assign the volume to each node and it will be replicated among all of them. This can also be used to select a single node by hostname, but it's more flexible to use <<s-kubernetes-replicasonsame,replicasOnSame>> to select a single node.  IMPORTANT: If you use this option, you must not use <<s-kubernetes-autoplace,`autoPlace`>>.  TIP: This option determines on which LINSTOR nodes the "
"underlying storage for volumes will be provisioned and is orthogonal from which _kubelets_ these volumes will be accessible."
msgstr ""
"`nodeList`は、割り当てるボリュームのノードのリストです。これにより、ボリュームが各ノードに割り当てられ、それらの間でレプリケーションされます。ホスト名で単一ノードを選択するためにも使用できますが、単一ノードを選択するには<<s-kubernetes-replicasonsame,replicasOnSame>>を使用する方が柔軟です。重要：このオプションを使用する場合は、<<s-kubernetes-autoplace,`autoPlace`>>を使用しないでください。ヒント：このオプションは、ボリュームの基礎ストレージがどのLINSTORノードにプロビ"
"ジョニングされるかを決定し、これらのボリュームがアクセス可能になる_kubelets_は異なります。"

#. type: Title ====
#, no-wrap
#| msgid "clientList"
msgid "`clientList`"
msgstr "`clientList`"

#. type: Table
msgid "  `clientList` is a list of nodes for diskless volumes to be assigned to. Use in conjunction with <<s-kubernetes-nodelist>>.  [[s-kubernetes-replicasonsame]]"
msgstr "`clientList` はディスクレスボリュームに割り当てるためのノードのリストです。 <<s-kubernetes-nodelist>> と併用して使用します。[[s-kubernetes-replicasonsame]]"

#. type: Title ====
#, no-wrap
#| msgid "replicasOnSame"
msgid "`replicasOnSame`"
msgstr "`replicasOnSame`"

#. type: Table
msgid ""
"  // These should link to the linstor documentation about node properties, but those // do not exist at the time of this commit.  `replicasOnSame` is a list of `key` or `key=value` items used as auto-placement selection labels when <<s-kubernetes-autoplace,`autoPlace`>> is used to determine where to provision storage. These labels correspond to LINSTOR node properties.  NOTE: The operator periodically synchronizes all labels from Kubernetes Nodes, so you can use "
"them as keys for scheduling constraints.  Let's explore this behavior with examples assuming a LINSTOR cluster such that `node-a` is configured with the following auxiliary property `zone=z1` and `role=backups`, while `node-b` is configured with only `zone=z1`.  If we configure a _StorageClass_ with `autoPlace: \"1\"` and `replicasOnSame: \"zone=z1 role=backups\"`, then all volumes created from that _StorageClass_ will be provisioned on `node-a`, since that is the "
"only node with all of the correct key=value pairs in the LINSTOR cluster. This is the most flexible way to select a single node for provisioning.  IMPORTANT: This guide assumes LINSTOR CSI version 0.10.0 or newer. All properties referenced in `replicasOnSame` and `replicasOnDifferent` are interpreted as auxiliary properties. If you are using an older version of LINSTOR CSI, you need to add the `Aux/` prefix to all property names. So `replicasOnSame: \"zone=z1\"` "
"would be `replicasOnSame: \"Aux/zone=z1\"` Using `Aux/` manually will continue to work on newer LINSTOR CSI versions.  If we configure a _StorageClass_ with `autoPlace: \"1\"` and `replicasOnSame: \"zone=z1\"`, then volumes will be provisioned on either `node-a` or `node-b` as they both have the `zone=z1` aux prop.  If we configure a _StorageClass_ with `autoPlace: \"2\"` and `replicasOnSame: \"zone=z1 role=backups\"`, then provisioning will fail, as there are not "
"two or more nodes that have the appropriate auxiliary properties.  If we configure a _StorageClass_ with `autoPlace: \"2\"` and `replicasOnSame: \"zone=z1\"`, then volumes will be provisioned on both `node-a` and `node-b` as they both have the `zone=z1` aux prop.  You can also use a property key without providing a value to ensure all replicas are placed on nodes with the same property value, with caring about the particular value. Assuming there are 4 nodes, "
"`node-a1` and `node-a2` are configured with `zone=a`. `node-b1` and `node-b2` are configured with `zone=b`. Using `autoPlace: \"2\"` and `replicasOnSame: \"zone\"` will place on either `node-a1` and `node-a2` OR on `node-b1` and `node-b2`.  [[s-kubernetes-replicasondifferent]]"
msgstr ""
"  // These should link to the linstor documentation about node properties, but those // do not exist at the time of this commit. `replicasOnSame` は、ストレージをどこにプロビジョニングするかを決定する際に `autoPlace` が使用されるときに自動配置選択ラベルとして使用される、`key` または `key=value` アイテムのリストです。 これらのラベルは、LINSTOR ノードプロパティに対応しています。\n"
"注意: オペレーターは定期的にKubernetesノードからすべてのラベルを同期するため、スケジューリング制約のキーとして使用することができます。`node-a`が`zone=z1`および`role=backups`の補助プロパティを持つように構成されているLINSTORクラスタを想定した例を使って、この動作を探ってみましょう。一方、`node-b`は`zone=z1`のみが構成されています。もし`autoPlace: \"1\"`と`replicasOnSame: \"zone=z1 role=backups\"`を持つ_StorageClass_を構成した場合、それから作成されるすべてのボリューム"
"は、LINSTORクラスタ内で正しいキーと値のペアをすべて持っている唯一のノードである`node-a`にプロビジョニングされます。これは、プロビジョニングのための単一のノードを選択する最も柔軟な方法です。\n"
"重要：このガイドは LINSTOR CSI バージョン 0.10.0 以上を前提としています。`replicasOnSame` と `replicasOnDifferent` に言及されたすべてのプロパティは補助プロパティとして解釈されます。LINSTOR CSI の古いバージョンを使用している場合は、すべてのプロパティ名に `Aux/` 接頭辞を追加する必要があります。したがって、`replicasOnSame: \"zone=z1\"` は `replicasOnSame: \"Aux/zone=z1\"` となります。\n"
"`Aux/`を手動で使用すると、新しいLINSTOR CSIバージョンでも引き続き機能します。`autoPlace: \"1\"`および`replicasOnSame: \"zone=z1\"`を持つ_StorageClass_を構成すると、`node-a`または`node-b`のどちらかにボリュームがプロビジョニングされます。なぜなら、両方のノードが`zone=z1`のauxプロパティを持っているからです。`autoPlace: \"2\"`および`replicasOnSame: \"zone=z1 role=backups\"`を持つ_StorageClass_を構成すると、2つ以上の適切な補助プロパティを持つノードが存在しないため、プロ"
"ビジョニングが失敗します。`autoPlace: \"2\"`および`replicasOnSame: \"zone=z1\"`を持つ_StorageClass_を構成すると、`node-a`と`node-b`の両方にボリュームがプロビジョニングされます。なぜなら、両方のノードが`zone=z1`のauxプロパティを持っているからです。\n"
"プロパティキーを値を提供せずに使用することもできます。これにより、すべてのレプリカが同じプロパティ値を持つノードに配置されるようになります。特定の値に関心を持たず、ということです。4つのノードがあると仮定すると、`node-a1`と`node-a2`は`zone=a`で構成され、`node-b1`と`node-b2`は`zone=b`で構成されています。`autoPlace: \"2\"`と`replicasOnSame: \"zone\"`を使用すると、`node-a1`と`node-a2`のいずれかに配置されるか、`node-b1`と`node-b2`のいずれかに配置されます。"

#. type: Title ====
#, no-wrap
#| msgid "replicasOnDifferent"
msgid "`replicasOnDifferent`"
msgstr "`replicasOnDifferent`"

#. type: Table
msgid ""
"  `replicasOnDifferent` takes a list of properties to consider, same as <<s-kubernetes-replicasonsame,replicasOnSame>>.  There are two modes of using `replicasOnDifferent`: * Preventing volume placement on specific nodes: + If a value is given for the property, the nodes which have that property-value pair assigned will be considered last.  + Example: `replicasOnDifferent: \"no-csi-volumes=true\"` will place no volume on any node with property `no-csi-"
"volumes=true` unless there are not enough other nodes to fulfill the `autoPlace` setting.  * Distribute volumes across nodes with different values for the same key: + If no property value is given, LINSTOR will place the volumes across nodes with different values for that property if possible.  + Example: Assuming there are 4 nodes, `node-a1` and `node-a2` are configured with `zone=a`. `node-b1` and `node-b2` are configured with `zone=b`. Using a _StorageClass_ "
"with `autoPlace: \"2\"` and `replicasOnDifferent: \"zone\"`, LINSTOR will create one replica on either `node-a1` or `node-a2` _and_ one replica on either `node-b1` or `node-b2`."
msgstr ""
"`replicasOnDifferent`は考慮するプロパティのリストを受け取り、`replicasOnSame`と同じです。`replicasOnDifferent`を使用する方法には2つのモードがあります。* 特定のノードにボリューム配置を防止する：+ プロパティに値が与えられた場合、そのプロパティ-値ペアが割り当てられているノードは最後に考慮されます。+ 例：`replicasOnDifferent: \"no-csi-volumes=true\"`は、`no-csi-volumes=true`プロパティを持つノードにはボリュームを配置しない（`autoPlace`設定を満たす他のノードが不足してい"
"ない限り）。\n"
"* 同じキーの異なる値を持つノードにボリュームを分散配置する: + プロパティの値が指定されていない場合、LINSTORは可能であればそのプロパティの値が異なるノードにボリュームを配置します。 + 例: 4つのノードがあると仮定し、`node-a1`と`node-a2`が`zone=a`で構成され、`node-b1`と`node-b2`が`zone=b`で構成されているとします。`autoPlace: \"2\"`および`replicasOnDifferent: \"zone\"`を持つ _StorageClass_ を使用すると、LINSTORは`node-a1`または`node-a2`のいずれかに1つのレプリカを作成"
"し、そして`node-b1`または`node-b2`のいずれかに1つのレプリカを作成します。"

#. type: Title ====
#, no-wrap
#| msgid "disklessOnRemaining"
msgid "`disklessOnRemaining`"
msgstr "`disklessOnRemaining`"

#. type: Table
msgid "  Create a diskless resource on _all_ nodes that were not assigned a diskful resource."
msgstr "ディスクフルリソースが割り当てられていないすべてのノードにディスクレスリソースを作成してください。"

#. type: Title ====
#, no-wrap
#| msgid "doNotPlaceWithRegex"
msgid "`doNotPlaceWithRegex`"
msgstr "`doNotPlaceWithRegex`"

#. type: Table
msgid "  Do not place the resource on a node which has a resource with a name matching the regular expression.  [[s-kubernetes-fsops]]"
msgstr "正規表現に一致する名前を持つリソースがあるノードにリソースを配置しないでください。 [[s-kubernetes-fsops]]"

#. type: Title ====
#, no-wrap
#| msgid "fsOpts"
msgid "`fsOpts`"
msgstr "`fsOpts`"

#. type: Table
msgid "`fsOpts` is an optional parameter that passes options to the volume's file system at creation time.  IMPORTANT: These values are specific to your chosen <<s-kubernetes-file-system, file system>>.  [[s-kubernetes-mountops]]"
msgstr "`fsOpts`は、作成時にボリュームのファイルシステムにオプションを渡すオプションパラメータです。 IMPORTANT: これらの値は選択したファイルシステムに固有です。[[s-kubernetes-mountops]]"

#. type: Title ====
#, no-wrap
msgid "`mountOpts`"
msgstr "`mountOpts`"

#. type: Table
msgid "`mountOpts` is an optional parameter that passes options to the volume's file system at mount time."
msgstr "`mountOpts`は、マウント時にボリュームのファイルシステムにオプションを渡すオプションパラメータです。"

#. type: Title ====
#, no-wrap
#| msgid "postMountXfsOpts"
msgid "`postMountXfsOpts`"
msgstr "`postMountXfsOpts`"

#. type: Table
msgid "  Extra arguments to pass to `xfs_io`, which gets called before right before first use of the volume.  [[s-kubernetes-storage-class-properties]]"
msgstr "`xfs_io`に渡す追加の引数は、ボリュームの最初の使用の直前に呼び出されます。[[s-kubernetes-storage-class-properties]]"

#. type: Title ====
#, no-wrap
#| msgid "property.linstor.csi.linbit.com/*"
msgid "`property.linstor.csi.linbit.com/*`"
msgstr "`property.linstor.csi.linbit.com/*`"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Parameters starting with `property.linstor.csi.linbit.com/` are translated to LINSTOR properties that are set on the\n"
"<<s-linstor-resource-groups,Resource Group>> associated with the StorageClass.\n"
"\n"
"For example, to set `DrbdOptions/auto-quorum` to `disabled`, use:\n"
"\n"
"----\n"
msgstr ""
"\n"
"`property.linstor.csi.linbit.com/`から始まるパラメータは、StorageClassに関連する<<s-linstor-resource-groups,リソースグループ>>に設定されたLINSTORプロパティに変換されます。\n"
"\n"
"たとえば、`DrbdOptions/auto-quorum` を `disabled` に設定する場合は、次のようにします:\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"property.linstor.csi.linbit.com/DrbdOptions/auto-quorum: disabled\n"
"----\n"
msgstr ""
"property.linstor.csi.linbit.com/DrbdOptions/auto-quorum: disabled\n"
"----\n"

#. type: Table
msgid "  The full list of options is available https://app.swaggerhub.com/apis-docs/Linstor/Linstor/1.7.0#/developers/resourceDefinitionModify[here]"
msgstr "フルなオプションの一覧は[こちら](https://app.swaggerhub.com/apis-docs/Linstor/Linstor/1.7.0#/developers/resourceDefinitionModify)で利用可能です。"

#. type: Title ====
#, no-wrap
#| msgid "DrbdOptions/*: <x>"
msgid "`DrbdOptions/*: <x>`"
msgstr "`DrbdOptions/*: <x>`"

#. type: Table
msgid "  NOTE: This option is deprecated, use the more general <<s-kubernetes-storage-class-properties, `property.linstor.csi.linbit.com/*`>> form.  Advanced DRBD options to pass to LINSTOR. For example, to change the replication protocol, use `DrbdOptions/Net/protocol: \"A\"`.  [[s-kubernetes-snapshots]]"
msgstr "注意: このオプションは非推奨となりました。より一般的な`property.linstor.csi.linbit.com/*`形式を使用してください。LINSTORに渡すための高度なDRBDオプション。たとえば、レプリケーションプロトコルを変更するには、`DrbdOptions/Net/protocol: \"A\"`を使用します。[[s-kubernetes-snapshots]]"

#. type: Title ===
#, no-wrap
msgid "Snapshots"
msgstr "スナップショット"

#. type: Table
msgid ""
"  Snapshots create a copy of the volume content at a particular point in time. This copy remains untouched when you make modifications to the volume content. This, for example, enables you to create backups of your data before performing modifications or deletions on your data.  Because a backup is useless unless you have a way to restore it, this section describes how to create a snapshot, and how to restore it, for example, in the case of accidental deletion of "
"your data.  The next subsection contains instructions around snapshots within Operator v2 deployments. If you have deployed LINBIT SDS in Kubernetes by using Operator v1, skip ahead to the <<s-kubernetes-add-snaphot-support-v1>> subsection.  [[s-kubernetes-snapshots]]"
msgstr ""
"スナップショットは特定の時点でボリュームの内容のコピーを作成します。このコピーはボリュームの内容を修正しても触れられずに残ります。例えば、データを変更や削除する前にバックアップを作成することができます。バックアップは復元方法がなければ役に立ちませんので、このセクションではスナップショットの作成方法と復元方法について説明します。例えば、データを誤って削除した場合などに備えてです。次のサブセクションでは、Operator v2デプロイメント内のスナップショットに関する手順が記載さ"
"れています。Operator v1を使用してKubernetesでLINBIT SDSをデプロイした場合は、<<s-kubernetes-add-snaphot-support-v1>>のサブセクションに進んでください。"

#. type: Title ====
#, no-wrap
msgid "Working With Snapshots"
msgstr "スナップショットとの使用"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Before you can add snapshot support within a LINBIT SDS deployment, you need to meet the following environment prerequisites:\n"
"\n"
"- Your cluster has a storage pool supporting snapshots. LINSTOR supports snapshots for `LVM_THIN`, `FILE_THIN`, `ZFS` and `ZFS_THIN` pools.\n"
"\n"
"- You have a `StorageClass`, `PersistentVolumeClaim`, and `Deployment` that uses a storage pool\n"
"  that supports snapshots.\n"
"\n"
"- Your cluster has a CSI snapshotter (link:https://github.com/kubernetes-csi/external-snapshotter/[`snapshot-controller`]) deployed. To verify if it is already deployed, you can enter the following command:\n"
"+\n"
"----\n"
msgstr ""
"\n"
"LINBIT SDSの<<デプロイメント,デプロイメント>>内でスナップショットサポートを追加する前に、次の環境前提条件を満たす必要があります。\n"
"\n"
"- クラスターにはスナップショットをサポートするストレージプールがあります。LINSTORは、`LVM_THIN`、`FILE_THIN`、`ZFS`、`ZFS_THIN` プールに対してスナップショットをサポートしています。\n"
"\n"
"- `StorageClass`、 `PersistentVolumeClaim`、およびサポートしているスナップショットを使用する `デプロイメント` を持っています。\n"
"\n"
"- お使いのクラスターにはCSIスナップショッター（リンク: https://github.com/kubernetes-csi/external-snapshotter/ [`snapshot-controller`]）がデプロイされています。すでにデプロイされているかどうかを確認するには、次のコマンドを入力してください：\n"
"+\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"$ kubectl api-resources --api-group=snapshot.storage.k8s.io -oname\n"
"----\n"
msgstr ""
"$ kubectl api-resources --api-group=snapshot.storage.k8s.io -oname\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"+\n"
"Output should be similar to the following if a snapshot controller is already deployed:\n"
"+\n"
"----\n"
msgstr ""
"+\n"
"スナップショットコントローラーが既にデプロイされている場合、出力は以下のようになります：\n"
"+\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"volumesnapshotclasses.snapshot.storage.k8s.io\n"
"volumesnapshotcontents.snapshot.storage.k8s.io\n"
"volumesnapshots.snapshot.storage.k8s.io\n"
"----\n"
msgstr ""
"volumesnapshotclasses.snapshot.storage.k8s.io\n"
"volumesnapshotcontents.snapshot.storage.k8s.io\n"
"volumesnapshots.snapshot.storage.k8s.io\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"+\n"
"If output from the command is empty, you can deploy a snapshot controller by entering the following commands:\n"
"+\n"
"[%autofit]\n"
"----\n"
msgstr ""
"+\n"
"もしコマンドからの出力が空の場合は、以下のコマンドを入力してスナップショットコントローラをデプロイできます:\n"
"+\n"
"[%autofit]\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"$ kubectl apply -k https://github.com/kubernetes-csi/external-snapshotter//client/config/crd\n"
"$ kubectl apply -k https://github.com/kubernetes-csi/external-snapshotter//deploy/kubernetes/snapshot-controller\n"
"----\n"
msgstr ""
"$ kubectl apply -k https://github.com/kubernetes-csi/external-snapshotter//client/config/crd\n"
"$ kubectl apply -k https://github.com/kubernetes-csi/external-snapshotter//deploy/kubernetes/snapshot-controller\n"
"----\n"

#. type: Table
msgid "  [[s-kubernetes-snapshot-creating]]"
msgstr "  [[s-kubernetes-snapshot-creating]]"

#. type: Title =====
#, no-wrap
msgid "Creating a Snapshot"
msgstr "スナップショットの作成"

#. type: Table
#, no-wrap
msgid ""
"\n"
"To create a volume snapshot, you first need to create a volume snapshot class (link:https://kubernetes.io/docs/concepts/storage/volume-snapshot-classes/[`VolumeSnapshotClass`]). This volume snapshot class will specify the `linstor.csi.linbit.com` provisioner, and sets the clean-up policy for the snapshots to `Delete`. This means that deleting the Kubernetes resources will also delete the snapshots in LINSTOR.\n"
"\n"
"You can create a volume snapshot class by entering the following command:\n"
"\n"
"----\n"
msgstr ""
"\n"
"ボリュームのスナップショットを作成するには、まずボリュームのスナップショットクラスを作成する必要があります（リンク:https://kubernetes.io/docs/concepts/storage/volume-snapshot-classes/[`VolumeSnapshotClass`]）。このボリュームのスナップショットクラスは、`linstor.csi.linbit.com` プロビジョナーを指定し、スナップショットのクリーンアップポリシーを `Delete` に設定します。つまり、Kubernetesのリソースを削除すると、LINSTOR内のスナップショットも削除されます。\n"
"\n"
"以下のコマンドを入力することでボリュームのスナップショットクラスを作成できます: \n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "apiVersion: snapshot.storage.k8s.io/v1\n"
#| "kind: VolumeSnapshotClass\n"
#| "metadata:\n"
#| "  name: my-first-linstor-snapshot-class\n"
#| "driver: linstor.csi.linbit.com\n"
#| "deletionPolicy: Delete\n"
msgid ""
"$ kubectl apply -f - <<EOF\n"
"apiVersion: snapshot.storage.k8s.io/v1\n"
"kind: VolumeSnapshotClass\n"
"metadata:\n"
"  name: linbit-sds-snapshots\n"
"driver: linstor.csi.linbit.com\n"
"deletionPolicy: Delete\n"
"EOF\n"
"----\n"
msgstr ""
"$ kubectl apply -f - <<EOF\n"
"apiVersion: snapshot.storage.k8s.io/v1\n"
"kind: VolumeSnapshotClass\n"
"metadata:\n"
"  name: linbit-sds-snapshots\n"
"driver: linstor.csi.linbit.com\n"
"deletionPolicy: Delete\n"
"EOF\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"To create a snapshot, you create a link:https://kubernetes.io/docs/concepts/storage/volume-snapshots/#volumesnapshots[`VolumeSnapshot`] resource. The `VolumeSnapshot` resource needs to reference a snapshot-compatible `PersistentVolumeClaim` resource, and the `VolumeSnapshotClass` that you just created. For example, you could create a snapshot (named `data-volume-snapshot-1`) of a PVC named `data-volume` by entering the following command:\n"
"\n"
"----\n"
msgstr ""
"\n"
"スナップショットを作成するには、`VolumeSnapshot`リソースを作成します。`VolumeSnapshot`リソースは、スナップショット互換の`PersistentVolumeClaim`リソースと、作成した`VolumeSnapshotClass`を参照する必要があります。たとえば、次のコマンドを入力して、`data-volume`というPVCのスナップショット（`data-volume-snapshot-1`という名前）を作成できます:\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid ""
#| "apiVersion: snapshot.storage.k8s.io/v1\n"
#| "kind: VolumeSnapshot\n"
#| "metadata:\n"
#| "  name: my-first-linstor-snapshot\n"
#| "spec:\n"
#| "  volumeSnapshotClassName: my-first-linstor-snapshot-class\n"
#| "  source:\n"
#| "    persistentVolumeClaimName: my-first-linstor-volume\n"
msgid ""
"$ kubectl apply -f - <<EOF\n"
"apiVersion: snapshot.storage.k8s.io/v1\n"
"kind: VolumeSnapshot\n"
"metadata:\n"
"  name: data-volume-snapshot-1\n"
"spec:\n"
"  volumeSnapshotClassName: linbit-sds-snapshots\n"
"  source:\n"
"    persistentVolumeClaimName: data-volume\n"
"EOF\n"
"----\n"
msgstr ""
"$ kubectl apply -f - <<EOF\n"
"apiVersion: snapshot.storage.k8s.io/v1\n"
"kind: VolumeSnapshot\n"
"metadata:\n"
"  name: data-volume-snapshot-1\n"
"spec:\n"
"  volumeSnapshotClassName: linbit-sds-snapshots\n"
"  source:\n"
"    persistentVolumeClaimName: data-volume\n"
"EOF\n"
"----\n"

#. type: Table
msgid "  [[s-kubernetes-snapshot-verifying-creation]]"
msgstr "  [[s-kubernetes-snapshot-verifying-creation]]"

#. type: Title =====
#, no-wrap
msgid "Verifying Snapshot Creation"
msgstr "スナップショットの作成を検証中"

#. type: Table
#, no-wrap
msgid ""
"\n"
"You can verify the creation of a snapshot by entering the following commands:\n"
"\n"
"----\n"
msgstr ""
"\n"
"以下のコマンドを入力することで、スナップショットの作成を確認できます:\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"$ kubectl wait volumesnapshot --for=jsonpath='{.status.readyToUse}'=true data-volume-snapshot-1\n"
"volumesnapshot.snapshot.storage.k8s.io/data-volume-snapshot-1 condition met\n"
"$ kubectl get volumesnapshot data-volume-snapshot-1\n"
"----\n"
msgstr ""
"$ kubectl wait volumesnapshot --for=jsonpath='{.status.readyToUse}'=true data-volume-snapshot-1\n"
"volumesnapshot.snapshot.storage.k8s.io/data-volume-snapshot-1 condition met\n"
"$ kubectl get volumesnapshot data-volume-snapshot-1\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Output should show a table of information about the volume snapshot resource, similar to the following:\n"
"\n"
"[%autofit]\n"
"----\n"
msgstr ""
"\n"
"出力は、以下のような形式でボリュームスナップショットリソースに関する情報の表を表示する必要があります。\n"
"\n"
"[%autofit]\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"NAME                     READYTOUSE   SOURCEPVC     SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS\n"
"data-volume-snapshot-1   true         data-volume                           1Gi           linbit-sds-snapshots\n"
"----\n"
msgstr ""
"NAME                     READYTOUSE   SOURCEPVC     SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS\n"
"data-volume-snapshot-1   true         data-volume                           1Gi           linbit-sds-snapshots\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"You can further verify the snapshot in LINSTOR, by entering the following command:\n"
"\n"
"----\n"
msgstr ""
"\n"
"次のコマンドを入力することで、LINSTORにおけるスナップショットをさらに検証できます: \n"
"\n"
"----\n"

#. type: Table
#, no-wrap
#| msgid "kubectl exec deployment/linstor-op-cs-controller -- linstor storage-pool list\n"
msgid ""
"$ kubectl -n linbit-sds exec deploy/linstor-controller -- linstor snapshot list\n"
"----\n"
msgstr ""
"$ kubectl -n linbit-sds exec deploy/linstor-controller -- linstor snapshot list\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"\n"
"Output should show a table similar to the following:\n"
"\n"
"----\n"
msgstr ""
"\n"
"出力は以下のようなテーブルを表示する必要があります:\n"
"\n"
"----\n"

#. type: Table
#, no-wrap
msgid ""
"+-----------------------------------------------------------------------------------------+\n"
"| ResourceName | SnapshotName   | NodeNames | Volumes  | CreatedOn           | State      |\n"
msgstr ""
"+-----------------------------------------------------------------------------------------+\n"
"| ResourceName | SnapshotName   | NodeNames | Volumes  | CreatedOn           | State      |\n"

#. type: Plain text
#, no-wrap
msgid ""
"| pvc-[...]    | snapshot-[...] | kube-0    | 0: 1 GiB | 2023-02-13 15:36:18 | Successful |\n"
"+-----------------------------------------------------------------------------------------+\n"
msgstr ""
"| pvc-[...]    | snapshot-[...] | kube-0    | 0: 1 GiB | 2023-02-13 15:36:18 | Successful |\n"
"+-----------------------------------------------------------------------------------------+\n"

#. type: delimited block -
#, no-wrap
msgid "[[s-kubernetes-snapshots-restoring]]\n"
msgstr "[[s-kubernetes-snapshots-restoring]]\n"

#. type: Title =====
#, no-wrap
msgid "Restoring a Snapshot"
msgstr "スナップショットの復元"

#. type: delimited block -
msgid "To restore a snapshot, you will need to create a new PVC to recover the volume snapshot to. You will replace the existing PVC, named `data-volume` in this example, with a new version based on the snapshot."
msgstr "スナップショットを復元するには、復元するボリュームスナップショット用の新しい PVC を作成する必要があります。この例では、既存の `data-volume` という名前の PVC をスナップショットに基づいた新しいバージョンに置き換えます。"

#. type: delimited block -
msgid "First, stop the deployment that uses the `data-volume` PVC. In this example, the deployment is named `volume-logger`."
msgstr "最初に、`data-volume` PVC を使用しているデプロイメントを停止してください。この例では、デプロイメントの名前は `volume-logger` です。"

#. type: Plain text
msgid "$ kubectl scale deploy/volume-logger --replicas=0 deployment.apps \"volume-logger\" deleted $ kubectl rollout status deploy/volume-logger deployment \"volume-logger\" successfully rolled out"
msgstr "$ kubectl scale deploy/volume-logger --replicas=0 deployment.apps \"volume-logger\" deleted $ kubectl rollout status deploy/volume-logger deployment \"volume-logger\" successfully rolled out"

#. type: delimited block -
#, no-wrap
msgid "Next, remove the PVC. You still have the snapshot resource, so this is a safe operation.\n"
msgstr "次に、PVCを削除します。スナップショットリソースがまだ残っているので、これは安全な操作です。\n"

#. type: Plain text
msgid "$ kubectl delete pvc/data-volume persistentvolumeclaim \"data-volume\" deleted"
msgstr "$ kubectl delete pvc/data-volume persistentvolumeclaim \"data-volume\" deleted"

#. type: delimited block -
#, no-wrap
msgid "Next, create a new PVC by referencing a previously created snapshot. This will create a volume which uses the data from the referenced snapshot.\n"
msgstr "次に、以前に作成したスナップショットを参照して新しいPVCを作成します。これにより、参照されたスナップショットのデータを使用するボリュームが作成されます。\n"

#. type: Plain text
#, no-wrap
#| msgid ""
#| "apiVersion: v1\n"
#| "kind: PersistentVolumeClaim\n"
#| "metadata:\n"
#| "  name: my-first-linstor-volume-from-snapshot\n"
#| "spec:\n"
#| "  storageClassName: linstor-basic-storage-class\n"
#| "  dataSource:\n"
#| "    name: my-first-linstor-snapshot\n"
#| "    kind: VolumeSnapshot\n"
#| "    apiGroup: snapshot.storage.k8s.io\n"
#| "  accessModes:\n"
#| "    - ReadWriteOnce\n"
#| "  resources:\n"
#| "    requests:\n"
#| "      storage: 500Mi\n"
msgid ""
"kubectl apply -f - <<EOF\n"
"apiVersion: v1\n"
"kind: PersistentVolumeClaim\n"
"metadata:\n"
"  name: data-volume\n"
"spec:\n"
"  storageClassName: linbit-sds-storage\n"
"  resources:\n"
"    requests:\n"
"      storage: 1Gi\n"
"  dataSource:\n"
"    apiGroup: snapshot.storage.k8s.io\n"
"    kind: VolumeSnapshot\n"
"    name: data-volume-snapshot-1\n"
"  accessModes:\n"
msgstr ""
"kubectl apply -f - <<EOF\n"
"apiVersion: v1\n"
"kind: PersistentVolumeClaim\n"
"metadata:\n"
"  name: data-volume\n"
"spec:\n"
"  storageClassName: linbit-sds-storage\n"
"  resources:\n"
"    requests:\n"
"      storage: 1Gi\n"
"  dataSource:\n"
"    apiGroup: snapshot.storage.k8s.io\n"
"    kind: VolumeSnapshot\n"
"    name: data-volume-snapshot-1\n"
"  accessModes:\n"

#. type: Plain text
#, no-wrap
msgid ""
"ReadWriteOnce\n"
"EOF\n"
msgstr ""
"ReadWriteOnce\n"
"EOF\n"

#. type: delimited block -
#, no-wrap
msgid "Because you named the new volume, `data-volume`, the same as the previous volume, you can just scale up the `Deployment` again, and the new pod will start using the restored volume.\n"
msgstr "`data-volume` という新しいボリュームを前のボリュームと同じ名前にしたため、単に `Deployment` を再度スケールアップすれば、新しいポッドが復元されたボリュームを使用し始めます。\n"

#. type: Plain text
msgid "$ kubectl scale deploy/volume-logger --replicas=1 deployment.apps/volume-logger scaled"
msgstr "$ kubectl scale deploy/volume-logger --replicas=1 deployment.apps/volume-logger scaled"

#. type: Title ====
#, no-wrap
msgid "Storing Snapshots on S3 Storage"
msgstr "スナップショットのS3ストレージへの保存"

#. type: delimited block -
msgid "LINSTOR can store snapshots on S3 compatible storage for disaster recovery. This is integrated in Kubernetes using a special VolumeSnapshotClass:"
msgstr "LINSTORは、災害復旧のためにS3互換ストレージ上にスナップショットを保存することができます。これは、特別なVolumeSnapshotClassを使用してKubernetesに統合されています:"

#. type: delimited block -
msgid "[source,yaml]"
msgstr "[source,yaml]"

#. type: Plain text
#, no-wrap
msgid ""
"kind: VolumeSnapshotClass\n"
"apiVersion: snapshot.storage.k8s.io/v1\n"
"metadata:\n"
"  name: linstor-csi-snapshot-class-s3\n"
"driver: linstor.csi.linbit.com\n"
"deletionPolicy: Retain\n"
"parameters:\n"
"  snap.linstor.csi.linbit.com/type: S3\n"
"  snap.linstor.csi.linbit.com/remote-name: backup-remote\n"
"  snap.linstor.csi.linbit.com/allow-incremental: \"false\"\n"
"  snap.linstor.csi.linbit.com/s3-bucket: snapshot-bucket\n"
"  snap.linstor.csi.linbit.com/s3-endpoint: s3.us-west-1.amazonaws.com\n"
"  snap.linstor.csi.linbit.com/s3-signing-region: us-west-1\n"
"  snap.linstor.csi.linbit.com/s3-use-path-style: \"false\"\n"
"  # Refer here to the secret that holds access and secret key for the S3 endpoint.\n"
"  # See below for an example.\n"
"  csi.storage.k8s.io/snapshotter-secret-name: linstor-csi-s3-access\n"
"  csi.storage.k8s.io/snapshotter-secret-namespace: storage\n"
"---\n"
msgstr ""
"kind: VolumeSnapshotClass\n"
"apiVersion: snapshot.storage.k8s.io/v1\n"
"metadata:\n"
"  name: linstor-csi-snapshot-class-s3\n"
"driver: linstor.csi.linbit.com\n"
"deletionPolicy: Retain\n"
"parameters:\n"
"  snap.linstor.csi.linbit.com/type: S3\n"
"  snap.linstor.csi.linbit.com/remote-name: backup-remote\n"
"  snap.linstor.csi.linbit.com/allow-incremental: \"false\"\n"
"  snap.linstor.csi.linbit.com/s3-bucket: snapshot-bucket\n"
"  snap.linstor.csi.linbit.com/s3-endpoint: s3.us-west-1.amazonaws.com\n"
"  snap.linstor.csi.linbit.com/s3-signing-region: us-west-1\n"
"  snap.linstor.csi.linbit.com/s3-use-path-style: \"false\"\n"
"  # Refer here to the secret that holds access and secret key for the S3 endpoint.\n"
"  # See below for an example.\n"
"  csi.storage.k8s.io/snapshotter-secret-name: linstor-csi-s3-access\n"
"  csi.storage.k8s.io/snapshotter-secret-namespace: storage\n"
"---\n"

#. type: Plain text
#, no-wrap
msgid ""
"kind: Secret\n"
"apiVersion: v1\n"
"metadata:\n"
"  name: linstor-csi-s3-access\n"
"  namespace: storage\n"
"immutable: true\n"
"type: linstor.csi.linbit.com/s3-credentials.v1\n"
"stringData:\n"
"  access-key: access-key\n"
"  secret-key: secret-key\n"
msgstr ""
"kind: Secret\n"
"apiVersion: v1\n"
"metadata:\n"
"  name: linstor-csi-s3-access\n"
"  namespace: storage\n"
"immutable: true\n"
"type: linstor.csi.linbit.com/s3-credentials.v1\n"
"stringData:\n"
"  access-key: access-key\n"
"  secret-key: secret-key\n"

#. type: delimited block -
#, no-wrap
msgid ""
"Refer to the instructions in the <<s-linstor-snapshots-shipping>> section for the exact meaning of the\n"
"`snap.linstor.csi.linbit.com/` parameters. The credentials used to log in are stored in a separate secret, as show in\n"
"the example above.\n"
msgstr "`snap.linstor.csi.linbit.com/` パラメータの正確な意味については、<<s-linstor-snapshots-shipping>> セクションの手順書を参照してください。ログインに使用される資格情報は、上記の例に示されているように、別個のシークレットに保存されています。\n"

#. type: delimited block -
#, no-wrap
#| msgid "Referencing the above storage class when creating snapshots causes the snapshots to be automatically uploaded to the configured S3 storage."
msgid ""
"Referencing the above storage class when creating snapshots causes the snapshots to be automatically uploaded to the\n"
"configured S3 storage.\n"
msgstr "上記のストレージクラスを参照してスナップショットを作成すると、スナップショットが自動的に構成されたS3ストレージにアップロードされます。\n"

#. type: Title =====
#, no-wrap
msgid "Restoring From Remote Snapshots"
msgstr "リモートスナップショットからの復元"

#. type: delimited block -
msgid "Restoring from remote snapshots is an important step in disaster recovery. A snapshot needs to be registered with Kubernetes before it can be used to restore."
msgstr "リモートスナップショットからの復元は、ディザスターリカバリーの重要なステップです。スナップショットは、リストアに使用する前にKubernetesに登録される必要があります。"

#. type: delimited block -
msgid "If the snapshot that should be restored is part of a backup to S3, the LINSTOR \"remote\" needs to be configured first."
msgstr "復元すべきスナップショットがS3へのバックアップの一部である場合、LINSTOR \"remote\" を最初に設定します。"

#. type: Plain text
#, no-wrap
msgid ""
"linstor remote create s3 backup-remote s3.us-west-1.amazonaws.com \\\n"
"  snapshot-bucket us-west-1 access-key secret-key\n"
"linstor backup list backup-remote\n"
msgstr ""
"linstor remote create s3 backup-remote s3.us-west-1.amazonaws.com \\\n"
"  snapshot-bucket us-west-1 access-key secret-key\n"
"linstor backup list backup-remote\n"

#. type: delimited block -
#, no-wrap
msgid "The snapshot you want to register needs to be one of the listed snapshots.\n"
msgstr "登録したいスナップショットは、リストされているスナップショットの1つである必要があります。\n"

#. type: delimited block -
#, no-wrap
msgid ""
"To register the snapshot with Kubernetes, you need to create two resources, one VolumeSnapshotContent referencing the\n"
"ID of the snapshot and one VolumeSnapshot, referencing the content.\n"
msgstr "Kubernetesにスナップショットを登録するには、スナップショットのIDを参照するVolumeSnapshotContentと、そのコンテンツを参照するVolumeSnapshotの2つのリソースを作成する必要があります。\n"

#. type: delimited block -
#, no-wrap
msgid "[source,yaml]\n"
msgstr "[source,yaml]\n"

#. type: Plain text
#, no-wrap
msgid ""
"apiVersion: snapshot.storage.k8s.io/v1\n"
"kind: VolumeSnapshot\n"
"metadata:\n"
"  name: example-backup-from-s3\n"
"  namespace: project\n"
"spec:\n"
"  source:\n"
"    volumeSnapshotContentName: restored-snap-content-from-s3\n"
"  volumeSnapshotClassName: linstor-csi-snapshot-class-s3\n"
"---\n"
msgstr ""
"apiVersion: snapshot.storage.k8s.io/v1\n"
"kind: VolumeSnapshot\n"
"metadata:\n"
"  name: example-backup-from-s3\n"
"  namespace: project\n"
"spec:\n"
"  source:\n"
"    volumeSnapshotContentName: restored-snap-content-from-s3\n"
"  volumeSnapshotClassName: linstor-csi-snapshot-class-s3\n"
"---\n"

#. type: Plain text
#, no-wrap
msgid ""
"apiVersion: snapshot.storage.k8s.io/v1\n"
"kind: VolumeSnapshotContent\n"
"metadata:\n"
"  name: restored-snap-content-from-s3\n"
"spec:\n"
"  deletionPolicy: Delete\n"
"  driver: linstor.csi.linbit.com\n"
"  source:\n"
"    snapshotHandle: snapshot-id\n"
"  volumeSnapshotClassName: linstor-csi-snapshot-class-s3\n"
"  volumeSnapshotRef:\n"
"    apiVersion: snapshot.storage.k8s.io/v1\n"
"    kind: VolumeSnapshot\n"
"    name: example-backup-from-s3\n"
"    namespace: project\n"
msgstr ""
"apiVersion: snapshot.storage.k8s.io/v1\n"
"kind: VolumeSnapshotContent\n"
"metadata:\n"
"  name: restored-snap-content-from-s3\n"
"spec:\n"
"  deletionPolicy: Delete\n"
"  driver: linstor.csi.linbit.com\n"
"  source:\n"
"    snapshotHandle: snapshot-id\n"
"  volumeSnapshotClassName: linstor-csi-snapshot-class-s3\n"
"  volumeSnapshotRef:\n"
"    apiVersion: snapshot.storage.k8s.io/v1\n"
"    kind: VolumeSnapshot\n"
"    name: example-backup-from-s3\n"
"    namespace: project\n"

#. type: delimited block -
#, no-wrap
#| msgid "Once applied, the VolumeSnapshot should be shown as `ready`, at which point you can reference it as a `dataSource` in a PVC."
msgid ""
"Once applied, the VolumeSnapshot should be shown as `ready`, at which point you can reference it as a `dataSource` in a\n"
"PVC.\n"
msgstr "適用後、VolumeSnapshotは `ready` として表示されるべきであり、その時点でPVC内で `dataSource` として参照できます。\n"

#. type: delimited block -
#, no-wrap
#| msgid "Volume Accessibility and Locality"
msgid "[[s-kubernetes-volume-accessibility-and-locality]]\n"
msgstr "[[s-kubernetes-volume-accessibility-and-locality]]\n"

#. type: Title ===
#, no-wrap
msgid "Volume Accessibility and Locality"
msgstr "ボリュームのアクセシビリティとローカリティ"

#. type: delimited block -
msgid ""
"// This only covers DRBD volumes, section might change if linked docs are updated.  LINSTOR volumes are typically accessible both locally and <<s-drbd_clients,over the network>>. The CSI driver will ensure that the volume is accessible on whatever node was selected for the consumer. The driver also provides options to ensure volume locality (the consumer is placed on the same node as the backing data) and restrict accessibility (only a subset of nodes can access "
"the volume over the network)."
msgstr ""
"これはDRBDボリュームにのみ適用され、関連ドキュメントが更新された場合はセクションが変更される可能性があります。 LINSTORボリュームは通常、<<s-drbd_clients、ネットワーク越しに>> ローカルおよびアクセス可能です。 CSIドライバーは、ボリュームが消費者向けに選択されたノードでアクセス可能であることを保証します。 ドライバーには、ボリュームの局所性（消費者がバッキングデータと同じノードに配置される）およびアクセシビリティの制限（ネットワーク越しにボリュームにアクセスできるノー"
"ドのサブセットのみ）を保証するオプションも用意されています。"

#. type: delimited block -
msgid "Volume locality is achieved by setting `volumeBindingMode: WaitForFirstConsumer` in the storage class. This tell Kubernetes and the CSI driver to wait until the first consumer (Pod) referencing the PVC is scheduled. The CSI driver then provisions the volume with backing data on the same node as the consumer. In case a node without appropriate storage pool was selected, a replacement node in the set of accessible nodes is chosen (see below)."
msgstr ""
"ボリュームの局所性は、ストレージクラスで `volumeBindingMode: WaitForFirstConsumer` を設定することで実現されます。これにより、Kubernetes と CSI ドライバーは、PVC を参照する最初のコンシューマー（ポッド）がスケジュールされるまで待機するように指示されます。次に、CSI ドライバーは、コンシューマーと同じノード上の下位データを使用してボリュームをプロビジョニングします。適切なストレージプールのないノードが選択された場合、アクセス可能なノードのセットから代替ノードが選択されま"
"す（以下を参照）。"

#. type: delimited block -
msgid "Volume accessibility is controlled by the <<s-kubernetes-params-allow-remote-volume-access,`allowRemoteVolumeAccess` parameter>>. Whenever the CSI plugin needs to place a volume, this parameter is consulted to get the set of \"accessible\" nodes. This means they can share volumes placed on them through the network. This information is also propagated to Kubernetes using label selectors on the PV."
msgstr "ボリュームのアクセシビリティは、`allowRemoteVolumeAccess`パラメータによって制御されます。CSIプラグインがボリュームを配置する必要がある場合、このパラメータが参照されて、「アクセス可能」ノードの一覧を取得します。これにより、ノード間でネットワークを介して配置されたボリュームを共有できるようになります。この情報はまた、PVのラベルセレクターを使用してKubernetesに伝播されます。"

#. type: Title ====
#, no-wrap
msgid "Volume Accessibility and Locality Examples"
msgstr "ボリュームのアクセスしやすさと局所性の例"

#. type: delimited block -
msgid "The following example show common scenarios where you want to optimize volume accessibility and locality. It also includes examples of how to spread volume replicas across zones in a cluster."
msgstr "次の例は、ボリュームのアクセスしやすさと局所性を最適化する一般的なシナリオを示しています。また、クラスター内のゾーン間でボリュームレプリカを分散する方法の例も含まれています。"

#. type: Title =====
#, no-wrap
msgid "Single-Zone Homogeneous Clusters"
msgstr "シングルゾーンの均質クラスター"

#. type: delimited block -
msgid "The cluster only spans a single zone, so latency between nodes is low. The cluster is homogeneous, that is, all nodes are configured similarly. All nodes have their own local storage pool."
msgstr "クラスタは単一のゾーンにのみまたがっています。つまり、ノード間の遅延は低くなっています。クラスタは同種です。つまり、すべてのノードが同様に構成され、すべてのノードに独自のローカルストレージプールがあります。"

#. type: delimited block -
#| msgid "example-storage-class.yaml"
msgid ".example-storage-class.yaml [source,yaml]"
msgstr ".example-storage-class.yaml [source,yaml]"

#. type: Plain text
#, no-wrap
msgid ""
"apiVersion: storage.k8s.io/v1\n"
"kind: StorageClass\n"
"metadata:\n"
"  name: linstor-storage\n"
"provisioner: linstor.csi.linbit.com\n"
"volumeBindingMode: WaitForFirstConsumer <1>\n"
"parameters:\n"
"  linstor.csi.linbit.com/storagePool: linstor-pool <2>\n"
"  linstor.csi.linbit.com/placementCount: \"2\" <3>\n"
"  linstor.csi.linbit.com/allowRemoteVolumeAccess: \"true\" <4>\n"
msgstr ""
"apiVersion: storage.k8s.io/v1\n"
"kind: StorageClass\n"
"metadata:\n"
"  name: linstor-storage\n"
"provisioner: linstor.csi.linbit.com\n"
"volumeBindingMode: WaitForFirstConsumer <1>\n"
"parameters:\n"
"  linstor.csi.linbit.com/storagePool: linstor-pool <2>\n"
"  linstor.csi.linbit.com/placementCount: \"2\" <3>\n"
"  linstor.csi.linbit.com/allowRemoteVolumeAccess: \"true\" <4>\n"

#. type: delimited block -
#, no-wrap
msgid ""
"<1> Enable late volume binding. This places one replica on the same node as the first\n"
"consuming pod, if possible.\n"
msgstr "<1> 遅延ボリュームバインディングを有効にします。可能な場合、最初の消費ポッドと同じノードにレプリカを配置します。\n"

#. type: delimited block -
#, no-wrap
msgid "<2> Set the storage pool(s) to use.\n"
msgstr "<2> 使用するストレージプールを設定します。\n"

#. type: delimited block -
#, no-wrap
msgid "<3> Ensure that the data is replicated, so that at least 2 nodes store the data.\n"
msgstr "<3> 少なくとも 2 つのノードがデータを格納することを確証します。\n"

#. type: delimited block -
#, no-wrap
msgid ""
"<4> Allow using the volume even on nodes without replica. Since all nodes are connected\n"
"equally, performance impact should be manageable.\n"
msgstr "<4> レプリカのないノードでもボリュームの使用を許可します。すべてのノードが均等に接続されているため、パフォーマンスへの影響は小さいです。\n"

#. type: Title =====
#, no-wrap
msgid "Multi-Zonal Homogeneous Clusters"
msgstr "マルチゾーン同質クラスター"

#. type: delimited block -
msgid "As before, in our homogeneous cluster all nodes are configured similarly with their own local storage pool. The cluster spans now multiple zones, with increased latency across nodes in different zones. To ensure low latency, we want to restrict access to the volume with a local replica to only those zones that do have a replica. At the same time, we want to spread our data across multiple zones."
msgstr "前述のように、均質なクラスターでは、すべてのノードがそれぞれ独自のローカルストレージプールで構成されています。クラスターは現在複数のゾーンにまたがっており、異なるゾーンのノード間で遅延が増大しています。低遅延を確保するために、ローカルレプリカを持つゾーンにのみボリュームへのアクセスを制限したいと考えています。同時に、データを複数のゾーンに分散させたいです。"

#. type: Plain text
#, no-wrap
msgid ""
"apiVersion: storage.k8s.io/v1\n"
"kind: StorageClass\n"
"metadata:\n"
"  name: linstor-storage\n"
"provisioner: linstor.csi.linbit.com\n"
"volumeBindingMode: WaitForFirstConsumer <1>\n"
"parameters:\n"
"  linstor.csi.linbit.com/storagePool: linstor-pool <2>\n"
"  linstor.csi.linbit.com/placementCount: \"2\" <3>\n"
"  linstor.csi.linbit.com/allowRemoteVolumeAccess: | <4>\n"
msgstr ""
"apiVersion: storage.k8s.io/v1\n"
"kind: StorageClass\n"
"metadata:\n"
"  name: linstor-storage\n"
"provisioner: linstor.csi.linbit.com\n"
"volumeBindingMode: WaitForFirstConsumer <1>\n"
"parameters:\n"
"  linstor.csi.linbit.com/storagePool: linstor-pool <2>\n"
"  linstor.csi.linbit.com/placementCount: \"2\" <3>\n"
"  linstor.csi.linbit.com/allowRemoteVolumeAccess: | <4>\n"

#. type: Plain text
#, no-wrap
msgid "fromSame:\n"
msgstr "fromSame:\n"

#. type: Plain text
#, no-wrap
msgid "topology.kubernetes.io/zone\n"
msgstr "topology.kubernetes.io/zone\n"

#. type: Plain text
#, no-wrap
msgid "linstor.csi.linbit.com/replicasOnDifferent: topology.kubernetes.io/zone <5>\n"
msgstr "linstor.csi.linbit.com/replicasOnDifferent: topology.kubernetes.io/zone <5>\n"

#. type: delimited block -
#, no-wrap
msgid ""
"<4> Allow using the volume on nodes in the same zone as a replica, under the assumption that\n"
"zone internal networking is fast and low latency.\n"
msgstr "同一ゾーン内のノード上でレプリカとしてボリュームを使用することを許可します。前提として、ゾーン内ネットワーキングが高速かつ低遅延であると仮定します。\n"

#. type: delimited block -
#, no-wrap
msgid "<5> Spread the replicas across different zones.\n"
msgstr "<5> レプリカを異なるゾーンに分散配置してください。\n"

#. type: Title =====
#, no-wrap
msgid "Multi-Region Clusters"
msgstr "マルチリージョンクラスター"

#. type: delimited block -
msgid "If your cluster spans multiple regions, you do not want to incur the latency penalty to replicate your data across regions. To accomplish this, you can configure your storage class to just replicate data in the same zone."
msgstr "クラスタが複数のリージョンにまたがっている場合、異なるリージョン間でデータをレプリケートする際に発生する遅延ペナルティを引き起こしたくありません。このために、ストレージクラスを設定して、データを同じゾーンでのみレプリケートするようにすることができます。"

#. type: Plain text
#, no-wrap
msgid "linstor.csi.linbit.com/replicasOnSame: topology.kubernetes.io/region <5>\n"
msgstr "linstor.csi.linbit.com/replicasOnSame: topology.kubernetes.io/region <5>\n"

#. type: delimited block -
#, no-wrap
msgid "<5> Restrict replicas to only a single region.\n"
msgstr "<5> レプリカを単一のリージョンのみに制限します。\n"

#. type: Title =====
#, no-wrap
msgid "Cluster with External Storage"
msgstr "外部ストレージを備えたクラスター"

#. type: delimited block -
msgid "Our cluster now only consists of compute nodes without local storage. Any volume access has to occur through remote volume access."
msgstr "クラスターはローカルストレージのないノードのみで構成されています。ボリュームアクセスは、リモートボリュームアクセスを介して行う必要があります。"

#. type: Plain text
#, no-wrap
msgid ""
"apiVersion: storage.k8s.io/v1\n"
"kind: StorageClass\n"
"metadata:\n"
"  name: linstor-storage\n"
"provisioner: linstor.csi.linbit.com\n"
"parameters:\n"
"  linstor.csi.linbit.com/storagePool: linstor-pool <1>\n"
"  linstor.csi.linbit.com/placementCount: \"1\" <2>\n"
"  linstor.csi.linbit.com/allowRemoteVolumeAccess: \"true\" <3>\n"
msgstr ""
"apiVersion: storage.k8s.io/v1\n"
"kind: StorageClass\n"
"metadata:\n"
"  name: linstor-storage\n"
"provisioner: linstor.csi.linbit.com\n"
"parameters:\n"
"  linstor.csi.linbit.com/storagePool: linstor-pool <1>\n"
"  linstor.csi.linbit.com/placementCount: \"1\" <2>\n"
"  linstor.csi.linbit.com/allowRemoteVolumeAccess: \"true\" <3>\n"

#. type: delimited block -
#, no-wrap
msgid "<1> Set the storage pool(s) to use.\n"
msgstr "<1> 使用するストレージプールを設定します。\n"

#. type: delimited block -
#, no-wrap
msgid ""
"<2> Assuming we only have one storage host, we can only place a single volume without\n"
"additional replicas.\n"
msgstr "<<2>> 個のストレージホストしか持っていないと仮定すると、単一のボリュームしか配置できません。\n"

#. type: delimited block -
#, no-wrap
msgid "<3> Our worker nodes need to be allowed to connect to the external storage host.\n"
msgstr "<3> 私たちのワーカーノードは、外部ストレージホストに接続することが許可される必要があります。\n"

#. type: delimited block -
#, no-wrap
msgid "[[s-kubernetes-affinity-controller]]\n"
msgstr "[[s-kubernetes-affinity-controller]]\n"

#. type: Title ===
#, no-wrap
msgid "LINSTOR Affinity Controller"
msgstr "LINSTOR アフィニティ コントローラ"

#. type: delimited block -
msgid "Volume Accessibility is controlled by the https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity[node affinity] of the PersistentVolume (PV). This affinity is static, that is once defined it cannot be changed."
msgstr "ボリュームのアクセシビリティは、PersistentVolume (PV) の https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity[node affinity] によって制御されます。このアフィニティは静的です。つまり、一度定義すると変更できません。"

#. type: delimited block -
msgid "This can be an issue if you want to use a strict affinity: Your PV is pinned to specific nodes, but you might want to remove or add nodes. While LINSTOR can move the volume (for example: this happens automatically if you remove a node in Kubernetes), the PV affinity is not updated to reflect this."
msgstr "この場合、厳密なアフィニティを使用したい場合に問題となる可能性があります: あなたのPVは特定のノードにピン留めされていますが、ノードを削除したり追加したい場合があります。LINSTORはボリュームを移動できます（例：Kubernetesでノードを削除するとこれが自動的に行われます）、しかしPVのアフィニティはこれを反映するよう更新されません。"

#. type: delimited block -
msgid "This is where the LINSTOR Affinity Controller comes in: it watches PVs and compares their affinity with the volumes' states in LINSTOR. If they go out of sync, the PV is replaced with an updated version."
msgstr "ここで、LINSTOR Affinity Controller の出番です。PV を監視し、それらのアフィニティを LINSTOR のボリュームの状態と比較します。それらが同期しなくなると、PV は更新されたバージョンに置き換えられます。"

#. type: delimited block -
msgid "The LINSTOR Affinity Controller is packaged in a Helm chart. If you install it in the same namespace as the Operator, simply run:"
msgstr "LINSTOR Affinity Controller は、Helm チャートにパッケージ化されています。 Operator と同じ名前空間にインストールする場合は、以下を実行するだけです。"

#. type: Plain text
msgid "$ helm repo update $ helm install linstor-affinity-controller linstor/linstor-affinity-controller"
msgstr "$ helm repo update $ helm install linstor-affinity-controller linstor/linstor-affinity-controller"

#. type: delimited block -
#, no-wrap
msgid "Additional options for the chart are available at the https://github.com/piraeusdatastore/linstor-affinity-controller[upstream project].\n"
msgstr "チャートの追加オプションは、https://github.com/piraeusdatastore/linstor-affinity-controller[upstream project]で利用可能です。\n"

#. type: delimited block -
#, no-wrap
msgid "[[s-kubernetes-scheduler]]\n"
msgstr "[[s-kubernetes-scheduler]]\n"

#. type: Title ===
#, no-wrap
msgid "Volume Locality Optimization Using LINSTOR Scheduler"
msgstr "LINSTOR スケジューラーを使用したボリューム局所性の最適化"

#. type: delimited block -
msgid "LINBIT maintains an open source plugin for the Kubernetes scheduler. The scheduler will take the current placement of volumes into account and optimize for data locality. If possible, the pod will be assigned to a node that also hosts replicas of attached volumes, reducing latency for read operations."
msgstr "LINBITは、Kubernetesのスケジューラー向けのオープンソースのプラグインをメンテナンスしています。このスケジューラーは、ボリュームの現在の配置を考慮し、データの局所性を最適化します。可能であれば、ポッドは、アタッチされたボリュームのレプリカをホストしているノードに割り当てられ、読み取り操作の遅延が低減されます。"

#. type: delimited block -
msgid "The scheduler is available as a separate chart https://artifacthub.io/packages/helm/piraeus-charts/linstor-scheduler[from artifacthub.io].  The chart will deploy a new scheduler, which you can later use when creating pod resources:"
msgstr "スケジューラーは別のチャート https://artifacthub.io/packages/helm/piraeus-charts/linstor-scheduler[artifacthub.io] から利用できます。チャートは、後で Pod リソースを作成するときに使用できる新しいスケジューラーをデプロイします。"

#. type: Plain text
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: Pod\n"
"metadata:\n"
"  name: busybox\n"
"spec:\n"
"  schedulerName: linstor-scheduler <1>\n"
"  containers:\n"
msgstr ""
"apiVersion: v1\n"
"kind: Pod\n"
"metadata:\n"
"  name: busybox\n"
"spec:\n"
"  schedulerName: linstor-scheduler <1>\n"
"  containers:\n"

#. type: Plain text
#, no-wrap
msgid ""
"name: busybox\n"
"image: busybox\n"
"command: [\"tail\", \"-f\", \"/dev/null\"]\n"
"volumeMounts:\n"
msgstr ""
"name: busybox\n"
"image: busybox\n"
"command: [\"tail\", \"-f\", \"/dev/null\"]\n"
"volumeMounts:\n"

#. type: Plain text
#, no-wrap
msgid ""
"name: my-first-linstor-volume\n"
"mountPath: /data\n"
msgstr ""
"name: my-first-linstor-volume\n"
"mountPath: /data\n"

#. type: Plain text
#, no-wrap
msgid "ports:\n"
msgstr "ports:\n"

#. type: Plain text
#, no-wrap
msgid "containerPort: 80\n"
msgstr "containerPort: 80\n"

#. type: Plain text
#, no-wrap
msgid "volumes:\n"
msgstr "volumes:\n"

#. type: Plain text
#, no-wrap
msgid ""
"name: my-first-linstor-volume\n"
"persistentVolumeClaim:\n"
msgstr ""
"name: my-first-linstor-volume\n"
"persistentVolumeClaim:\n"

#. type: Plain text
#, no-wrap
msgid "claimName: \"test-volume\"\n"
msgstr "claimName: \"test-volume\"\n"

#. type: delimited block -
#, no-wrap
msgid "<1> Add the name of the scheduler to your pod.\n"
msgstr "<1> スケジューラの名前をポッドに追加します。\n"

#. type: delimited block -
#, no-wrap
msgid "[[s-kubernetes-drbd-module-loader-configuring-v2]]\n"
msgstr "[[s-kubernetes-drbd-module-loader-configuring-v2]]\n"

#. type: Title ===
#, no-wrap
msgid "Configuring the DRBD Module Loader in Operator v2 Deployments"
msgstr "Operator v2デプロイメントにおけるDRBDモジュールローダーの設定:"

#. type: delimited block -
msgid "// https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/how-to/drbd-loader.md"
msgstr "// https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/how-to/drbd-loader.md"

#. type: delimited block -
msgid "NOTE: To follow the steps in this section, you should be familiar with editing link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md#linstorsatelliteconfiguration[`LinstorSatelliteConfiguration`] resources."
msgstr "注意: このセクションの手順に従うには、link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md#linstorsatelliteconfiguration[`LinstorSatelliteConfiguration`] リソースの編集に慣れている必要があります。"

#. type: delimited block -
msgid "The DRBD module loader is the component responsible for making the DRBD kernel module available, in addition to loading other useful kernel modules for LINBIT SDS in Kubernetes. This section describes how you can configure various aspects of the DRBD kernel module loader, within a LINSTOR Operator v2 deployment."
msgstr "DRBDモジュールローダーは、LINBIT SDSにおいてDRBDカーネルモジュールを利用可能にし、Kubernetesでの他の便利なカーネルモジュールの読み込みを行うコンポーネントです。このセクションでは、LINSTOR Operator v2デプロイメント内で、DRBDカーネルモジュールローダーのさまざまな側面を構成する方法について説明します。"

#. type: delimited block -
msgid "Besides the DRBD kernel module, these modules are also loaded if available:"
msgstr "提供されている場合、DRBDカーネルモジュール以外にも、以下のモジュールがロードされます:"

#. type: Table
#, no-wrap
msgid ""
"[cols=\"1,1\"]\n"
"|Module | Purpose\n"
"\n"
"| `libcrc32c` | dependency for DRBD\n"
"| `nvmet_rdma`, `nvme_rdma` | LINSTOR NVME layer\n"
"| `loop` | LINSTOR when using loop devices as backing disks\n"
"| `dm_writecache` | LINSTOR writecache layer\n"
"| `dm_cache` | LINSTOR cache layer\n"
"| `dm_thin_pool` | LINSTOR thin-provisioned storage\n"
"| `dm_snapshot` | LINSTOR Snapshots\n"
"| `dm_crypt` | LINSTOR encrypted volumes\n"
msgstr ""
"[cols=\"1,1\"]\n"
"|Module | Purpose\n"
"\n"
"| `libcrc32c` | dependency for DRBD\n"
"| `nvmet_rdma`, `nvme_rdma` | LINSTOR NVME layer\n"
"| `loop` | LINSTOR when using loop devices as backing disks\n"
"| `dm_writecache` | LINSTOR writecache layer\n"
"| `dm_cache` | LINSTOR cache layer\n"
"| `dm_thin_pool` | LINSTOR thin-provisioned storage\n"
"| `dm_snapshot` | LINSTOR Snapshots\n"
"| `dm_crypt` | LINSTOR encrypted volumes\n"

#. type: Title ====
#, no-wrap
msgid "Disabling the DRBD Module Loader"
msgstr "DRBDモジュールローダーの無効化"

#. type: Plain text
msgid "In some circumstances it might be necessary to disable the DRBD module loader entirely. For example, if you are using an immutable operating system, and DRBD and other modules are loaded as part of the host configuration."
msgstr "いくつかの状況下では、DRBDモジュールローダーを完全に無効にする必要がある場合があります。たとえば、イミュータブルなオペレーティングシステムを使用していて、DRBDや他のモジュールがホスト設定の一部としてロードされている場合などです。"

#. type: Plain text
msgid "To disable the DRBD module loader completely, apply the following YAML configuration to your deployment:"
msgstr "DRBDモジュールローダーを完全に無効にするには、次のYAML構成をデプロイメントに適用してください:"

#. type: delimited block -
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: no-loader\n"
"spec:\n"
"  patches:\n"
"    - target:\n"
"        kind: Pod\n"
"        name: satellite\n"
"      patch: |\n"
"        apiVersion: v1\n"
"        kind: Pod\n"
"        metadata:\n"
"          name: satellite\n"
"        spec:\n"
"          initContainers:\n"
"          - name: drbd-module-loader\n"
"            $patch: delete\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: no-loader\n"
"spec:\n"
"  patches:\n"
"    - target:\n"
"        kind: Pod\n"
"        name: satellite\n"
"      patch: |\n"
"        apiVersion: v1\n"
"        kind: Pod\n"
"        metadata:\n"
"          name: satellite\n"
"        spec:\n"
"          initContainers:\n"
"          - name: drbd-module-loader\n"
"            $patch: delete\n"

#. type: Title ====
#, no-wrap
msgid "Selecting a Different DRBD Module Loader Version"
msgstr "異なるDRBDモジュールローダーバージョンの選択: "

#. type: Plain text
msgid "By default, the Operator will try to find a DRBD module loader that matches the host operating system. The Operator determines the host distribution by inspecting the `.status.nodeInfo.osImage` field of the Kubernetes `Node` resource. A user-defined image can be used if the automatic mapping does not succeed or if you have different module loading requirements."
msgstr "デフォルトでは、Operatorはホストのオペレーティングシステムに一致するDRBDモジュールローダーを見つけようとします。Operatorは、Kubernetesの`Node`リソースの`.status.nodeInfo.osImage`フィールドを調査することでホストディストリビューションを判別します。自動マッピングが成功しない場合や異なるモジュールローディング要件がある場合は、ユーザー定義のイメージを使用できます。"

#. type: Plain text
msgid "The following YAML configuration overrides the chosen DRBD module loader image with a user-defined image `example.com/drbd-loader:v9`:"
msgstr "次のYAML構成は、選択したDRBDモジュールローダーのイメージをユーザー定義のイメージ `example.com/drbd-loader:v9` で上書きします。"

#. type: delimited block -
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: custom-drbd-module-loader-image\n"
"spec:\n"
"  patches:\n"
"    - target:\n"
"        kind: Pod\n"
"        name: satellite\n"
"      patch: |\n"
"        apiVersion: v1\n"
"        kind: Pod\n"
"        metadata:\n"
"          name: satellite\n"
"        spec:\n"
"          initContainers:\n"
"          - name: drbd-module-loader\n"
"            image: example.com/drbd-loader:v9\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: custom-drbd-module-loader-image\n"
"spec:\n"
"  patches:\n"
"    - target:\n"
"        kind: Pod\n"
"        name: satellite\n"
"      patch: |\n"
"        apiVersion: v1\n"
"        kind: Pod\n"
"        metadata:\n"
"          name: satellite\n"
"        spec:\n"
"          initContainers:\n"
"          - name: drbd-module-loader\n"
"            image: example.com/drbd-loader:v9\n"

#. type: Plain text
msgid "`drbd.io`, available to LINBIT customers only, maintains the following module loader container images:"
msgstr "`drbd.io`は、LINBITの顧客のみが利用可能で、以下のモジュールローダーコンテナイメージを維持しています。"

#. type: Table
#, no-wrap
msgid ""
"| Image | Distribution\n"
"\n"
"| `drbd.io/drbd9-amzn2:v9.2.5` | Amazon Linux 2\n"
"| `drbd.io/drbd9-bionic:v9.2.5` | Ubuntu 18.04\n"
"| `drbd.io/drbd9-focal:v9.2.5` | Ubuntu 20.04\n"
"| `drbd.io/drbd9-jammy:v9.2.5` | Ubuntu 22.04\n"
"| `drbd.io/drbd9-rhel7:v9.2.5` | Red Hat Enterprise Linux 7\n"
"| `drbd.io/drbd9-rhel8:v9.2.5` | Red Hat Enterprise Linux 8\n"
"| `drbd.io/drbd9-rhel9:v9.2.5` | Red Hat Enterprise Linux 9\n"
msgstr ""
"| Image | Distribution\n"
"\n"
"| `drbd.io/drbd9-amzn2:v9.2.5` | Amazon Linux 2\n"
"| `drbd.io/drbd9-bionic:v9.2.5` | Ubuntu 18.04\n"
"| `drbd.io/drbd9-focal:v9.2.5` | Ubuntu 20.04\n"
"| `drbd.io/drbd9-jammy:v9.2.5` | Ubuntu 22.04\n"
"| `drbd.io/drbd9-rhel7:v9.2.5` | Red Hat Enterprise Linux 7\n"
"| `drbd.io/drbd9-rhel8:v9.2.5` | Red Hat Enterprise Linux 8\n"
"| `drbd.io/drbd9-rhel9:v9.2.5` | Red Hat Enterprise Linux 9\n"

#. type: Plain text
msgid "If you need to create a module loader image for your own distribution, you can refer to link:https://github.com/piraeusdatastore/piraeus/tree/master/dockerfiles/drbd-driver-loader[the container source files] which are available in the upstream Piraeus project."
msgstr "自分自身のディストリビューション用のモジュールローダーイメージを作成する必要がある場合は、上流のPiraeusプロジェクトで利用可能な<<the container source files>>[コンテナのソースファイル]にアクセスすることができます。詳細はリンク:https://github.com/piraeusdatastore/piraeus/tree/master/dockerfiles/drbd-driver-loader  をご参照ください。"

#. type: Title ====
#, no-wrap
msgid "Changing How the Module Loader Loads the DRBD Kernel Module"
msgstr "モジュールローダーがDRBDカーネルモジュールをロードする方法の変更"

#. type: Plain text
msgid "By default, the DRBD module loader will try to build the kernel module from source. The module loader can also be configured to load the module from a DEB or RPM package included in the image, or skip loading DRBD entirely."
msgstr "デフォルトでは、DRBDモジュールローダーはソースからカーネルモジュールをビルドしようとします。モジュールローダーは、イメージに含まれるDEBやRPMパッケージからモジュールを読み込むように構成することもでき、また、完全にDRBDの読み込みをスキップすることもできます。"

#. type: Plain text
msgid "To change the behavior of the DRBD module loader, set the `LB_HOW` environment variable to an appropriate value shown in the following table:"
msgstr "DRBDモジュールローダーの動作を変更するには、`LB_HOW`環境変数を、以下の表に示されている適切な値に設定してください。"

#. type: Table
#, no-wrap
msgid ""
"| `LB_HOW` | Module Loader Behavior\n"
"\n"
"| `compile` | The default value. Builds the DRBD module from source and tries to load all optional modules from the host.\n"
"| `shipped_modules` | Searches for `.rpm` or `.deb` packages at `/pkgs` and inserts contained the DRBD modules. Optional modules are loaded from the host if available.\n"
"| `deps_only` | Only tries to load the optional modules. No DRBD module will be loaded.\n"
msgstr ""
"| `LB_HOW` | Module Loader Behavior\n"
"\n"
"| `compile` | The default value. Builds the DRBD module from source and tries to load all optional modules from the host.\n"
"| `shipped_modules` | Searches for `.rpm` or `.deb` packages at `/pkgs` and inserts contained the DRBD modules. Optional modules are loaded from the host if available.\n"
"| `deps_only` | Only tries to load the optional modules. No DRBD module will be loaded.\n"

#. type: Plain text
msgid "After setting the `LB_HOW` environment variable, apply the following YAML configuration to your deployment. Based on the name within the metadata section, the example below would be used with an `LB_HOW` environment variable that was set to `deps_only`."
msgstr "`LB_HOW`環境変数を設定した後、デプロイメントに以下のYAML構成を適用してください。メタデータセクション内の名前に基づいて、以下の例は`LB_HOW`環境変数が`deps_only`に設定されている場合に使用されます。"

#. type: delimited block -
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: no-drbd-module-loader\n"
"spec:\n"
"  patches:\n"
"    - target:\n"
"        kind: Pod\n"
"        name: satellite\n"
"      patch: |\n"
"        apiVersion: v1\n"
"        kind: Pod\n"
"        metadata:\n"
"          name: satellite\n"
"        spec:\n"
"          initContainers:\n"
"          - name: drbd-module-loader\n"
"            env:\n"
"            - name: LB_HOW\n"
"              value: deps_only\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: no-drbd-module-loader\n"
"spec:\n"
"  patches:\n"
"    - target:\n"
"        kind: Pod\n"
"        name: satellite\n"
"      patch: |\n"
"        apiVersion: v1\n"
"        kind: Pod\n"
"        metadata:\n"
"          name: satellite\n"
"        spec:\n"
"          initContainers:\n"
"          - name: drbd-module-loader\n"
"            env:\n"
"            - name: LB_HOW\n"
"              value: deps_only\n"

#. type: Title ===
#, no-wrap
msgid "Using the Host Network for DRBD Replication in Operator v2 Deployments"
msgstr "Operator v2 デプロイメントにおいて DRBD レプリケーション用ホストネットワークを使用する"

#.  https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/how-to/drbd-host-networking.md
#. type: Plain text
msgid "Instructions in this section will describe how you can use the host network for DRBD replication traffic."
msgstr "このセクションの手順では、ホストネットワークを使用してDRBDレプリケーショントラフィックを行う方法について説明します。"

#. type: Plain text
msgid "By default, DRBD will use the container network to replicate volume data. This ensures replication works on a wide range of clusters without further configuration. It also enables use of `NetworkPolicy` to block unauthorized access to DRBD traffic. Since the network interface of the pod is tied to the lifecycle of the pod, it also means DRBD will temporarily disrupt replication when the LINSTOR satellite pod is restarted."
msgstr "デフォルトでは、DRBDはコンテナネットワークを使用してボリュームデータを複製します。これにより、さらなる構成を行わずに幅広い範囲のクラスタで複製が機能することを保証します。また、`NetworkPolicy`の使用を可能にし、DRBDトラフィックへの未認可アクセスをブロックすることができます。ポッドのネットワークインターフェースはポッドのライフサイクルに結び付いているため、LINSTORサテライトポッドが再起動されると、DRBDは一時的に複製を中断することを意味します。"

#. type: Plain text
msgid "In contrast, using the host network for DRBD replication will cause replication to work independently of the LINSTOR satellite pod. The host network might also offer better performance than the container network. As a downside, you will have to manually ensure connectivity between nodes on the relevant ports."
msgstr "ホストネットワークを使用すると、DRBDレプリケーションはLINSTORサテライトポッドとは独立して動作するため、一方通行とは異なります。ホストネットワークは、コンテナネットワークよりもパフォーマンスが向上する場合があります。デメリットとしては、関連ポート間でノード間の接続性を手動で確保する必要があります。"

#. type: Plain text
msgid "To follow the steps in this section, you should be familiar with editing link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md#linstorsatelliteconfiguration[`LinstorSatelliteConfiguration`] resources."
msgstr "このセクションの手順に従うには、link:https://github.com/piraeusdatastore/piraeus-operator/blob/v2/docs/reference/linstorsatelliteconfiguration.md#linstorsatelliteconfiguration[`LinstorSatelliteConfiguration`] リソースの編集に慣れている必要があります。"

#. type: Title ====
#, no-wrap
msgid "Configuring DRBD Replication to Use the Host Network"
msgstr "ホストネットワークを使用するように DRBD レプリケーションを構成する"

#. type: Plain text
msgid "Switching from the default container network to the host network for DRBD replication is possible at any time. Existing DRBD resources will then be reconfigured to use the host network interface."
msgstr "デフォルトのコンテナネットワークからホストネットワークに切り替えて DRBD レプリケーションを行うことはいつでも可能です。既存の DRBD リソースはその後、ホストネットワークインターフェイスを使用するように再構成されます。"

#. type: Plain text
msgid "To configure the host network for the LINSTOR satellite, apply the following YAML configuration to your deployment:"
msgstr "サテライトのホストネットワークを設定するには、次のYAML構成をデプロイメントに適用してください。"

#. type: delimited block -
#, no-wrap
msgid ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: host-network\n"
"spec:\n"
"  patches:\n"
"    - target:\n"
"        kind: Pod\n"
"        name: satellite\n"
"      patch: |\n"
"        apiVersion: v1\n"
"        kind: Pod\n"
"        metadata:\n"
"          name: satellite\n"
"        spec:\n"
"          hostNetwork: true\n"
msgstr ""
"apiVersion: piraeus.io/v1\n"
"kind: LinstorSatelliteConfiguration\n"
"metadata:\n"
"  name: host-network\n"
"spec:\n"
"  patches:\n"
"    - target:\n"
"        kind: Pod\n"
"        name: satellite\n"
"      patch: |\n"
"        apiVersion: v1\n"
"        kind: Pod\n"
"        metadata:\n"
"          name: satellite\n"
"        spec:\n"
"          hostNetwork: true\n"

#. type: Plain text
msgid "After the satellite pods are recreated, they will use the host network. Any existing DRBD resources are reconfigured to use a new IP address on the host network rather than an IP address on the container network."
msgstr "サテライトポッドが再作成されると、ホストネットワークを使用します。既存の DRBD リソースは、コンテナネットワーク上の IP アドレスではなく、ホストネットワーク上の新しい IP アドレスを使用するように再構成されます。"

#. type: Title ====
#, no-wrap
msgid "Configuring DRBD Replication to Use the Container Network"
msgstr "コンテナーネットワークを使用するためのDRBDレプリケーションの設定"

#. type: Plain text
msgid "Switching back from host network to container network involves manually resetting the configured peer addresses used by DRBD. You can do this by rebooting every node, or by manually resetting the addresses by using the `drbdadm` CLI command on each node. Each method is described below."
msgstr "ホストネットワークからコンテナーネットワークに戻す際には、DRBDで使用される構成されたピアアドレスを手動でリセットする必要があります。この作業は、すべてのノードを再起動するか、各ノードで `drbdadm` CLIコマンドを使用してアドレスを手動でリセットすることによって行うことができます。それぞれの方法について以下に説明します。"

#. type: Plain text
msgid "[[s-kubernetes-drbd-replication-switching-from-host-to-container-network-node-rebooting-v2"
msgstr "[[s-kubernetes-drbd-replication-switching-from-host-to-container-network-node-rebooting-v2"

#. type: Title =====
#, no-wrap
msgid "Rebooting Nodes to Switch DRBD Replication from the Host to the Container Network"
msgstr "ホストからコンテナネットワークへのDRBDレプリケーション切り替えのためにノードを再起動します。"

#. type: Plain text
msgid "First, you need to remove the `LinstorSatelliteConfiguration` that set `hostNetwork: true`. You can do this by entering the following `kubectl` command:"
msgstr "最初に、`hostNetwork: true` を設定した `LinstorSatelliteConfiguration` を削除する必要があります。次の `kubectl` コマンドを入力することでこれを行うことができます："

#. type: delimited block -
#, no-wrap
msgid ""
"$ kubectl delete linstorsatelliteconfigurations.piraeus.io host-network\n"
"linstorsatelliteconfiguration.piraeus.io \"host-network\" deleted\n"
msgstr ""
"$ kubectl delete linstorsatelliteconfigurations.piraeus.io host-network\n"
"linstorsatelliteconfiguration.piraeus.io \"host-network\" deleted\n"

#. type: Plain text
msgid "Next, reboot each cluster node, either serially, one by one, or else all at once. In general, replication will not work between rebooted nodes and non-rebooted nodes. The non-rebooted nodes will continue to use the host network addresses, which are generally not reachable from the container network."
msgstr "次に、各クラスターノードを順次または一度にすべて再起動します。一般的に、再起動したノードと再起動していないノードの間でレプリケーションは機能しません。 再起動していないノードはホストネットワークアドレスを引き続き使用し、これらは通常、コンテナネットワークから到達できません。"

#. type: Plain text
msgid "After all nodes have restarted, all resources will be configured to use the container network, and all DRBD connections should be connected again."
msgstr "全てのノードが再起動された後、全てのリソースはコンテナネットワークを使用するように構成され、全ての DRBD 接続が再度接続されるはずです。"

#. type: Plain text
msgid "[[s-kubernetes-drbd-replication-switching-from-host-to-container-network-node-drbdadm-v2"
msgstr "[[s-kubernetes-drbd-replication-switching-from-host-to-container-network-node-drbdadm-v2"

#. type: Title =====
#, no-wrap
msgid "Using the DRBD Administration Tool to Switch DRBD Replication from the Host to the Container Network"
msgstr "DRBD管理ツールを使用して、ホストからコンテナネットワークへのDRBDレプリケーションの切り替え"

#. type: Plain text
msgid "During this procedure, ensure that no new volumes or snapshots are created, otherwise the migration to the container network might not be applied to all resources."
msgstr "この手順中は、新しいボリュームやスナップショットが作成されないように注意してください。さもないと、コンテナネットワークへの移行がすべてのリソースに適用されない可能性があります。"

#. type: Plain text
msgid "First, you need to temporarily stop all DRBD replication and suspend all DRBD volume I/O operations by using the `drbdadm suspend-io all` command. Enter the command once on each LINSTOR satellite pod."
msgstr "まず、`drbdadm suspend-io all`コマンドを使用してすべてのDRBDレプリケーションを一時停止し、すべてのDRBDボリュームI/O操作を一時停止する必要があります。各LINSTORサテライトポッドでコマンドを一度入力してください。"

#. type: delimited block -
#, no-wrap
msgid ""
"$ kubectl exec ds/linstor-satellite.node1.example.com -- drbdadm suspend-io all\n"
"$ kubectl exec ds/linstor-satellite.node2.example.com -- drbdadm suspend-io all\n"
"$ kubectl exec ds/linstor-satellite.node3.example.com -- drbdadm suspend-io all\n"
msgstr ""
"$ kubectl exec ds/linstor-satellite.node1.example.com -- drbdadm suspend-io all\n"
"$ kubectl exec ds/linstor-satellite.node2.example.com -- drbdadm suspend-io all\n"
"$ kubectl exec ds/linstor-satellite.node3.example.com -- drbdadm suspend-io all\n"

#. type: Plain text
msgid "Next, disconnect all DRBD connections on all nodes."
msgstr "次に、すべてのノードですべてのDRBD接続を切断してください。"

#. type: delimited block -
#, no-wrap
msgid ""
"$ kubectl exec ds/linstor-satellite.node1.example.com -- drbdadm disconnect --force all\n"
"$ kubectl exec ds/linstor-satellite.node2.example.com -- drbdadm disconnect --force all\n"
"$ kubectl exec ds/linstor-satellite.node3.example.com -- drbdadm disconnect --force all\n"
msgstr ""
"$ kubectl exec ds/linstor-satellite.node1.example.com -- drbdadm disconnect --force all\n"
"$ kubectl exec ds/linstor-satellite.node2.example.com -- drbdadm disconnect --force all\n"
"$ kubectl exec ds/linstor-satellite.node3.example.com -- drbdadm disconnect --force all\n"

#. type: Plain text
msgid "Next, you can safely reset all DRBD connection paths. This frees the connection on each node to be moved to the container network."
msgstr "次に、すべてのDRBD接続パスを安全にリセットできます。これにより、各ノード上の接続がコンテナネットワークに移動できます。"

#. type: delimited block -
#, no-wrap
msgid ""
"$ kubectl exec ds/linstor-satellite.node1.example.com -- drbdadm del-path all\n"
"$ kubectl exec ds/linstor-satellite.node2.example.com -- drbdadm del-path all\n"
"$ kubectl exec ds/linstor-satellite.node3.example.com -- drbdadm del-path all\n"
msgstr ""
"$ kubectl exec ds/linstor-satellite.node1.example.com -- drbdadm del-path all\n"
"$ kubectl exec ds/linstor-satellite.node2.example.com -- drbdadm del-path all\n"
"$ kubectl exec ds/linstor-satellite.node3.example.com -- drbdadm del-path all\n"

#. type: Plain text
msgid "Finally, remove the `LinstorSatelliteConfiguration` resource configuration that set `hostNetwork: true`. This will result in the creation of new LINSTOR satellite pods that use the container network."
msgstr "最後に、`hostNetwork: true`を設定した`LinstorSatelliteConfiguration`リソース構成を削除します。これにより、コンテナネットワークを使用する新しいLINSTORサテライトポッドが作成されます。"

#. type: Plain text
msgid "After the pods are recreated and the LINSTOR satellites are `Online`, the DRBD resource will be reconfigured and resume I/O operations."
msgstr "ポッドが再作成され、LINSTORのサテライトが `オンライン` になった後、DRBDリソースは再構成され、I/O操作が再開されます。"

#. type: Title ===
#, no-wrap
msgid "Evacuating a Node in Kubernetes"
msgstr "Kubernetes でのノードの退避"

#. type: Plain text
msgid "If you want to evacuate a LINSTOR node of its resources, so that they are placed onto other nodes within your cluster, the process is detailed in <<s-linstor-node-evacuate>>. However, before evacuating a LINSTOR node in Kubernetes, you need to take an additional action."
msgstr "リソースの LINSTOR ノードを退避させて、クラスター内の他のノードに配置する場合、そのプロセスは <<s-linstor-node-evacuate>> で詳しく説明されています。 ただし、Kubernetes で LINSTOR ノードを退避させる前に、追加のアクションを実行する必要があります。"

#. type: Plain text
msgid "First, prevent new Kubernetes workloads from being scheduled to the node and then move the node's workload to another node. You can do this by entering the following commands:"
msgstr "最初に、新しいKubernetesのワークロードがそのノードにスケジュールされないように防止し、その後にノードのワークロードを別のノードに移動します。次のコマンドを入力することでこれを行うことができます:"

#. type: delimited block -
#, no-wrap
msgid ""
"# kubectl cordon <node_name>\n"
"# kubectl drain --ignore-daemonsets <node_name>\n"
msgstr ""
"# kubectl cordon <node_name>\n"
"# kubectl drain --ignore-daemonsets <node_name>\n"

#. type: Plain text
msgid "After verifying that your cluster is running as expected, you can continue to follow the steps in <<s-linstor-node-evacuate>>."
msgstr "クラスターが期待通りに動作していることを確認した後、<<s-linstor-node-evacuate>>の手順に従って続行できます。"

#. type: Plain text
msgid "If you are planning on evacuating more than one node, enter the following command on all the nodes that you will be evacuating:"
msgstr "複数のノードの退避を計画している場合は、退避するすべてのノードで次のコマンドを入力します。"

#. type: delimited block -
#, no-wrap
msgid "# linstor node set-property n1.k8s-mwa.at.linbit.com AutoplaceTarget false\n"
msgstr "# linstor node set-property n1.k8s-mwa.at.linbit.com AutoplaceTarget false\n"

#. type: Plain text
msgid "This ensures that LINSTOR will not place resources from a node that you are evacuating onto another node that you plan on evacuating."
msgstr "これにより、退避しようとしているノードから、退避を計画している別のノードに LINSTOR がリソースを配置することがなくなります。"

#. type: Title ===
#, no-wrap
msgid "Deleting a LINSTOR Node in Kubernetes"
msgstr "KubernetesでのLINSTORノードの削除"

#. type: Plain text
msgid "Before you can delete a LINSTOR storage node from a Kubernetes cluster, the node must have no deployed LINSTOR resources or Kubernetes workloads. To remove resources and workloads from a node, you can follow the instructions in the <<s-kubernetes-evacuate-node>> section."
msgstr "KubernetesクラスタからLINSTORストレージノードを削除する前に、ノードにデプロイされたLINSTORリソースやKubernetesワークロードがない必要があります。ノードからリソースとワークロードを削除するには、<<s-kubernetes-evacuate-node>>セクションの手順に従うことができます。"

#. type: Plain text
msgid "Alternatively, let the LINSTOR Operator handle the node evacuation for you. To do this, you need to take a few preparatory steps."
msgstr "代わりに、LINSTOR Operatorにノードの避難を処理させても良いです。これを行うには、いくつかの準備手順を踏む必要があります。"

#. type: Plain text
msgid "First, use `kubectl` to apply a label, `marked-for-deletion` in the following example, to the node that you want to delete from your Kubernetes cluster."
msgstr "まず、次の例のように、Kubernetesクラスタから削除したいノードに`marked-for-deletion`のラベルを適用するために`kubectl`を使用してください。"

#. type: delimited block -
#, no-wrap
msgid "# kubectl label nodes <node-name> marked-for-deletion=\n"
msgstr "# kubectl label nodes <node-name> marked-for-deletion=\n"

#. type: Plain text
msgid "You must add an equals sign (`=`) to the end of your label name."
msgstr "あなたはラベル名の最後に等号（`=`）を追加する必要があります。"

#. type: Plain text
msgid "Next, configure the `linstorcluster` Kubernetes resource so that it does not deploy LINSTOR resources to any node with the label that you have applied. To do this, edit the resource and replace the contents with the lines shown in the following example, including the `...` line.  Change the `key` to match the label that you previously applied to your node."
msgstr "次に、適用したラベルを持つノードに LINSTOR リソースをデプロイしないように、`linstorcluster` Kubernetes リソースを構成します。これを行うには、リソースを編集し、以下の例に示す行を含む内容に置き換えます（`...` 行を含む）。`key` を、以前にノードに適用したラベルに一致するように変更してください。"

#. type: delimited block -
#, no-wrap
msgid ""
"# kubectl edit linstorclusters linstorcluster\n"
"...\n"
"spec:\n"
"  nodeAffinity:\n"
"    nodeSelectorTerms:\n"
"    - matchExpressions:\n"
"      - key: marked-for-deletion\n"
"        operator: DoesNotExist\n"
msgstr ""
"# kubectl edit linstorclusters linstorcluster\n"
"...\n"
"spec:\n"
"  nodeAffinity:\n"
"    nodeSelectorTerms:\n"
"    - matchExpressions:\n"
"      - key: marked-for-deletion\n"
"        operator: DoesNotExist\n"

#. type: Plain text
msgid "After taking these steps, the LINSTOR Operator will automatically delete the corresponding `LinstorSatellite` resource. That will stop the LINSTOR satellite service on that node, but only after LINSTOR runs a `node evacuate` operation and there are no more resources on the node.  Enter the following command to wait for the Operator to finish evacuating the node:"
msgstr "これらの手順を実行すると、LINSTOR Operator は自動的に対応する `LinstorSatellite` リソースを削除します。これにより、そのノード上の LINSTOR サテライト サービスが停止しますが、LINSTOR が `node evacuate` 操作を実行し、ノード上にリソースがもう存在しない場合にのみ停止します。次のコマンドを入力して、Operator がノードを避難させるのを待ちます:"

#. type: delimited block -
#, no-wrap
msgid "# kubectl wait linstorsatellite/<node> --for=condition=EvacuationCompleted\n"
msgstr "# kubectl wait linstorsatellite/<node> --for=condition=EvacuationCompleted\n"

#. type: Plain text
msgid "This can take some time. You can also use `kubectl describe linstorsatellite/<node>` to get a status update on the evacuation process."
msgstr "これには時間がかかることがあります。また、'kubectl describe linstorsatellite/<node>' を使用して、避難プロセスの状況の更新を取得することもできます。"

#. type: Plain text
msgid "After the Operator finishes evacuating the node, you can delete the node by entering the following command:"
msgstr "オペレーターがノードを避難させた後、以下のコマンドを入力してノードを削除できます:"

#. type: delimited block -
#, no-wrap
msgid "# kubectl delete node <node-name>\n"
msgstr "# kubectl delete node <node-name>\n"

#. type: Title ===
#, no-wrap
msgid "Monitoring With Prometheus"
msgstr "Prometheus による監視"

#. type: Plain text
msgid ""
"A LINSTOR deployment in Kubernetes offers the possibility of integrating with the link:https://github.com/prometheus-operator/kube-prometheus[Prometheus monitoring stack]. You can use https://prometheus.io/[Prometheus] to monitor LINSTOR and related components in Kubernetes. This monitoring solution is configurable to suit your purposes and includes features such as a Grafana dashboard for Prometheus scraped metrics, and the ability to set up alert rules and get "
"alerts for events."
msgstr "KubernetesでのLINSTORデプロイメントは、link:https://github.com/prometheus-operator/kube-prometheus[Prometheus monitoring stack]と統合する可能性を提供します。https://prometheus.io/[Prometheus] を使用して、Kubernetes内のLINSTORおよび関連コンポーネントを監視できます。この監視ソリューションは、あなたの目的に合わせて設定可能であり、Prometheusスクレイプメトリクス用のGrafanaダッシュボードやアラートルールの設定、イベントのアラートを受け取る機能などを含んでいます。"

#. type: Plain text
msgid "The Prometheus monitoring integration with LINSTOR configures:"
msgstr "Prometheusモニタリングは、LINSTORとの統合を設定:"

#. type: Plain text
msgid "Metrics scraping for the LINSTOR and DRBD state."
msgstr "LINSTORおよびDRBD状態のメトリクス収集。"

#. type: Plain text
msgid "Alerts based on the cluster state"
msgstr "クラスターの状態に基づくアラート"

#. type: Plain text
msgid "A Grafana dashboard"
msgstr "Grafanaダッシュボード"

#. type: Plain text
msgid "To configure monitoring for your LINSTOR in Kubernetes deployment, you should be familiar with:"
msgstr "LINSTORをKubernetesデプロイメント内でモニタリングするために、以下の点に精通している必要があります:"

#. type: Plain text
msgid "Deploying workloads in Kubernetes using link:https://helm.sh/[Helm]"
msgstr "Kubernetesでのワークロードのデプロイメントを行うには、<<https://helm.sh/>>[Helm]を使用します。"

#. type: Plain text
msgid "Deploying resources using link:https://kubernetes.io/docs/tasks/tools/[`kubectl`]"
msgstr "リンク: https://kubernetes.io/docs/tasks/tools/[`kubectl`]を使用してリソースをデプロイします。"

#. type: Title ====
#, no-wrap
msgid "Configuring Monitoring with Prometheus in Operator v2 Deployments"
msgstr "Operator v2 デプロイメントでのPrometheusを使用したモニタリングの設定"

#. type: Plain text
msgid "This section describes configuring monitoring with Prometheus for LINSTOR Operator v2 deployments in Kubernetes. If you need to configure monitoring with Prometheus for a LINSTOR Operator v1 deployment, refer to the instructions in the <<s-kubernetes-monitoring-v1,>> section."
msgstr "このセクションでは、LINSTOR Operator v2のKubernetesにおけるPrometheusを用いたモニタリングの設定方法について説明します。LINSTOR Operator v1デプロイメントの場合は、<<s-kubernetes-monitoring-v1,>>セクションの手順を参照してください。"

#. type: Title =====
#, no-wrap
msgid "Deploying the Prometheus Operator for LINSTOR Operator v2 Deployments"
msgstr "LINSTOR Operator v2 デプロイメント向けの Prometheus Operator のデプロイメント"

#. type: Plain text
msgid "If you already have a working Prometheus Operator deployment, skip the steps in this section."
msgstr "もしすでに運用中のプロメテウスオペレーターのデプロイメントがある場合は、このセクションの手順をスキップしてください。"

#. type: Plain text
msgid "To deploy the Prometheus monitoring integration in Kubernetes, you need to deploy the Prometheus Operator. A simple way to do this is to use the Helm chart provided by the Prometheus Community."
msgstr "KubernetesでPrometheusモニタリングインテグレーションをデプロイするには、Prometheus Operatorをデプロイする必要があります。これを行う簡単な方法は、Prometheusコミュニティが提供するHelmチャートを使用することです。"

#. type: Plain text
msgid "First, add the Helm chart repository to your local Helm configuration, by entering the following command:"
msgstr "最初に、以下のコマンドを入力して、HelmチャートリポジトリをローカルのHelm構成に追加してください:"

#. type: delimited block -
#, no-wrap
msgid "# helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n"
msgstr "# helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n"

#. type: Plain text
msgid "Then, deploy the link:https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack[`kube-prometheus-stack`] chart. This chart will set up Prometheus, Alertmanager, and Grafana for your cluster. Configure it to search for monitoring and alerting rules in all namespaces:"
msgstr "その後、リンクをデプロイしてください:https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack[`kube-prometheus-stack`] チャート。このチャートは、クラスターにPrometheus、Alertmanager、およびGrafanaをセットアップします。全ての名前空間で監視とアラートのルールを検索するように構成してください:"

#. type: delimited block -
#, no-wrap
msgid ""
"# helm install \\\n"
"--create-namespace -n monitoring prometheus prometheus-community/kube-prometheus-stack \\\n"
"--set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\\n"
"--set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \\\n"
"--set prometheus.prometheusSpec.ruleSelectorNilUsesHelmValues=false\n"
msgstr ""
"# helm install \\\n"
"--create-namespace -n monitoring prometheus prometheus-community/kube-prometheus-stack \\\n"
"--set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\\n"
"--set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \\\n"
"--set prometheus.prometheusSpec.ruleSelectorNilUsesHelmValues=false\n"

#. type: Plain text
msgid "By default, the deployment will only monitor resources in the `kube-system` and its own namespace. LINSTOR is usually deployed in a different namespace, `linbit-sds`, so you need to configure Prometheus to watch this namespace. In the example above, this is achieved by setting the various `*NilUsesHelmValues` parameters to false."
msgstr "デフォルトでは、デプロイメントは `kube-system` および独自のネームスペース内のリソースのみ監視します。LINSTOR は通常、異なるネームスペースで展開されるため、Prometheus にこのネームスペースを監視するよう構成する必要があります。上記の例では、異なる `*NilUsesHelmValues` パラメータを false に設定することでこれが達成されます。"

#. type: Title =====
#, no-wrap
msgid "Deploying Prometheus Monitoring and Alerting Rules for LINSTOR"
msgstr "LINSTOR用のPrometheus監視とアラートルールのデプロイメント"

#. type: Plain text
msgid "After creating a Prometheus Operator deployment and configuring it to watch all namespaces, apply the monitoring and alerting resources for LINSTOR. You can do this by either using the `kustomize` or `helm` utilities."
msgstr "Prometheus Operatorのデプロイメントを作成し、すべての名前空間を監視するように構成した後、LINSTOR用の監視およびアラートリソースを適用してください。これは、`kustomize`または`helm`ユーティリティを使用して行うことができます。"

#. type: Plain text
msgid "====== Using Kustomize to Deploy Prometheus Monitoring and Alerting Rules"
msgstr "====== デプロイメントにKustomizeを使用して、Prometheusの監視とアラートルールを展開する方法"

#. type: Plain text
msgid "To deploy Prometheus monitoring and alerting rules for LINSTOR by using the `kustomize` utility, you can apply a LINBIT GitHub-hosted `kustomization` configuration, by entering the following command:"
msgstr "`kustomize`ユーティリティを使用してLINSTOR用のPrometheus監視とアラートルールをデプロイするには、LINBITがGitHubでホストしている`kustomization`構成を適用できます。次のコマンドを入力してください："

#. type: delimited block -
#, no-wrap
msgid ""
"# kubectl apply -k \\\n"
"\"https://github.com/linbit/linstor-operator-builder//config/monitoring?ref=v2\"\n"
msgstr ""
"# kubectl apply -k \\\n"
"\"https://github.com/linbit/linstor-operator-builder//config/monitoring?ref=v2\"\n"

#. type: delimited block =
msgid "If you have configured SSL/TLS for your LINSTOR deployment in Kubernetes, you will need to apply a different version of the monitoring configuration. You can do this by entering the following command:"
msgstr "KubernetesでのLINSTORデプロイメントにSSL/TLSを設定している場合、監視構成の異なるバージョンを適用する必要があります。 これは、次のコマンドを入力することで行うことができます:"

#. type: delimited block -
#, no-wrap
msgid ""
"kubectl apply -k \\\n"
"\"https://github.com/linbit/linstor-operator-builder//config/monitoring-with-api-tls?ref=v2\"\n"
msgstr ""
"kubectl apply -k \\\n"
"\"https://github.com/linbit/linstor-operator-builder//config/monitoring-with-api-tls?ref=v2\"\n"

#. type: delimited block =
msgid "Output from applying the monitoring configuration to your deployment should show the following:"
msgstr "デプロイメントに監視構成を適用した結果、次の内容が表示されるはずです:"

#. type: delimited block -
#, no-wrap
msgid ""
"configmap/linbit-sds-dashboard created\n"
"podmonitor.monitoring.coreos.com/linstor-satellite created\n"
"prometheusrule.monitoring.coreos.com/linbit-sds created\n"
"servicemonitor.monitoring.coreos.com/linstor-controller created\n"
msgstr ""
"configmap/linbit-sds-dashboard created\n"
"podmonitor.monitoring.coreos.com/linstor-satellite created\n"
"prometheusrule.monitoring.coreos.com/linbit-sds created\n"
"servicemonitor.monitoring.coreos.com/linstor-controller created\n"

#. type: Plain text
msgid "====== Using Helm to Deploy Prometheus Monitoring and Alerting Rules"
msgstr "====== Helmを使用してPrometheusモニタリングおよびアラートルールをデプロイ"

#. type: Plain text
msgid "If you are using Helm, you can deploy Prometheus monitoring for your LINSTOR in Kubernetes deployment by enabling monitoring in the `linbit-sds` chart. The following command will install a new LINSTOR in Kubernetes deployment with Prometheus monitoring, or upgrade an existing Helm-installed deployment and enable Prometheus monitoring:"
msgstr "Helmを使用している場合、`linbit-sds`チャートで監視を有効にすることで、KubernetesデプロイメントにPrometheusモニタリングをデプロイできます。次のコマンドは、Prometheusモニタリング付きの新しいLINSTORをKubernetesデプロイメントにインストールし、または既存のHelmでインストールされたデプロイメントをアップグレードしてPrometheusモニタリングを有効にします:"

#. type: delimited block -
#, no-wrap
msgid ""
"# helm repo update linstor && \\\n"
"helm upgrade --install linbit-sds linstor/linbit-sds \\\n"
"--set monitoring.enabled=true\n"
msgstr ""
"# helm repo update linstor && \\\n"
"helm upgrade --install linbit-sds linstor/linbit-sds \\\n"
"--set monitoring.enabled=true\n"

#. type: Plain text
msgid "Unlike the `kustomize` deployment method, you can use the same `helm upgrade --install` command for a regular and an SSL/TLS-enabled LINSTOR deployment. The Helm deployment configures the correct endpoint automatically."
msgstr "`kustomize` デプロイメント方法とは異なり、通常のそしてSSL/TLS対応のLINSTORデプロイメントに対して同じ `helm upgrade --install` コマンドを使用することができます。Helmデプロイメントは自動で正しいエンドポイントを設定します。"

#. type: Plain text
msgid "Output from the command above should show that the `linbit-sds` chart was successfully deployed."
msgstr "上記コマンドの出力には、`linbit-sds` チャートが正常にデプロイされたことが表示されるはずです。"

#. type: Title =====
#, no-wrap
msgid "Verifying a Monitoring Deployment"
msgstr "監視デプロイメントの検証"

#. type: Plain text
msgid "You can verify that the monitoring configuration is working in your deployment by checking the local Prometheus, Alertmanager, and Grafana web consoles."
msgstr "デプロイメント内で監視構成が機能していることを確認するには、ローカルのPrometheus、Alertmanager、およびGrafanaのウェブコンソールをチェックすることができます。"

#. type: Plain text
msgid "====== Verifying the Prometheus Web Console Deployment"
msgstr "====== プロメテウスWebコンソールデプロイメントの検証"

#. type: Plain text
msgid "First, get access to the Prometheus web console from your local browser by forwarding the Prometheus web console service to local port 9090:"
msgstr "最初に、Prometheusウェブコンソールにアクセスするために、Prometheusウェブコンソールサービスをローカルポート9090に転送してください。:"

#. type: delimited block -
#, no-wrap
msgid "# kubectl port-forward -n monitoring services/prometheus-kube-prometheus-prometheus 9090:9090\n"
msgstr "# kubectl port-forward -n monitoring services/prometheus-kube-prometheus-prometheus 9090:9090\n"

#. type: Plain text
msgid "If you need to access the Prometheus instance from a system other than `localhost`, you might need to add the `--address 0.0.0.0` argument to the previous command."
msgstr "もし`localhost`以外のシステムからPrometheusインスタンスにアクセスする必要がある場合は、前述のコマンドに`--address 0.0.0.0`引数を追加する必要があるかもしれません。"

#. type: Plain text
msgid "Next, in a web browser, open `http://localhost:9090/graph` and display the `linstor_info` and `drbd_version` metrics, for example, by entering each metric into the search field, and clicking the Execute button."
msgstr "次に、Webブラウザで`http://localhost:9090/graph`を開き、`linstor_info`と`drbd_version`のメトリクスを表示してください。例えば、それぞれのメトリクスを検索フィールドに入力して、実行ボタンをクリックします。"

#. type: Block title
#, no-wrap
msgid "The `linstor_info` Prometheus metric"
msgstr "`linstor_info` Prometheusメトリック"

#. type: Positional ($1) AttributeList argument for macro 'image'
#, no-wrap
msgid "the `linstor_info` Prometheus metric"
msgstr "`linstor_info` Prometheusメトリック"

#. type: Target for macro image
#, no-wrap
msgid "images/linstor-k8s-prometheus-linstor_info-metic.png"
msgstr "images/linstor-k8s-prometheus-linstor_info-metic.png"

#. type: Block title
#, no-wrap
msgid "The `drbd_version` Prometheus metric"
msgstr "`drbd_version` Prometheus メトリック"

#. type: Positional ($1) AttributeList argument for macro 'image'
#, no-wrap
msgid "the `drbd_version` Prometheus metric"
msgstr "the `drbd_version` Prometheus メトリック"

#. type: Target for macro image
#, no-wrap
msgid "images/linstor-k8s-prometheus-drbd_version-metic.png"
msgstr "images/linstor-k8s-prometheus-drbd_version-metic.png"

#. type: Plain text
msgid "====== Verifying the Prometheus Alertmanager Web Console Deployment"
msgstr "====== プロメテウス・アラートマネージャーのWebコンソールデプロイメントを検証"

#. type: Plain text
msgid "To view the Alertmanager console, forward the Prometheus Alertmanager service to local port 9093, by entering the following command:"
msgstr "以下のコマンドを入力して、Prometheus Alertmanagerサービスをローカルポート9093にフォワードし、Alertmanagerコンソールを表示:"

#. type: delimited block -
#, no-wrap
msgid "# kubectl port-forward -n monitoring services/prometheus-kube-prometheus-alertmanager 9093:9093\n"
msgstr "# kubectl port-forward -n monitoring services/prometheus-kube-prometheus-alertmanager 9093:9093\n"

#. type: Plain text
msgid "If you need to access the Prometheus Alertmanager instance from a system other than `localhost`, you might need to add the `--address 0.0.0.0` argument to the previous command."
msgstr "Prometheus Alertmanagerインスタンスに`localhost`以外からアクセスする必要がある場合は、前のコマンドに`--address 0.0.0.0`引数を追加する必要があるかもしれません。"

#. type: Plain text
msgid "Next, in a web browser, open `http://localhost:9093`. The Alertmanager console should show an itemized list of alert groups for your deployment, including an alert group for the `linbit-sds` namespace."
msgstr "次に、ウェブブラウザーで`http://localhost:9093`を開いてください。Alertmanagerコンソールでは、デプロイメント用のアラートグループの項目付きリストが表示されます。その中には`linbit-sds`ネームスペース用のアラートグループも含まれています。"

#. type: Plain text
msgid ""
"You can verify that an alert will fire and be shown in your Alertmanager console by running a command that will cause an event applicable to an alert that you want to test. For example, you can disconnect a DRBD resource. This will cause a `drbdResourceSuspended` alert. To test this, assuming that you have a LINSTOR satellite pod named `kube-0` that is running in the `linbit-sds` namespace and that the satellite is in a secondary role for a DRBD resource named `my-"
"res`, enter the following command:"
msgstr ""
"アラートが発生し、Alertmanagerコンソールに表示されることを確認できます。テストしたいアラートに適用可能なイベントを引き起こすコマンドを実行することで。たとえば、DRBDリソースを切断することができます。これにより、`drbdResourceSuspended`アラートが発生します。これをテストするために、`linbit-sds`ネームスペースで実行されている名前が`kube-0`であるLINSTORサテライトポッドを持っていると仮定し、サテライトが`my-res`というDRBDリソースのセカンダリロールにある場合、次のコマンドを"
"入力してください："

#. type: delimited block -
#, no-wrap
msgid "# kubectl exec -it -n linbit-sds kube-0 -- drbdadm disconnect --force my-res\n"
msgstr "# kubectl exec -it -n linbit-sds kube-0 -- drbdadm disconnect --force my-res\n"

#. type: Plain text
msgid "Use caution with the types of events that you use to test alerts, especially on a production system. The above `drbdadm disconnect` command should be safe if you run it on a satellite node pod that is in a secondary role for the resource you disconnect. You can verify the resource role state that the satellite node pod is in by using a `drbdadm status <resource-name>` command."
msgstr "アラートをテストする際に使用するイベントのタイプには注意してください、特に本番システムで。上記の `drbdadm disconnect` コマンドは、切断するリソースのセカンダリロールを持つサテライトノードポッドで実行する場合は安全です。サテライトノードポッドがどのリソースロール状態にあるかは、`drbdadm status <resource-name>` コマンドを使用して確認できます。"

#. type: Block title
#, no-wrap
msgid "Alerts for the `my-res` DRBD resource in a disconnected state"
msgstr "`my-res` DRBDリソースが切断状態にある際のアラート"

#. type: Positional ($1) AttributeList argument for macro 'image'
#, no-wrap
msgid "alerts for the `my-res` DRBD resource in a disconnected state"
msgstr "`my-res` DRBD リソースが切断状態にある際のアラート"

#. type: Target for macro image
#, no-wrap
msgid "images/linstor-k8s-prometheus-alertmanager-drbdconnectionnotconnected.png"
msgstr "images/linstor-k8s-prometheus-alertmanager-drbdconnectionnotconnected.png"

#. type: Plain text
msgid "After verifying that the Prometheus Alertmanager web console shows new alerts that relate to the event you caused, you should revert your deployment to its previous state. To do this for the previous `drbdadm disconnect` example, enter the following command:"
msgstr "Prometheus Alertmanagerのウェブコンソールが、あなたが引き起こしたイベントに関連する新しいアラートを表示していることを確認した後、デプロイメントを以前の状態に戻す必要があります。前述の`drbdadm disconnect`の例については、次のコマンドを入力してください:"

#. type: delimited block -
#, no-wrap
msgid "# kubectl exec -it -n linbit-sds kube-0 -- drbdadm connect my-res\n"
msgstr "# kubectl exec -it -n linbit-sds kube-0 -- drbdadm connect my-res\n"

#. type: Plain text
msgid "You can verify that the `kube-0` satellite node pod is again connected to the `my-res` DRBD resource by entering a `drbdadm status` command:"
msgstr "`kube-0` サテライトノードポッドが `my-res` DRBD リソースに再度接続されていることを、`drbdadm status` コマンドを入力して確認できます:"

#. type: delimited block -
#, no-wrap
msgid "# kubectl exec -it -n linbit-sds kube-0 -- drbdadm status my-res\n"
msgstr "# kubectl exec -it -n linbit-sds kube-0 -- drbdadm status my-res\n"

#. type: Plain text
msgid "Output from the command should show that the resource is in an up-to-date state on all diskful and connected nodes."
msgstr "コマンドの出力は、リソースがディスクがいっぱいで、かつ接続されたすべてのノードで最新の状態であることを示す必要があります。"

#. type: delimited block -
#, no-wrap
msgid ""
"my-res role:Secondary\n"
"  disk:UpToDate\n"
"  kube-1 role:Secondary\n"
"    peer-disk:Diskless\n"
"  kube-2 role:Secondary\n"
"    peer-disk:UpToDate\n"
msgstr ""
"my-res role:Secondary\n"
"  disk:UpToDate\n"
"  kube-1 role:Secondary\n"
"    peer-disk:Diskless\n"
"  kube-2 role:Secondary\n"
"    peer-disk:UpToDate\n"

#. type: Plain text
msgid "====== Verifying the Grafana Web Console and LINBIT SDS Dashboard Deployment"
msgstr "====== Grafana WebコンソールとLINBIT SDSダッシュボードのデプロイメントを検証"

#. type: Plain text
msgid "To view the Grafana console and LINBIT SDS dashboard, first forward the Grafana service to local port 3000, by entering the following command:"
msgstr "GrafanaコンソールとLINBIT SDSダッシュボードを表示するには、まずGrafanaサービスをローカルポート3000に転送し、次のコマンドを入力します:"

#. type: delimited block -
#, no-wrap
msgid "# kubectl port-forward -n monitoring services/prometheus-grafana 3000:http-web\n"
msgstr "# kubectl port-forward -n monitoring services/prometheus-grafana 3000:http-web\n"

#. type: Plain text
msgid "If you need to access the Grafana instance from a system other than the `localhost`, you might need to add the `--address 0.0.0.0` argument to the previous command."
msgstr "`localhost`以外のシステムからGrafanaインスタンスにアクセスする必要がある場合は、前のコマンドに`--address 0.0.0.0`引数を追加する必要があります。"

#. type: Plain text
msgid "Next, in a web browser, open `http://localhost:3000` and log in. If you are using the example deployment from above, enter username `admin` and password `prom-operator` to log in to the Grafana instance. After logging in, change your password (`http://192.168.121.20:3000/profile/password`) to something other than the default password."
msgstr "次に、Webブラウザを開いて`http://localhost:3000`にアクセスしてログインしてください。もし前述の例でデプロイメントを使用している場合は、ユーザー名を`admin`、パスワードを`prom-operator`としてGrafanaインスタンスにログインしてください。ログインしたら、デフォルトパスワード以外のパスワードに変更してください（`http://192.168.121.20:3000/profile/password`）。"

#. type: Plain text
msgid "Select \"LINBIT SDS\" from the available dashboards (`http://localhost:3000/dashboards`) to show an overview of the health status of your LINSTOR deployment, including various metrics and statistics."
msgstr "提供されているダッシュボード (`http://localhost:3000/dashboards`) から \"LINBIT SDS\" を選択し、LINSTORデプロイメントの健康状態の概要、さまざまなメトリクスや統計を表示してください。"

#. type: Block title
#, no-wrap
msgid "The LINBIT SDS Grafana dashboard"
msgstr "The LINBIT SDS Grafana ダッシュボード"

#. type: Positional ($1) AttributeList argument for macro 'image'
#, no-wrap
msgid "the LINBIT SDS Grafana dashboard"
msgstr "LINBIT SDS Grafana ダッシュボード"

#. type: Target for macro image
#, no-wrap
msgid "images/linstor-k8s-linbit-sds-grafana-dashboard.png"
msgstr "images/linstor-k8s-linbit-sds-grafana-dashboard.png"

#. type: Title ====
#, no-wrap
msgid "Monitoring with Prometheus in Operator v1 Deployments"
msgstr "Operator v1 デプロイメントでの Prometheusによるモニタリング"

#. type: Plain text
msgid "In Operator v1 deployments, the operator will set up monitoring containers along the existing components and make them available as a `Service`."
msgstr "Operator v1デプロイメントでは、オペレーターは既存のコンポーネントに沿ってモニタリングコンテナをセットアップし、それらを `Service` として利用可能にします。"

#. type: Plain text
msgid "If you use the https://prometheus-operator.dev/[Prometheus Operator], the LINSTOR Operator will also set up the `ServiceMonitor` instances. The metrics will automatically be collected by the Prometheus instance associated to the operator, assuming https://prometheus-operator.dev/docs/kube/monitoring-other-namespaces/[watching the Piraeus namespace is enabled]."
msgstr "https://prometheus-operator.dev/[Prometheus Operator] を使用する場合、Linstor オペレーターは `ServiceMonitor` インスタンスも設定します。 https://prometheus-operator.dev/docs/kube/monitoring-other-namespaces/[Piraeus 名前空間が有効] と仮定して、メトリックはオペレーターに関連付けられた Prometheus インスタンスによって自動的に収集されます。"

#. type: Plain text
msgid "To disable exporting of metrics, set `operator.satelliteSet.monitoringImage` to an empty value."
msgstr "メトリックのエクスポートを無効にするには `operator.satelliteSet.monitoringImage` を空の値に設定します。"

#. type: Title =====
#, no-wrap
msgid "LINSTOR Controller Monitoring in Operator v1 Deployments"
msgstr "Operator v1 デプロイメントにおける LINSTOR コントローラーのモニタリング"

#. type: Plain text
msgid "The LINSTOR controller exports cluster-wide metrics. Metrics are exported on the existing controller service, using the path https://linbit.com/drbd-user-guide/linstor-guide-1_0-en/#s-linstor-monitoring[`/metrics`]."
msgstr "LINSTORコントローラーはクラスタ全体のメトリクスをエクスポートします。 メトリクスは既存のコントローラーサービスでエクスポートされ、パスhttps://linbit.com/drbd-user-guide/linstor-guide-1_0-en/#s-linstor-monitoring[`/metrics`]を使用しています。"

#. type: Title =====
#, no-wrap
msgid "DRBD Resource Monitoring in Operator v1 Deployments"
msgstr "Operator v1 デプロイでの DRBD リソースの監視"

#. type: Plain text
msgid "All satellites are bundled with a secondary container that uses https://github.com/LINBIT/drbd-reactor/[`drbd-reactor`] to export metrics directly from DRBD. The metrics are available on port 9942, for convenience a headless service named `<linstorsatelliteset-name>-monitoring` is provided."
msgstr "すべてのサテライトは https://github.com/LINBIT/drbd-reactor/[`drbd-reactor`] を使用して DRBD から直接メトリックをエクスポートするセカンダリコンテナにバンドルされています。メトリックはポート 9942 で、ヘッドレスサービス `<linstorsatelliteset-name>-monitoring` という名前で提供されています。"

#. type: Plain text
msgid "If you want to disable the monitoring container, set `monitoringImage` to `\"\"` in your `LinstorSatelliteSet` resource."
msgstr "`LinstorSatelliteSet` リソース内で、監視コンテナを無効にする場合は、`monitoringImage` を `\\\"\\\"` に設定してください。"

#~ msgid "The Operator itself is installed using a Helm v3 chart as follows:"
#~ msgstr "オペレーター自体は、次のように Helm v3 チャートを使用してインストールされます。"

#~ msgid "LINBIT's container image repository (http://drbd.io), used in the `kubectl create` command below, is only available to LINBIT customers or through LINBIT customer trial accounts.  link:https://linbit.com/contact-us/[Contact LINBIT for information on pricing or to begin a trial]. Alternatively, you may use LINSTOR SDS' upstream project named link:https://github.com/piraeusdatastore/piraeus-operator[Piraeus], without being a LINBIT customer."
#~ msgstr "以下の `kubectl create` コマンドで使用される LINBIT のコンテナーイメージリポジトリ (http://drbd.io) は、LINBIT のお客様または LINBIT のお客様の試用アカウントを通じてのみ利用できます。価格についての情報や試用開始するには link:https://linbit.com/contact-us/[こちら] を参照ください。また、LINSTOR SDS のアップストリームプロジェクト link:https://github.com/piraeusdatastore/piraeus-operator[Piraeus] は LINBIT の顧客ではなくてもを使用できます。"

#~ msgid "Create a Kubernetes secret containing your my.linbit.com credentials:"
#~ msgstr "my.linbit.com 認証情報を含む kubernetes シークレットを作成します。"

#~ msgid "The name of this secret must match the one specified in the Helm values, by default `drbdiocred`."
#~ msgstr "このシークレットの名前は、Helm で指定されたものと一致する必要があります。デフォルトは `drbdiocred` です。"

#~ msgid "Configure the LINSTOR database back end. By default, the chart configures etcd as database back end. The Operator can also configure LINSTOR to use <<s-kubernetes-linstor-k8s-backend,Kubernetes as datastore>> directly. If you go the etcd route, you should configure persistent storage for it:"
#~ msgstr "LINSTORデータベースのバックエンドを設定します。デフォルトでは、チャートはデータベースとしてetcdを設定します。Operator は、LINSTORが <<s-kubernetes-linstor-k8s-backend,Kubernetes as datastore>> を直接使用するように設定することも可能です。etcd を使う場合は、etcd 用に永続的ストレージを設定します。"

#~ msgid "Use an existing storage provisioner with a default `StorageClass`."
#~ msgstr "デフォルトの `StorageClass` で既存のストレージプロビジョナーを使用する。"

#~ msgid "<<s-kubernetes-etcd-hostpath-persistence,Use `hostPath` volumes>>."
#~ msgstr "<<s-kubernetes-etcd-hostpath-persistence, `hostPath` ボリューム>> を使用する。"

#~ msgid "Disable persistence, **for basic testing only**. This can be done by adding `--set etcd.persistentVolume.enabled=false` to the `helm install` command below."
#~ msgstr "基本的なテストのみで永続化を無効にします。これは以下の `helm install` コマンドに `--set etcd.persistentVolume.enabled=false` を追加することで実行できます。"

#~ msgid "Read <<s-kubernetes-storage, the storage guide>> and configure a basic storage setup for LINSTOR"
#~ msgstr "<<s-kubernetes-storage, ストレージの構成>> を確認し、LINSTOR の基本的なストレージを構成してください。"

#~ msgid "Read the <<s-kubernetes-securing-deployment,section on securing the deployment>> and configure as needed."
#~ msgstr "<<s-kubernetes-securing-deployment,デプロイメントの保護に関するセクション>> を参照して、必要に応じて設定します。"

#~ msgid "Select the appropriate kernel module injector using `--set` with the `helm install` command in the final step."
#~ msgstr "最後のステップとして `helm install` コマンドで `--set` を使用して、適切なカーネルモジュールインジェクタを選択します。"

#~ msgid "Choose the injector according to the distribution you are using. Select the latest version from one of `drbd9-rhel7`, `drbd9-rhel8`,...  from http://drbd.io/ as appropriate. The drbd9-rhel8 image should also be used for RHCOS (OpenShift). For the SUSE CaaS Platform use the SLES injector that matches the base system of the CaaS Platform you are using (e.g., `drbd9-sles15sp1`). For example:"
#~ msgstr "使用しているディストリビューションに応じてインジェクターを選択してください。 http://drbd.io/ から、 `drbd9-rhel7`, `drbd9-rhel8`, ... 等の最新バージョンを適宜選択します。drbd9-rhel8 イメージは、RHCOS(OpenShift) でも使用します。SUSE CaaS プラットフォームの場合、使用している CaaS プラットフォームのシステムと一致する SLES インジェクターを使用します（ `drbd9-sles15sp1` など）。例えば以下のようにします。"

#, no-wrap
#~ msgid "k8s-backend.yaml"
#~ msgstr "k8s-backend.yaml"

#~ msgid "It is **NOT** possible to migrate from an existing cluster with etcd back end to the Kubernetes back end."
#~ msgstr "現在、etcd バックエンドを持つ既存のクラスタから Kubernetes バックエンドへの移行できません。"

#~ msgid "Create the `hostPath` persistent volumes, substituting cluster node names accordingly in the `nodes=` option:"
#~ msgstr "`nodePath =` オプションでクラスターノード名を指定して `hostPath` 永続ボリュームを作成します。"

#~ msgid "By default, a PV is created on every `control-plane` node. You can manually select the storage nodes by passing `--set \"nodes={<NODE0>,<NODE1>,<NODE2>}\"` to the install command."
#~ msgstr "デフォルトで PV はすべての `control-plane` ノードに作成されます。インストールコマンドに `--set \"nodes={<NODE0>,<NODE1>,NODE2>}\"` を渡すことにより、ストレージノードを手動で選択できます。"

#, no-wrap
#~ msgid "Configuring Storage"
#~ msgstr "ストレージの構成"

#~ msgid "First create a file with the storage configuration like:"
#~ msgstr "まず、次のようなストレージ構成でファイルを作成します。"

#~ msgid "Are a root device (no partition)"
#~ msgstr "ルートデバイス（パーティションなし）"

#~ msgid "do not contain partition information"
#~ msgstr "パーティション情報を含まない。"

#~ msgid "have more than 1 GiB"
#~ msgstr "1 GiB 以上である。"

#~ msgid "To enable automatic configuration of devices, set the `devicePaths` key on `storagePools` entries:"
#~ msgstr "デバイスの自動構成を有効にするには `storagePools` エントリに `devicePaths` キーを設定します。"

#~ msgid "The available keys for `lvmPools` entries are:"
#~ msgstr "`lvmPools` エントリで利用可能なキーは以下のとおりです。"

#~ msgid "`name` name of the LINSTOR storage pool. [Required]"
#~ msgstr "`name` LINSTOR ストレージプールの名前。必須"

#~ msgid "`volumeGroup` name of the VG to create. [Required]"
#~ msgstr "`volumeGroup` 作成する VG の名前。必須"

#~ msgid "`devicePaths` devices to configure for this pool. Must be empty and >= 1GiB to be recognized. [Optional]"
#~ msgstr "`devicePaths` このプール用に構成するデバイス。認識されるには、空で 1GiB 以上ある。オプション"

#~ msgid "`raidLevel` LVM raid level. [Optional]"
#~ msgstr "`raidLevel` LVM RAID レベル。オプション"

#~ msgid "`vdo` Enable [VDO] (requires VDO tools in the satellite). [Optional]"
#~ msgstr "`vdo` [VDO]を有効にする（サテライトに VDO ツールが必要）。オプション"

#~ msgid "`vdoLogicalSizeKib` Size of the created VG (expected to be bigger than the backing devices by using VDO). [Optional]"
#~ msgstr "`vdoLogicalSizeKib` 作成された VG のサイズ（VDO を使用することで下位デバイスよりも大きくなることが予想される）。オプション"

#~ msgid "`vdoSlabSizeKib` Slab size for VDO. [Optional]"
#~ msgstr "`vdoSlabSizeKib` VDO のスラブサイズ。オプション"

#~ msgid "[VDO]: https://www.redhat.com/en/blog/look-vdo-new-linux-compression-layer"
#~ msgstr "[VDO]: https://www.redhat.com/en/blog/look-vdo-new-linux-compression-layer"

#~ msgid "The volume group created by LINSTOR for LVM thin pools will always follow the scheme \"linstor_$THINPOOL\"."
#~ msgstr "LVMTHIN プール用に LINSTOR によって作成されたボリュームグループは、常にスキーム \"linstor_$THINPOOL\" に従います。"

#~ msgid "`zPool` name of the zpool to use. Must already be present on all machines. [Required]"
#~ msgstr "`zPool` 使用する zpool の名前。すべてのマシンにすでに存在している必要があります。必須"

#~ msgid "`thin` `true` to use thin provisioning, `false` otherwise. [Required]"
#~ msgstr "`thin` シンプロビジョニングを使用するには `true`, 使用しないなら `false` を設定する。必須"

#~ msgid "The possible values for `operator.satelliteSet.automaticStorageType`:"
#~ msgstr "`operator.satelliteSet.automaticStorageType` の可能な値は次のとおりです。"

#~ msgid "`None` no automatic set up (default)"
#~ msgstr "`None` 自動セットアップなし（デフォルト）"

#~ msgid "`LVM` create a LVM (thick) storage pool"
#~ msgstr "`LVM` LVM（シック）ストレージプールを作成"

#~ msgid "`LVMTHIN` create a LVM thin storage pool"
#~ msgstr "`LVMTHIN` LVM thin ストレージプールを作成"

#~ msgid "`ZFS` create a ZFS based storage pool (**UNTESTED**)"
#~ msgstr "`ZFS` ZFS ベースストレージプールを作成 (** 未テスト **)"

#~ msgid "The secret can then be passed to the controller by passing the following argument to `helm install`"
#~ msgstr "次の引数を `helm install` に渡すことで、シークレットをコントローラーに渡すことができます。"

#~ msgid "If this option is active, the secret specified in the above section must contain two additional keys:"
#~ msgstr "このオプションが有効な場合、上記のセクションで指定されたシークレットには、2つの追加キーが含まれている必要があります。"

#~ msgid "`client.cert` PEM formatted certificate presented to `etcd` for authentication"
#~ msgstr " `client.cert` 認証のために `etcd` に提示される PEM 形式の証明書"

#~ msgid "`client.key` private key **in PKCS8 format**, matching the above client certificate."
#~ msgstr "`client.key` 上記のクライアント証明書と一致する PKCS8 形式の秘密鍵"

#~ msgid "Keys can be converted into PKCS8 format using `openssl`:"
#~ msgstr "鍵は `openssl` を使用して PKCS8 形式に変換できます。"

#~ msgid "Create kubernetes secrets that can be passed to the controller and node pods:"
#~ msgstr "コントローラとノードポッドに渡すことができるkubernetes シークレットを作成します。"

#~ msgid "If you want to use a custom passphrase, store it in a secret:"
#~ msgstr "カスタムパスフレーズを使用する場合は、シークレットに保存します。"

#~ msgid "This is not suggested for any use outside of testing."
#~ msgstr "警告：これは、テスト以外での使用は推奨されていません。"

#~ msgid "Install with LINSTOR storage-pools defined at install through `sp-values.yaml`, persistent hostPath volumes, 3 etcd replicas, and by compiling the DRBD kernel modules for the host kernels."
#~ msgstr "`sp-values.yaml` で定義された LINSTOR storage-pools, 永続的な hostPath ボリューム, 3 つの etcd レプリカ, ホストカーネル用にコンパイルされた DRBD カーネルモジュールを使用してインストールします。"

#~ msgid "This should be adequate for most basic deployments. Please note that this deployment is not using the pre-compiled DRBD kernel modules just to make this command more portable. Using the pre-compiled binaries will make for a much faster install and deployment. Using the `Compile` option would not be suggested for use in a large Kubernetes clusters."
#~ msgstr "これは、ほとんどの基本的なデプロイメントに適しています。このデプロイメントでは、コマンドの移植性を高めるために、コンパイル済みの DRBD カーネルモジュールを使用していないことに注意してください。コンパイル済みのバイナリを使用すると、インストールとデプロイメントは速くなります。大規模な Kubernetes クラスターでの使用には、 `Compile`  オプションの使用は推奨されません。"

#~ msgid "Install with LINSTOR storage-pools defined at install through `sp-values.yaml`, use an already created PostgreSQL DB (preferably clustered), rather than etcd, and use already compiled kernel modules for DRBD."
#~ msgstr "`sp-values.yaml` で定義された LINSTOR storage-pools, etcd の代わりに作成済みの Postgres DB（できればクラスター化）, DRBD 用にコンパイル済みのカーネルモジュールを使用してインストールします。"

#~ msgid "To protect the storage infrastructure of the cluster from accidentally deleting vital components, it is necessary to perform some manual steps before deleting a Helm deployment."
#~ msgstr "重要なコンポーネントを誤って削除しないようにクラスターのストレージインフラストラクチャを保護するには、Helm デプロイメントを削除する前にいくつかの手動手順を実行します。"

#~ msgid "These volumes, once deleted, cannot be recovered."
#~ msgstr "これらのボリュームは、いったん削除されると復元できません。"

#~ msgid "Delete the Helm deployment."
#~ msgstr "Helm デプロイメントの終了"

#~ msgid "If you removed all PVCs and all LINSTOR pods have terminated, you can uninstall the Helm deployment"
#~ msgstr "すべての PVC を削除し、すべての LINSTOR ポッドが終了した場合、helm デプロイメントをアンインストールできます。"

#~ msgid "The Helm charts provide a set of further customization options for advanced use cases."
#~ msgstr "Helm チャートは、高度なユースケースのために、さらなるカスタマイズオプションのセットを提供します。"

#~ msgid "Sets the pull policy for all images."
#~ msgstr "すべてのイメージのプルポリシーを設定します。"

#~ msgid "Controls the number of replicas for each component."
#~ msgstr "各コンポーネントのレプリカ数を制御します。"

#~ msgid "Set container resource requests and limits. See https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/[the kubernetes docs]."
#~ msgstr "コンテナリソースのリクエストと制限を設定します。詳細は https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/[kubernetes ドキュメント] を参照ください。"

#~ msgid "Most containers need a minimal amount of resources, except for:"
#~ msgstr "以下を除いて、ほとんどのコンテナは、最小限のリソースしか必要としません。"

#~ msgid "`etcd.resources` See the https://etcd.io/docs/v3.4.0/op-guide/hardware/[etcd docs]"
#~ msgstr "`etcd.resources` https://etcd.io/docs/v3.4.0/op-guide/hardware/[etcd docs] を参照"

#~ msgid "`operator.controller.resources` Around `700MiB` memory is required"
#~ msgstr "`operator.controller.resources` 約 `700MiB` のメモリが必要"

#~ msgid "`operater.satelliteSet.resources` Around `700MiB` memory is required"
#~ msgstr "`operater.satelliteSet.resources` 約 `700MiB` のメモリが必要"

#~ msgid "`operator.satelliteSet.kernelModuleInjectionResources` If kernel modules are compiled, 1GiB of memory is required."
#~ msgstr "`operator.satelliteSet.kernelModuleInjectionResources` カーネルモジュールをコンパイルする場合、1GiBのメモリが必要"

#~ msgid "Affinity and toleration determine where pods are scheduled on the cluster. See the https://kubernetes.io/docs/concepts/scheduling-eviction/[kubernetes docs on affinity and toleration].  This may be especially important for the `operator.satelliteSet` and `csi.node*` values. To schedule a pod using a LINSTOR persistent volume, the node requires a running LINSTOR satellite and LINSTOR CSI pod."
#~ msgstr "AffinityとTolerationは、クラスタ上のどこにポッドがスケジュールされるかを決定します。詳細は https://kubernetes.io/docs/concepts/scheduling-eviction/[kubernetes docs on affinity and toleration] を参照ください。これは `operator.satelliteSet` と `csi.node*` の値に対して特に重要かもしれません。LINSTOR 永続ボリュームを使用してポッドをスケジュールするには、そのノードで実行中の LINSTOR satellite と LINSTOR CSI ポッドが必要です。"

#~ msgid "Sets additional environments variables to pass to the LINSTOR Controller and Satellites.  Uses the same format as https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/[the `env` value of a container]"
#~ msgstr "コントローラとサテライトポッドに渡す kubernetes シークレットを作成します。Linstor コントローラーとサテライトに渡す追加の環境変数を設定します。 https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/[コンテナ環境変数] と同じフォーマットを使用します。"

#~ msgid "Sets additional properties on the LINSTOR Controller. Expects a simple mapping of `<property-key>: <value>`."
#~ msgstr "Linstor コントローラーに追加のプロパティを設定します。単純な `<property-key>: <value>` マッピングを使用します。"

#~ msgid "Kubelet expects every CSI plug-in to mount volumes under a specific subdirectory of its own state directory. By default, this state directory is `/var/lib/kubelet`. Some Kubernetes distributions use a different directory:"
#~ msgstr "Kubelet は、すべての CSI プラグインが独自の状態ディレクトリの特定のサブディレクトリにボリュームをマウントすることを想定しています。デフォルトでは、この状態ディレクトリは `/var/lib/kubelet` です。一部の Kubernetes ディストリビューションは異なるディレクトリを使用します。"

#~ msgid "microk8s: `/var/snap/microk8s/common/var/lib/kubelet`"
#~ msgstr "microk8s: `/var/snap/microk8s/common/var/lib/kubelet`"

#~ msgid "Directory on the host that is required for building kernel modules. Only needed if using the `Compile` injection method. Defaults to `/usr/src`, which is where the actual kernel sources are stored on most distributions. Use `\"none\"` to not mount any additional directories."
#~ msgstr "カーネルモジュールのビルドに必要な、ホスト上のディレクトリ。Compile` インジェクションメソッドを使用する場合のみ必要です。デフォルトは `/usr/src` で、ほとんどのディストリビューションで実際のカーネルソースがここに格納されています。追加のディレクトリをマウントしない場合は、 `\"none\"` を使用します。"

#~ msgid "Set the number of worker threads used by the CSI driver. Higher values put more load on the LINSTOR Controller, which may lead to instability when creating many volumes at once."
#~ msgstr "CSIドライバによって使用されるワーカスレッドの数を設定します。より高い値はLINSTORコントローラに多くの負荷を与え、一度に多くのボリュームを作成する際に不安定になる可能性があります。"

#~ msgid "If set to true, the satellite containers will have the following files and directories mounted from the host OS:"
#~ msgstr "true に設定すると、サテライトコンテナーには、ホスト OS からマウントされた次のファイルとディレクトリが含まれます。"

#~ msgid "`/etc/drbd/drbd.conf` (file)"
#~ msgstr "`/etc/drbd/drbd.conf` (ファイル)"

#~ msgid "`/etc/drbd.d` (directory)"
#~ msgstr "`/etc/drbd.d` (ディレクトリ)"

#~ msgid "`/var/lib/drbd` (directory)"
#~ msgstr "`/var/lib/drbd` (ディレクトリ)"

#~ msgid "`/var/lib/linstor.d` (directory)"
#~ msgstr "`/var/lib/linstor.d` (ディレクトリ)"

#~ msgid "All files and directories must already exist on the host."
#~ msgstr "すべてのファイルとディレクトリは、ホスト上にすでに存在している必要があります。"

#~ msgid "To skip the creation of a LINSTOR Controller deployment and configure the other components to use your existing LINSTOR Controller, use the following options when running `helm install`:"
#~ msgstr "LINSTOR コントローラーデプロイメントの作成をスキップし、既存の LINSTOR コントローラーを使用するように他のコンポーネントを構成するには、 `helm install` を実行するときに次のオプションを使用します。"

#~ msgid "`operator.controller.enabled=false` This disables creation of the `LinstorController` resource"
#~ msgstr "`operator.controller.enabled=false` これにより `LinstorController` リソースを作成しなくなります。"

#~ msgid "`operator.etcd.enabled=false` Since no LINSTOR Controller will run on Kubernetes, no database is required."
#~ msgstr "`operator.etcd.enabled=false` Kubernetes では LINSTOR コントローラーが実行されないため、データベースは必要ありません。"

#~ msgid "`controllerEndpoint=<url-of-linstor-controller>` The HTTP endpoint of the existing LINSTOR Controller. For example: `http://linstor.storage.cluster:3370/`"
#~ msgstr "`controllerEndpoint=<url-of-linstor-controller>` 既存の LINSTOR コントローラーの HTTP エンドポイント。 例えば `http://linstor.storage.cluster:3370/` "

#~ msgid "After all pods are ready, you should see the Kubernetes cluster nodes as satellites in your LINSTOR setup."
#~ msgstr "すべての pod の準備が整うと、既存の LINSTOR 環境で Kubernetes クラスターノードがサテライトとして表示されます。"

#~ msgid "Your kubernetes nodes must be reachable using their IP by the controller and storage nodes."
#~ msgstr "kubernetes ノードは、コントローラーとストレージノードが IP を使用して到達可能である必要があります。"

#~ msgid "Create a storage class referencing an existing storage pool on your storage nodes."
#~ msgstr "ストレージノードで既存のストレージプールを参照するストレージクラスを作成します。"

#, no-wrap
#~ msgid "Deploying with the Piraeus Operator"
#~ msgstr "Piraeus オペレーターを使用したデプロイメント"

#~ msgid "The community supported edition of the LINSTOR deployment in Kubernetes is called Piraeus. The Piraeus project provides https://github.com/piraeusdatastore/piraeus-operator[an operator] for deployment."
#~ msgstr "Kubernetes でコミュニティがサポートする LINSTOR デプロイメントのエディションは、Piraeus と呼ばれます。Piraeus プロジェクトに関しては https://github.com/piraeusdatastore/piraeus-operator[オペレータ] を参照ください。"

#~ msgid "For a convenient shortcut to the above command, download https://github.com/piraeusdatastore/kubectl-linstor/releases[`kubectl-linstor`] and install it alongside `kubectl`. Then you can use `kubectl linstor` to get access to the complete LINSTOR CLI."
#~ msgstr "上記のコマンドへの便利なショートカットを使用する場合、 https://github.com/piraeusdatastore/kubectl-linstor/releases[`kubectl-linstor`] をダウンロードし `kubectl` と一緒にインストールしてください。 `kubectl linstor` を使用して Linstor CLI にアクセスできます。"

#~ msgid "It also expands references of the form `pod:[<namespace>/]<podname>` into a list resources in use by the pod."
#~ msgstr "また `pod:[<namespace>/]<podname>` 形式の参照をポッドで使用中のリソースリストへ展開します。"

#, no-wrap
#~ msgid "Basic Configuration and Deployment"
#~ msgstr "基本的な構成とデプロイメント"

#~ msgid "Once all linstor-csi __Pod__s are up and running, we can provision volumes using the usual Kubernetes workflows."
#~ msgstr "すべての linstor-csi __Pod__ が稼働したら、通常のKubernetesワークフローを使用してボリュームをプロビジョニングできます。"

#~ msgid "Configuring the behavior and properties of LINSTOR volumes deployed through Kubernetes is accomplished using __StorageClass__es."
#~ msgstr "Kubernetes を介してデプロイされた LINSTOR ボリュームの動作とプロパティの構成は __StorageClass__es を使用して行います。"

#~ msgid "the \"resourceGroup\" parameter is mandatory. Usually you want it to be unique and the same as the storage class name."
#~ msgstr "\"resourceGroup\" パラメータは必須です。通常、一意にしてストレージクラス名と同じにします。"

#~ msgid "Here below is the simplest practical _StorageClass_ that can be used to deploy volumes:"
#~ msgstr "以下は、ボリュームのデプロイに使用できる最も単純で実用的な _StorageClass_ です。"

#~ msgid "We can create the _StorageClass_ with the following command:"
#~ msgstr "次のコマンドで _StorageClass_ を作成できます。"

#, no-wrap
#~ msgid "my-first-linstor-volume-pvc.yaml"
#~ msgstr "my-first-linstor-volume-pvc.yaml"

#~ msgid "For our example, we create a simple Pod, which mounts or volume by referencing the _PersistentVolumeClaim_."
#~ msgstr "この例では、_PersistentVolumeClaim_ を参照してマウントまたはボリューム化する単純なポッドを作成します。"

#~ msgid "Running `kubectl describe pod fedora` can be used to confirm that _Pod_ scheduling and volume attachment succeeded. Examining the _PersistentVolumeClaim_, we can see that it is now bound to a volume."
#~ msgstr "`kubectl describe pod fedora` を実行すると、 _Pod_ スケジューリングとボリューム接続が成功したことを確認できます。 _PersistentVolumeClaim_ を見ると、ボリュームにバインドされていることがわかります。"

#, no-wrap
#~ msgid "kubectl delete pod fedora # unschedule the pod.\n"
#~ msgstr "kubectl delete pod fedora # podをアンスケジュール。\n"

#, no-wrap
#~ msgid "kubectl get pod -w # wait for pod to be unscheduled\n"
#~ msgstr "kubectl get pod -w # podがアンスケジュールされるまで待つ\n"

#~ msgid "`linstor.csi.linbit.com/` is an optional, but recommended prefix for LINSTOR CSI specific parameters."
#~ msgstr "`linstor.csi.linbit.com/` はオプションですが、LINSTOR CSI 固有のパラメーターのプレフィックスとして推奨されます。"

#~ msgid "`ext4` (default)"
#~ msgstr "`ext4` (デフォルト)"

#~ msgid "`xfs`"
#~ msgstr "`xfs`"

#~ msgid "`autoPlace` is an integer that determines the amount of replicas a volume of this _StorageClass_ will have. For instance, `autoPlace: \"3\"` will produce volumes with three-way replication. If neither `autoPlace` nor `nodeList` are set, volumes will be <<s-autoplace-linstor,automatically placed>> on one node."
#~ msgstr "`autoPlace` は、この _StorageClass_ のボリュームが持つレプリカの量を決定する整数値です。例えば、`autoPlace: \"3\"` は３つの複製をもつボリュームを生成します。`autoPlace` と `nodeList` のどちらも設定されていない場合、１つのノード上にボリュームが生成されます。<<s-autoplace-linstor,自動配備>> を参照ください。"

#~ msgid "If you use this option, you must not use <<s-kubernetes-nodelist,nodeList>>."
#~ msgstr "このオプションを使用する場合は、 <<s-kubernetes-nodelist,nodeList>> を使用しないでください。"

#~ msgid "You have to use quotes, otherwise Kubernetes will complain about a malformed _StorageClass_."
#~ msgstr "引用符を使用する必要があります。そうしないと、Kubernetes は不正な形式の _StorageClass_ について文句を言います。"

#~ msgid "This option (and all options which affect autoplacement behavior) modifies the number of LINSTOR nodes on which the underlying storage for volumes will be provisioned and is orthogonal to which _kubelets_ those volumes will be accessible from."
#~ msgstr "このオプション（および自動配置の動作に影響を与えるすべてのオプション）は、ボリューム用のストレージがプロビジョニングされるLINSTORノードの数を変更し、 _kubelets_ からこれらのボリュームにアクセスできるようにします。"

#~ msgid "`storagePool` is the name of the LINSTOR <<s-storage_pools,storage pool>> that will be used to provide storage to the newly-created volumes."
#~ msgstr "`storagePool` は LINSTOR <<s-storage_pools,ストレージプール>> の名前で、新規に作成されたボリュームにストレージを供給するときに使用されます。"

#~ msgid "Select from one of the available volume schedulers:"
#~ msgstr "使用可能なボリュームスケジューラの 1 つから選択します。"

#~ msgid "`AutoPlaceTopology`, the default: Use topology information from Kubernetes together with user provided constraints (see <<s-kubernetes-replicasonsame>> and <<s-kubernetes-replicasondifferent>>)."
#~ msgstr "`AutoPlaceTopology` (デフォルト)： ユーザー提供の制約とともに、Kubernetesからのトポロジ情報を使用する(詳細は see <<s-kubernetes-replicasonsame>> と <<s-kubernetes-replicasondifferent>> を参照)。"

#~ msgid "`AutoPlace` Use LINSTOR autoplace, influenced by <<s-kubernetes-replicasonsame>> and <<s-kubernetes-replicasondifferent>>"
#~ msgstr "`AutoPlace` : LINSTOR autoplace を使う。 <<s-kubernetes-replicasonsame>> と <<s-kubernetes-replicasondifferent>> の影響をうける。"

#~ msgid "`FollowTopology`: Use CSI Topology information to place at least one volume in each \"preferred\" zone. Only useable if CSI Topology is enabled."
#~ msgstr "`FollowTopology`: CSI トポロジ情報を使用して、各 \"preferred\" ゾーンに少なくとも 1 つのボリュームを配置します。CSI トポロジが有効になっている場合にのみ使用できます。"

#~ msgid "`Manual`: Use only the nodes listed in `nodeList` and `clientList`."
#~ msgstr "`Manual`: `nodeList` と `clientList` にリストされているノードのみを使用する。"

#~ msgid "`Balanced`: **EXPERIMENTAL** Place volumes across failure domains, using the least used storage pool on each selected node."
#~ msgstr "`Balanced`: **実験的** 選択した各ノードで最も使用されていないストレージプールを使用して、障害のあるドメイン全体にボリュームを配置する。"

#~ msgid "Control on which nodes a volume is accessible. The value for this option can take two different forms:"
#~ msgstr "ボリュームにアクセスできるノードを制御します。このオプションの値は、次の 2 つの異なる形式をとることができます。"

#~ msgid "A simple `\"true\"` or `\"false\"` allows access from all nodes, or only those nodes with diskfull resources."
#~ msgstr "`\"true\"` または `\"false\"` は、すべてのノードからのアクセス、またはディスクフルリソースを持つノードのみからのアクセスを許可します。"

#~ msgid "Advanced rules, which allow more granular rules on which nodes can access the volume."
#~ msgstr "高度なルール。ノードがボリュームにアクセスできる、より詳細なルールを可能にします。"

#~ msgid "The current implementation can grant access to the volume for nodes that share the same labels. For example, if you want to allow access from all nodes in the same region and zone as a diskfull resource, you could use:"
#~ msgstr "現在の実装では、同じラベルを共有するノードのボリュームへのアクセスを許可できます。たとえば、ディスクフルリソースと同じリージョンおよびゾーン内のすべてのノードからのアクセスを許可する場合は、次を使用できます。"

#~ msgid "If you use this option, you must not use <<s-kubernetes-autoplace,autoPlace>>."
#~ msgstr "このオプションを使用する場合は <<s-kubernetes-autoplace,autoPlace>> を使用しないでください。"

#~ msgid "This option determines on which LINSTOR nodes the underlying storage for volumes will be provisioned and is orthogonal from which _kubelets_ these volumes will be accessible."
#~ msgstr "このオプションは、ボリュームのストレージをどのLINSTORノード上でプロビジョニングするかを決定し、 _kubelets_ からこれらのボリュームにアクセスできるようにします。"

#~ msgid "`replicasOnSame` is a list of `key` or `key=value` items used as autoplacement selection labels when <<s-kubernetes-autoplace,autoplace>> is used to determine where to provision storage. These labels correspond to LINSTOR node properties."
#~ msgstr "`replicasOnSame` は自動配置選択ラベルとして使用される `key` または `key=value` アイテムのリストです。これは <<s-kubernetes-autoplace,autoplace>> がストレージをプロビジョニングする場所を決定するために使用されます。これらのラベルは、LINSTOR ノードのプロパティに対応しています。"

#~ msgid "The operator periodically synchronizes all labels from Kubernetes Nodes, so you can use them as keys for scheduling constraints."
#~ msgstr "オペレーターは、Kubernetes ノードからのすべてのラベルを定期的に同期するため、それらをスケジューリング制約のキーとして使用できます。"

#~ msgid "Let's explore this behavior with examples assuming a LINSTOR cluster such that `node-a` is configured with the following auxiliary property `zone=z1` and `role=backups`, while `node-b` is configured with only `zone=z1`."
#~ msgstr "`node-a` が補助プロパティ `zone=z1`, `role=backups` で、 `node-b` が `zone=z1` のみで構成されているような LINSTOR クラスターを想定した例でこの動作を調べてみましょう。"

#~ msgid "If we configure a _StorageClass_ with `autoPlace: \"1\"` and `replicasOnSame: \"zone=z1 role=backups\"`, then all volumes created from that _StorageClass_ will be provisioned on `node-a`, since that is the only node with all of the correct key=value pairs in the LINSTOR cluster. This is the most flexible way to select a single node for provisioning."
#~ msgstr "`autoPlace: \"1\"` と `replicasOnSame: \"zone=z1 role=backups\"` を持つ _StorageClass_ を設定すると、この _StorageClass_ から生成されるすべてのボリュームは `node-a` にプロビジョニングされます。これは LINSTOR クラスタ内ですべての key = value ペアを持つ唯一のノードだからです。これは、プロビジョニングに１つのノードを選択する最も柔軟な方法です。"

#~ msgid "This guide assumes LINSTOR CSI version 0.10.0 or newer. All properties referenced in `replicasOnSame` and `replicasOnDifferent` are interpreted as auxiliary properties. If you are using an older version of LINSTOR CSI, you need to add the `Aux/` prefix to all property names. So `replicasOnSame: \"zone=z1\"` would be `replicasOnSame: \"Aux/zone=z1\"` Using `Aux/` manually will continue to work on newer LINSTOR CSI versions."
#~ msgstr ""
#~ "このガイドは、LINSTOR CSI バージョン 0.10.0 以降を想定しています。 `replicasOnSame` および `replicasOnDifferent` で参照されるすべてのプロパティは、補助プロパティとして解釈されます。古いバージョンの LINSTOR CSI を使用している場合は、すべてのプロパティ名に `Aux/` プレフィックスを追加する必要があります。したがって、 `replicasOnSame: \"zone=z1\"` は `replicasOnSame: \"Aux/zone=z1\"` になります。 `Aux/` を手動で追加すると、新しい LINSTOR CSI バージョンで引き続き機能"
#~ "します。"

#~ msgid "If we configure a _StorageClass_ with `autoPlace: \"1\"` and `replicasOnSame: \"zone=z1\"`, then volumes will be provisioned on either `node-a` or `node-b` as they both have the `zone=z1` aux prop."
#~ msgstr "`autoPlace: \"1\"` と `replicasOnSame: \"zone=z1\"` を持つ _StorageClass_ を設定すると、ボリュームは `node-a` か `node-b` のどちらかにプロビジョニングされます。これは、両方が `zone=z1` aux prop を持つからです。"

#~ msgid "If we configure a _StorageClass_ with `autoPlace: \"2\"` and `replicasOnSame: \"zone=z1 role=backups\"`, then provisioning will fail, as there are not two or more nodes that have the appropriate auxiliary properties."
#~ msgstr "`autoPlace: \"2\"` と `replicasOnSame: \"zone=z1 role=backups\"` を持つ _StorageClass_ を設定すると、適切な補助プロパティを持つノードが2つ以上ないためプロビジョニングは失敗します。"

#~ msgid "If we configure a _StorageClass_ with `autoPlace: \"2\"` and `replicasOnSame: \"zone=z1\"`, then volumes will be provisioned on both `node-a` and `node-b` as they both have the `zone=z1` aux prop."
#~ msgstr "`autoPlace: \"2\"` と `replicasOnSame: \"zone=z1\"` を持つ _StorageClass_ を設定すると、ボリュームは `node-a` と `node-b` の両方にプロビジョニングされます。これは、両方が `zone=z1` aux prop を持つからです。"

#~ msgid "You can also use a property key without providing a value to ensure all replicas are placed on nodes with the same property value, with caring about the particular value. Assuming there are 4 nodes, `node-a1` and `node-a2` are configured with `zone=a`. `node-b1` and `node-b2` are configured with `zone=b`. Using `autoPlace: \"2\"` and `replicasOnSame: \"zone\"` will place on either `node-a1` and `node-a2` OR on `node-b1` and `node-b2`."
#~ msgstr "値を指定せずにプロパティキーを使用して、特定の値を考慮しながら、すべてのレプリカが同じプロパティ値を持つノードに配置されるようにすることもできます。 4つのノードがあると仮定し `node-a1` と `node-a2` は `zone=a` で構成、 `node-b1` と `node-b2` は `zone=b` で構成されているとします。 `autoPlace: \"2\"` と `replicasOnSame: \"zone\"` を使用すると `node-a1` と `node-a2`、または `node-b1` と `node-b2` のいずれかに配置されます。"

#~ msgid "`replicasOnDifferent` takes a list of properties to consider, same as <<s-kubernetes-replicasonsame,replicasOnSame>>.  There are two modes of using `replicasOnDifferent`:"
#~ msgstr "`replicasOnDifferent` は <<s-kubernetes-replicasonsame,replicasOnSame>> と同じように、考慮すべきプロパティのリストを取ります。 `replicasOnDifferent` の使用には 2 つのモードがあります。"

#~ msgid "Preventing volume placement on specific nodes:"
#~ msgstr "特定のノードでのボリューム配置の防止:"

#~ msgid "If a value is given for the property, the nodes which have that property-value pair assigned will be considered last."
#~ msgstr "プロパティに値が指定されている場合、そのプロパティと値のペアが割り当てられているノードが最後に考慮されます。"

#~ msgid "Example: `replicasOnDifferent: \"no-csi-volumes=true\"` will place no volume on any node with property `no-csi-volumes=true` unless there are not enough other nodes to fulfill the `autoPlace` setting."
#~ msgstr "例： `replicasOnDifferent: \"no-csi-volumes=true\"` は `autoPlace` 設定を満たすのに十分なノードが他にない限り、プロパティ `no-csi-volumes=true` を持つノードにボリュームを配置しません。"

#~ msgid "Distribute volumes across nodes with different values for the same key:"
#~ msgstr "同じキーの値が異なるノード間でボリュームを分散します。"

#~ msgid "If no property value is given, LINSTOR will place the volumes across nodes with different values for that property if possible."
#~ msgstr "プロパティ値が指定されていない場合、LINSTOR は、可能であれば、そのプロパティの値が異なるノード間でボリュームを配置します。"

#~ msgid "Example: Assuming there are 4 nodes, `node-a1` and `node-a2` are configured with `zone=a`. `node-b1` and `node-b2` are configured with `zone=b`. Using a _StorageClass_ with `autoPlace: \"2\"` and `replicasOnDifferent: \"zone\"`, LINSTOR will create one replica on either `node-a1` or `node-a2` _and_ one replica on either `node-b1` or `node-b2`."
#~ msgstr "例： 4 つのノードがあると仮定し `node-a1` と `node-a2` は `zone=a` で、 `node-b1` と `node-b2` は `zone=b` で構成されているとします。 _StorageClass_ を `autoPlace: \"2\"` および `replicasOnDifferent: \"zone\"` で使用すると、LINSTOR は `node-a1` または `node-a2` のいずれかに 1 つのレプリカを作成し、 `node-b1` または `node-b2` のいずれかにもう 1 つのレプリカを作成します。"

#~ msgid "Please note these values are specific to your chosen <<s-kubernetes-filesystem, filesystem>>."
#~ msgstr "これらの値は選択した <<s-kubernetes-filesystem, filesystem>> 固有です。"

#~ msgid "For example, to set `DrbdOptions/auto-quorum` to `disabled`, use:"
#~ msgstr "たとえば `DrbdOptions/auto-quorum` を `disabled` に設定するには、次を使用します。"

#~ msgid "Advanced DRBD options to pass to LINSTOR. For example, to change the replication protocol, use `DrbdOptions/Net/protocol: \"A\"`."
#~ msgstr "LINSTOR に渡す高度な DRBD オプション。たとえば、レプリケーションプロトコルを変更するには `DrbdOptions/Net/protocol: \"A\"` を使用します。"

#~ msgid "Creating <<s-linstor-snapshots, snapshots>> and creating new volumes from snapshots is done using __VolumeSnapshot__s, __VolumeSnapshotClass__es, and __PVC__s."
#~ msgstr "<<s-linstor-snapshots, スナップショット>> の作成とスナップショットからの新しいボリュームの作成は __VolumeSnapshot__s, __VolumeSnapshotClass__es, と __PVC__s を使用して行われます。"

#~ msgid "LINSTOR supports the volume snapshot feature, which is configured in some, but not all Kubernetes distributions."
#~ msgstr "LINSTOR は、すべてではありませんが一部の Kubernetes ディストリビューションでボリュームスナップショット機能をサポートしています。"

#~ msgid "To check if your Kubernetes distribution supports snapshots out of the box, run the following command:"
#~ msgstr "Kubernetes ディストリビューションがスナップショットをサポートしているかどうかを確認するには、次のコマンドを実行します。"

#, no-wrap
#~ msgid ""
#~ "$ kubectl get --raw /apis/snapshot.storage.k8s.io/v1\n"
#~ "{\"kind\":\"APIResourceList\",\"apiVersion\":\"v1\",\"groupVersion\":\"snapshot.storage.k8s.io/v1\"...\n"
#~ "$ # If your distribution does NOT support snapshots out of the box, you will get a different response:\n"
#~ "$ kubectl get --raw /apis/snapshot.storage.k8s.io/v1\n"
#~ "Error from server (NotFound): the server could not find the requested resource\n"
#~ msgstr ""
#~ "$ kubectl get --raw /apis/snapshot.storage.k8s.io/v1\n"
#~ "{\"kind\":\"APIResourceList\",\"apiVersion\":\"v1\",\"groupVersion\":\"snapshot.storage.k8s.io/v1\"...\n"
#~ "$ # If your distribution does NOT support snapshots out of the box, you will get a different response:\n"
#~ "$ kubectl get --raw /apis/snapshot.storage.k8s.io/v1\n"
#~ "Error from server (NotFound): the server could not find the requested resource\n"

#~ msgid "In case your Kubernetes distribution _does not_ support snapshots, you can manually add the {snapshot-controller-link}[required components] from the Kubernetes Storage SIG. For convenience, you can use {piraeus-charts-link}[Helm Charts] provided by the {piraeus-org}[Piraeus team] to add these components."
#~ msgstr "Kubernetes ディストリビューションがスナップショットをサポートしていない場合は、Kubernetes Storage SIG から {snapshot-controller-link}[required components] を手動で追加できます。 {piraeus-org}[Piraeus team] が提供する {piraeus-charts-link}[Helm Charts] を使用してこれらのコンポーネントを追加することも可能です。"

#, no-wrap
#~ msgid "Adding snapshot support using the Piraeus Charts"
#~ msgstr "Piraeus チャートを使用したスナップショットサポートの追加"

#, no-wrap
#~ msgid ""
#~ "$ kubectl create namespace snapshot-controller\n"
#~ "$ helm repo add piraeus-charts https://piraeus.io/helm-charts/\n"
#~ "$ helm install -n snapshot-controller snapshot-validation-webhook \\\n"
#~ "  piraeus-charts/snapshot-validation-webhook\n"
#~ "$ helm install -n snapshot-controller snapshot-controller \\\n"
#~ "  piraeus-charts/snapshot-controller --wait\n"
#~ msgstr ""
#~ "$ kubectl create namespace snapshot-controller\n"
#~ "$ helm repo add piraeus-charts https://piraeus.io/helm-charts/\n"
#~ "$ helm install -n snapshot-controller snapshot-validation-webhook \\\n"
#~ "  piraeus-charts/snapshot-validation-webhook\n"
#~ "$ helm install -n snapshot-controller snapshot-controller \\\n"
#~ "  piraeus-charts/snapshot-controller --wait\n"

#~ msgid "Then we can create our _VolumeSnapshotClass_:"
#~ msgstr "それで _VolumeSnapshotClass_ を作成できます。"

#, no-wrap
#~ msgid "my-first-linstor-snapshot-class.yaml"
#~ msgstr "my-first-linstor-snapshot-class.yaml"

#~ msgid "Create the _VolumeSnapshotClass_ with `kubectl`:"
#~ msgstr "`kubectl` を使用して _VolumeSnapshotClass_ を作成します。"

#, no-wrap
#~ msgid "kubectl create -f my-first-linstor-snapshot-class.yaml\n"
#~ msgstr "kubectl create -f my-first-linstor-snapshot-class.yaml\n"

#~ msgid "Now we will create a volume snapshot for the volume that we created above. This is done with a _VolumeSnapshot_:"
#~ msgstr "次に上記で作成したボリュームのボリュームスナップショットを作成します。これは _VolumeSnapshot_ を使用します。"

#, no-wrap
#~ msgid "my-first-linstor-snapshot.yaml"
#~ msgstr "my-first-linstor-snapshot.yaml"

#~ msgid "Create the _VolumeSnapshot_ with `kubectl`:"
#~ msgstr "`kubectl` を使用して _VolumeSnapshot_ を作成します。"

#, no-wrap
#~ msgid "kubectl create -f my-first-linstor-snapshot.yaml\n"
#~ msgstr "kubectl create -f my-first-linstor-snapshot.yaml\n"

#~ msgid "You can check that the snapshot creation was successful"
#~ msgstr "スナップショットの作成が成功したことを確認できます。"

#, no-wrap
#~ msgid ""
#~ "kubectl describe volumesnapshots.snapshot.storage.k8s.io my-first-linstor-snapshot\n"
#~ "...\n"
#~ "Spec:\n"
#~ "  Source:\n"
#~ "    Persistent Volume Claim Name:  my-first-linstor-snapshot\n"
#~ "  Volume Snapshot Class Name:      my-first-linstor-snapshot-class\n"
#~ "Status:\n"
#~ "  Bound Volume Snapshot Content Name:  snapcontent-b6072ab7-6ddf-482b-a4e3-693088136d2c\n"
#~ "  Creation Time:                       2020-06-04T13:02:28Z\n"
#~ "  Ready To Use:                        true\n"
#~ "  Restore Size:                        500Mi\n"
#~ msgstr ""
#~ "kubectl describe volumesnapshots.snapshot.storage.k8s.io my-first-linstor-snapshot\n"
#~ "...\n"
#~ "Spec:\n"
#~ "  Source:\n"
#~ "    Persistent Volume Claim Name:  my-first-linstor-snapshot\n"
#~ "  Volume Snapshot Class Name:      my-first-linstor-snapshot-class\n"
#~ "Status:\n"
#~ "  Bound Volume Snapshot Content Name:  snapcontent-b6072ab7-6ddf-482b-a4e3-693088136d2c\n"
#~ "  Creation Time:                       2020-06-04T13:02:28Z\n"
#~ "  Ready To Use:                        true\n"
#~ "  Restore Size:                        500Mi\n"

#~ msgid "Finally, we'll create a new volume from the snapshot with a _PVC_."
#~ msgstr "最後にスナップショットから _PVC_ で新しいボリュームを作成します。"

#, no-wrap
#~ msgid "my-first-linstor-volume-from-snapshot.yaml"
#~ msgstr "my-first-linstor-volume-from-snapshot.yaml"

#~ msgid "Create the _PVC_ with `kubectl`:"
#~ msgstr "`kubectl` を使用して _PVC_ を作成します。"

#, no-wrap
#~ msgid "kubectl create -f my-first-linstor-volume-from-snapshot.yaml\n"
#~ msgstr "kubectl create -f my-first-linstor-volume-from-snapshot.yaml\n"

#, no-wrap
#~ msgid ""
#~ "apiVersion: storage.k8s.io/v1\n"
#~ "kind: StorageClass\n"
#~ "metadata:\n"
#~ "  name: linstor-storage\n"
#~ "provisioner: linstor.csi.linbit.com\n"
#~ "volumeBindingMode: WaitForFirstConsumer <1>\n"
#~ "parameters:\n"
#~ "  linstor.csi.linbit.com/storagePool: linstor-pool <2>\n"
#~ "  linstor.csi.linbit.com/placementCount: \"2\" <3>\n"
#~ "  linstor.csi.linbit.com/allowRemoteVolumeAccess: | <4>\n"
#~ "    - fromSame:\n"
#~ "      - topology.kubernetes.io/zone\n"
#~ "  linstor.csi.linbit.com/replicasOnDifferent: topology.kubernetes.io/zone <5>\n"
#~ msgstr ""
#~ "apiVersion: storage.k8s.io/v1\n"
#~ "kind: StorageClass\n"
#~ "metadata:\n"
#~ "  name: linstor-storage\n"
#~ "provisioner: linstor.csi.linbit.com\n"
#~ "volumeBindingMode: WaitForFirstConsumer <1>\n"
#~ "parameters:\n"
#~ "  linstor.csi.linbit.com/storagePool: linstor-pool <2>\n"
#~ "  linstor.csi.linbit.com/placementCount: \"2\" <3>\n"
#~ "  linstor.csi.linbit.com/allowRemoteVolumeAccess: | <4>\n"
#~ "    - fromSame:\n"
#~ "      - topology.kubernetes.io/zone\n"
#~ "  linstor.csi.linbit.com/replicasOnDifferent: topology.kubernetes.io/zone <5>\n"

#, no-wrap
#~ msgid ""
#~ "apiVersion: storage.k8s.io/v1\n"
#~ "kind: StorageClass\n"
#~ "metadata:\n"
#~ "  name: linstor-storage\n"
#~ "provisioner: linstor.csi.linbit.com\n"
#~ "volumeBindingMode: WaitForFirstConsumer <1>\n"
#~ "parameters:\n"
#~ "  linstor.csi.linbit.com/storagePool: linstor-pool <2>\n"
#~ "  linstor.csi.linbit.com/placementCount: \"2\" <3>\n"
#~ "  linstor.csi.linbit.com/allowRemoteVolumeAccess: | <4>\n"
#~ "    - fromSame:\n"
#~ "      - topology.kubernetes.io/zone\n"
#~ "  linstor.csi.linbit.com/replicasOnSame: topology.kubernetes.io/region <5>\n"
#~ msgstr ""
#~ "apiVersion: storage.k8s.io/v1\n"
#~ "kind: StorageClass\n"
#~ "metadata:\n"
#~ "  name: linstor-storage\n"
#~ "provisioner: linstor.csi.linbit.com\n"
#~ "volumeBindingMode: WaitForFirstConsumer <1>\n"
#~ "parameters:\n"
#~ "  linstor.csi.linbit.com/storagePool: linstor-pool <2>\n"
#~ "  linstor.csi.linbit.com/placementCount: \"2\" <3>\n"
#~ "  linstor.csi.linbit.com/allowRemoteVolumeAccess: | <4>\n"
#~ "    - fromSame:\n"
#~ "      - topology.kubernetes.io/zone\n"
#~ "  linstor.csi.linbit.com/replicasOnSame: topology.kubernetes.io/region <5>\n"

#, no-wrap
#~ msgid ""
#~ "apiVersion: v1\n"
#~ "kind: Pod\n"
#~ "metadata:\n"
#~ "  name: busybox\n"
#~ "spec:\n"
#~ "  schedulerName: linstor-scheduler <1>\n"
#~ "  containers:\n"
#~ "  - name: busybox\n"
#~ "    image: busybox\n"
#~ "    command: [\"tail\", \"-f\", \"/dev/null\"]\n"
#~ "    volumeMounts:\n"
#~ "    - name: my-first-linstor-volume\n"
#~ "      mountPath: /data\n"
#~ "    ports:\n"
#~ "    - containerPort: 80\n"
#~ "  volumes:\n"
#~ "  - name: my-first-linstor-volume\n"
#~ "    persistentVolumeClaim:\n"
#~ "      claimName: \"test-volume\"\n"
#~ msgstr ""
#~ "apiVersion: v1\n"
#~ "kind: Pod\n"
#~ "metadata:\n"
#~ "  name: busybox\n"
#~ "spec:\n"
#~ "  schedulerName: linstor-scheduler <1>\n"
#~ "  containers:\n"
#~ "  - name: busybox\n"
#~ "    image: busybox\n"
#~ "    command: [\"tail\", \"-f\", \"/dev/null\"]\n"
#~ "    volumeMounts:\n"
#~ "    - name: my-first-linstor-volume\n"
#~ "      mountPath: /data\n"
#~ "    ports:\n"
#~ "    - containerPort: 80\n"
#~ "  volumes:\n"
#~ "  - name: my-first-linstor-volume\n"
#~ "    persistentVolumeClaim:\n"
#~ "      claimName: \"test-volume\"\n"

#~ msgid "When node failures occur, Kubernetes is very conservative in rescheduling stateful workloads. This means it can take more than 15 minutes for Pods to be moved from unreachable nodes. With the information available to DRBD and LINSTOR, this process can be sped up significantly."
#~ msgstr "ノードに障害が発生した場合、Kubernetes はステートフルワークロードの再スケジュールを非常に慎重に行います。これは、ポッドが到達不能なノードから移動されるまでに 15 分以上かかる可能性があることを意味します。DRBD と LINSTOR が利用できる情報により、このプロセスを大幅にスピードアップできます。"

#~ msgid "The LINSTOR High Availability Controller (HA Controller) speeds up the failover process for stateful workloads using LINSTOR for storage. It monitors and manages any Pod that is attached to at least one DRBD resource."
#~ msgstr "LINSTOR 高可用性コントローラー (HA コントローラー) は、ストレージに LINSTOR を使用して、ステートフル ワークロードのフェイルオーバー プロセスを高速化します。少なくとも 1 つの DRBD リソースにアタッチされている Pod を監視および管理します。"

#~ msgid "For the HA Controller to work properly, you need quorum, that is at least three replicas (or two replicas + one diskless tiebreaker). If using lower replica counts, attached Pods will be ignored and are not eligible for faster failover."
#~ msgstr "HA コントローラーが正しく機能するには、クォーラム、つまり少なくとも 3 つのレプリカ (または 2 つのレプリカと 1 つのディスクレス タイブレーカー) が必要です。使用するレプリカ数が少ない場合、アタッチされた Pod は無視され、より高速なフェイルオーバーの対象にはなりません。"

#~ msgid "The HA Controller is packaged as a Helm chart, and can be deployed using:"
#~ msgstr "HA コントローラーは Helm チャートとしてパッケージ化されており、以下を使用してデプロイできます。"
