[[ch-openstack]]
== OpenstackでのDRBDボリューム

indexterm:[Openstack]indexterm:[Cinder]indexterm:[Nova]本章では永続的で冗長化され、しかも高性能なブロックストレージとしてDRBDをOpenstacｋで使用する方法について説明します。

[[s-openstack-overview]]
=== Openstack概要

Openstackそれ自体は様々な個々のサービスから構成されています。DRBDと関連があるものは主にCinderとNovaの２つです。*Cinder*
はブロックストレージサービスであり、 *Nova* はコンピュートサービスです。またNovaは仮想マシンがボリュームを利用するための管理もします。

DRBDストレージボリュームには2通りの方法でアクセスできます。iSCSIプロトコルを使用する方法(互換性最大)と、DRBDクライアント機能を使う方法(Openstackプロジェクトへ提出済)です。これら2つの方法の違いの詳細については<<s-openstack-transport-protocol>>をご参照ください。

[[s-openstack-install]]
=== OpenstackへのDRBDインストール

drbdmanage ドライバはLibertyリリースからOpenstackのメインストリームになっています。主に `c-vol`
サービスで使用されており、 これを動かすノードでdrbdmanageとDRBD9をインストールする必要があります。


使用しているOpenstackによってパスやユーザー名等に少々違いがあります。

.ディストリビューション依存の設定
[format="csv", separator=";", options="header"]
|============================================
対象   ;   rdostack   ;   devstack
Cinder/DRBD Manageドライバファイルの場所; `/usr/lib/python2.6/site-packages/cinder/volume/drivers/drbdmanagedrv.py` ; `/opt/stack/cinder/cinder/volume/drivers/drbdmanagedrv.py` 
Cinder設定ファイル ; `/usr/share/cinder/cinder-dist.conf` ; `/etc/cinder/cinder.conf` 
シェルに組み込むためのAdminアクセス情報 ; `/etc/nagios/keystonerc_admin` ; `stack/devstack/accrc/admin/admin` 
`c-vol` サービスのユーザー ; `cinder` ; `stack ` 
|============================================


一般的なインストール手順

  *  `cinder.conf` で次のように設定します。`volume_driver` はクラス名(最後の箇所)とファイルのパスから構成されます。
+
--

-------
[DEFAULT]
enabled_backends=drbd-1

[drbd-1]
volume_driver=cinder.volume.drivers.drbdmanagedrv.DrbdManageIscsiDriver
volume_backend_name=DRBD-Managed
drbdmanage_redundancy=1
-------

iSCSIとDRBDトランスポートモードを使用の際には<<s-openstack-transport-protocol>>を参照のうえで選択してください。あわせて<<s-openstack-addtl-conf,他の設定項目>>もご参照ください。
--

  * バックエンド登録をします。(認証の環境変数を `source <admin access data>` から取得する必要があるかもしれません)
+
--

-------
# cinder type-create drbd-1
# cinder type-key drbd-1 set volume_backend_name=DRBD-Managed
-------
--

  * DBusを通じて "org.drbd.drbdmanaged" サービスにユーザーがアクセスできるようにします。そのために
    `/etc/dbus-1/system.d/org.drbd.drbdmanaged.conf`
    ファイルに以下を追加して拡張します。(USERをユーザ名で置き換えます。)
+
--

-------
<policy user="USER">
  <allow own="org.drbd.drbdmanaged"/>
  <allow send_interface="org.drbd.drbdmanaged"/>
  <allow send_destination="org.drbd.drbdmanaged"/>
</policy>
-------
--


以上です。あとは `c-vol` サービスを再起動すれば、DRBDボリュームが作成できます。


[[s-openstack-addtl-conf]]
==== 追加設定

cinder.conf の drbdmanage 下位デバイスの設定で、詳細な動作を調整するための、いくつかの追加設定を行うことができます。

[[s-openstack-redundancy]]
  * (((OpenStack,Redundancy))) `drbdmanage_redundancy = 2`  :
    各ボリュームが2箇所に置かれます。つまり、 レプリケーションが1回行われます。この場合、ストレージに2回書き込みが行われてから
    使用可能な空き容量が<<s-openstack-free-space,減った>>とみなされます。
+
--
2以上のデータのコピーを指定できます。上限の数はDRBD9と定義しているストレージホストの数によって制限されます。
--

  *  `drbdmanage_devs_on_controller = True` :
    デフォルトでは各ボリュームはCinderコントローラーノードにマッピングされたDRBDクライアントを使用します。また、
    iSCSIエクスポートに使用する他に、デバッグ用途にも役立ちます。

  * indexterm:[iSCSI,OpenStack Cinder]indexterm:[Openstack, Cinder iSCSI
    transport] 異なるiSCSI下位デバイスを選択する必要がある場合、 `iscsi_helper=lioadm`
    のような追加設定をすることができます。

  * (((OpenStack,resize policy)))`drbdmanage_resize_policy` 、
    (((OpenStack,resource policy)))`drbdmanage_resource_policy`、
    (((OpenStack,snapshot policy)))`drbdmanage_snapshot_policy` :
	各ボリュームのリサイズ時、スナップショットまたは（新規またはスナップショットからの両方での）リソースの作成時の動作を設定します。
+
--
以下のようなJSONブロックとして解析可能な文字列です。

	drbdmanage_snapshot_policy={'count': '1', 'timeout': '60'}

使用可能なポリシーや設定アイテムの詳細は<<s-drbdmanage-deployment-policy>>を参照してください。

[NOTE]
PythonのJSONパーサは厳密なものです。一例としてはシングルクォートの使用が必要です。JSONの仕様と制限によく注意してください。

この目的で使用できる次のようなプラグインもあります。`drbdmanage_resize_plugin` 、
`drbdmanage_resource_plugin`、`drbdmanage_snapshot_plugin`。
--

  * `drbdmanage_resource_options` に対応する `drbdmanage_net_options`
    は各新規作成ボリュームのDRBDの設定に使用できます。これらこれらには有効なデフォルト値がありますので、上書きしたい場合には再度追加を忘れないでください。
+
--
また、これらはJSONブロックとして構文解析されます。デフォルトでは次のようになっています。

    drbdmanage_net_options = {'connect-int': '4', 'allow-two-primaries': 'yes', 'ko-count': '30'}
    drbdmanage_resource_options = {'auto-promote-timeout': '300'}

--

  * `drbdmanage_late_local_assign` と `drbdmanage_late_local_assign_exclude`
    はハイパーコンバージド構成のパフォーマンス最適化で使用します。これは少し検討する余地があるので、詳細は
    <<s-openstack-late-local-assign>> を参照ください。


設定は下位デバイスごとに行うことができます。



[[s-openstack-transport-protocol]]
=== トランスポートプロトコルの選択

CinderでDRBDを動作させるには2つの方法があります。

  * <<s-openstack-iscsi,iSCSIエクスポート>>経由でアクセスする

  * <<s-openstack-drbd,DRBDプロトコル>>を使って接続する

これらは排他的なものではありません。複数の下位デバイスがあるときには、ある下位デバイスはiSCSIを使って、他のものはDRBDプロトコルを使うといった事も可能です。


[[s-openstack-iscsi]]
==== iSCSIトランスポート

CinderボリュームのエクスポートはデフォルトではiSCSI経由になっています。この場合には互換性が非常に高い長所があります。iSCSIはVMWare、Xen、HyperV、KVMなどすべてのハイパーバイザで使用することができます。

欠点は、すべてのデーターがCinderノードに送信されますが、処理がユーザー空間のiSCSIデーモンによって行われるために、データがカーネルとユーザスペースの境界を通過する際の遷移でパフォーマンス低下が起きることです。

TODO: performance comparision


[[s-openstack-drbd]]
==== DRBDトランスポート

もう一つの方法は、DRBDプロトコルを使用して仮想マシンにデータを取得する方法です。DRBD9footnote:[カーネルモジュールとユーザー空間、また現時点ではDRBD
manageデーモンも含む。<<s-openstack-drbd-external-NOTE>>の注記も参照してください]はNovaノードにもインストールしてある必要があり、そのため現時点ではKVMが使用できるLinuxという制限があります。

この方法の長所の一つは、仮想マシンによるストレージアクセス要求はDRBDカーネルモジュールを通じてストレージノードに送信され、直接割り当てられたLVにアクセスできる点です。つまりカーネルとユーザー空間の遷移が起きないので、高いパフォーマンスを得ることができます。
RDMAが使用できるハードウェアを使用すれば、仮想マシンがFC接続で直接下位デバイスにアクセスしているのと同等のパフォーマンスを得ることができます。

また、DRBDの持つHA対策ソフトウェアというバックグラウンドが持つ長所を生かすことができます。
複数ノード間で異なるネットワークコネクションを持つ事がきますので、単一障害点を回避して冗長性をもたせることができます。


[[s-openstack-drbd-external-NOTE]]
[NOTE]
--
現時点では、DRBD manageクラスタとしてのハイパーバイザーノードが必要です。

DRBD Manageを"external
nodes"で稼働させることができると、ハイパーバイザーノードの要求はDRBD9のカーネルモジュールとユーザー空間ツールだけになります。
--


[[s-openstack-conf-transport-protocol]]
==== トランスポートプロトコルの設定

cinder.conf のストレージの節で使用するボリュームドライバを定義することができます。
異なる下位デバイスの設定では異なるドライバを使用できます。2重の冗長性のiSCSI下位デバイス、2重の冗長性のDRBD下位デバイス、3重のDRBD下位デバイスを同時に定義することもできます。Horizonfootnote:[OpenstackのGUI]
はボリューム作成時にこれらのストレージ下位デバイスを提供します、

2つのドライバに使用ができる設定アイテムは、

	* iSCSI用
+
--

    volume_driver=cinder.volume.drivers.drbdmanagedrv.DrbdManageIscsiDriver
--

および

    * DRBD用
+
--

    volume_driver=cinder.volume.drivers.drbdmanagedrv.DrbdManageDrbdDriver

--


古いクラス名の"DrbdManageDriver"は、互換性の観点で残しており、単純なiSCSIドライバのエイリアスになっています。


[[s-openstack-notes]]
=== その他注意点


[[s-openstack-free-space]]
==== 空き容量の通知

Cinderドライバが報告する空き容量は、<<s-openstack-addtl-conf,`drbdmanage_redundancy`>>の設定を使用してDRBDマネージから取得します。

この時、このレプリケーションカウントで作成することができる最大ボリューム１つのサイズを返します。10個のストレージノードが各々1TiBの空き容量であり、冗長カウントが3であれば1TiBが返されます。また、ボリュームの割り当てを行っても、十分な空き容量のあるノードが3つ以上あるので、空き容量として返される値は変更されません。20GiB、15GiB、10GiB、5GiBの空き容量のあるストレージノードがあるとき、
`drbdmanage_redundancy` が3の場合は10GiBが返されます。2の場合は15GiBが返されます。

この問題はシンLVMのプールの場合(1つまたは複数であるか、DRBD
manageのストレージバックエンドに依存して)、またCinderボリュームから取得したスナップショットであるかによって複雑になります。

詳細な情報についてはシンプロビジョニングについてのOpenstac仕様を参照ください。
- https://blueprints.launchpad.net/cinder/`spec/over-subscription-in-thin-provisioning[blueprint]
  および
  https://github.com/openstack/cinder-specs/blob/master/specs/kilo/over-subscription-in-thin-provisioning.rst[text]があります。


[[s-openstack-late-local-assign]]
==== Hyperconverged Setups

The configuration item `drbdmanage_late_local_assign` (available in the DRBD
Manage Cinder driver from 1.2.0 on, requiring DRBD Manage 0.98.3 or better)
is a performance optimization for hyperconverged setups. + With that
feature, the driver tries to get a local copy of the data assigned to the
hypervisor; that in turn will speed up read IOs, as these won't have to go
across the network.

At the time of writing, Nova doesn't pass enough information to Cinder;
Cinder isn't told which hypervisor will be used. + So the DRBD Manage driver
assigns all but one copies at `create_volume` time; the last one is done in
the `attach_volume` step, when the hypervisor is known. If this hypervisor
is out of space, defined as a storage-less node in DRBD Manage, or otherwise
not eligible to receive a copy, any other storage node is used instead, and
the target node will receive a _client_ assignment only.


Because an image might be copied to the volume before it gets attached to a
VM, the "local" assignment can't simply be done on the first
accessfootnote:[If it assigned on first access, the image copy node (Glance)
would receive the copy of the data]. The Cinder driver must be told which
nodes are not eligible for local copies; this can be done via
`drbdmanage_late_local_assign_exclude`.


For volumes that get cloned from an image stored within Cinder (via a DRBD
Manage snapshot), the new resource will be empty until the `attach_volume`
call; at that time the Cinder driver can decide on which nodes the volumes
will be deployed, and can actually clone the volume on these.


.Free Space Misreported
[WARNING]
--
Late allocation invariably means that the free space numbers are wrong. You
might prepare 300 VMs, only to find out that you're running out of disk
space when their volumes are in the middle of synchronizing.

But that is a common problem with all thin allocation schemes, so we won't
discuss that in more details here.
--


To summarize:

	* You'll need the DRBD Manage Cinder driver 1.2.0 or later, and DRBD Manage
	  0.98.3 or later.

	* The <<s-openstack-drbd,DRBD transport protocol>> must be used; iSCSI won't
	  offer any locality benefits.

	* The <<s-openstack-redundancy,`drbdmanage_redundancy` setting>> must be set
	  to at least two copies.

	* To generally enable this feature, set `drbdmanage_late_local_assign` to
	  `True`.

	* To specify which hosts should *not* get a local copy, set
	  `drbdmanage_late_local_assign_exclude` to a comma-separated list of
	  hostnames; this should typically include Glance and the Cinder-controller
	  nodes (but not the Cinder-storage nodes!).

	* Take care to not run out of disk space.



[[s-openstack-performance]]

Here are a few links that show you collected performance data.

  * https://www.3ware.co.jp[Thirdware Inc.] did a Ceph vs. DRBD9 comparison,
    too; the japanese original can be found in their
    https://www.3ware.co.jp/download/technical-docs[technical documentation]
    area.
	A translated (English) version is available on request at sales@linbit.com.

  * http://links.linbit.com/Ceph-DRBD9["__Ceph vs. DRBD9 Performance
	Comparison__"] discusses IOPs, bandwidth, and IO latency; this one needs a
	free registration on the LINBIT site.

