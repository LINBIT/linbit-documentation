[[ch-gfs]]
== DRBDでGFSを使用する

indexterm:[GFS]indexterm:[Global File
System]本章では共有グローバルファイルシステム(GFS)のブロックデバイスをDRBDリソースとするために必要な手順を説明します。GFS、GFS2の両方に対応しています。

DRBD上でGFSを使用するためには、indexterm:[dual-primary
mode]<<s-dual-primary-mode,デュアルプライマリモード>>でDRBDを設定する必要があります。

[IMPORTANT]
===============================
全クラスタファイルシステムには fencingが_必要_ です。DRBDリソース経由だけでなくSTONITHもです。問題のあるノードは _必ず_
killされる必要があります。

次のような設定がよいでしょう。

	net {
		fencing resource-and-stonith;
	}
	handlers {
		# Make sure the other node is confirmed
		# dead after this!
		outdate-peer "/sbin/kill-other-node.sh";
	}

これらは _非揮発性_ のキャッシュである必要があります。 詳細は
https://fedorahosted.org/cluster/wiki/DRBD_Cookbook を参考にしてください。
===============================


[[s-gfs-primer]]
=== GFS基礎

Red Hat Global File System (GFS)は、同時アクセス共有ストレージファイルシステムのRed
Hatによる実装です。同様のファイルシステムのように、GFSでも複数のノードが読み取り/書き込みモードで、安全に同時に同じストレージデバイスにアクセスすることが可能です。これには、クラスタメンバからの同時アクセスを管理するDLM
(Distributed Lock Manager)が使用されています。

本来、GFSは従来型の共有ストレージデバイスを管理するために設計されたものですが、デュアルプライマリモードでDRBDをGFS用のレプリケートされたストレージデバイスとして問題なく使用することができます。アプリケーションについては、読み書きの待ち時間が短縮されるというメリットがあります。
これは、GFSが一般的に実行されるSANデバイスとは異なり、DRBDが通常はローカルストレージに対して読み書きを行うためです。また、DRBDは各GFSファイルシステムに物理コピーを追加して、冗長性を確保します。

GFSはクラスタ対応版のindexterm:[LVM]LVMで、クラスタ化論理ボリュームマネージャ
(indexterm:[CLVM]CLVM)を使用します。このような対応関係が、GFSのデータストレージとしてDRBDを使用することと、<<s-lvm-drbd-as-pv,従来のLVMの物理ボリュームとしてDRBDを使用する>>こととの間に存在します。

GFSファイルシステムは通常はRedHat独自のクラスタ管理フレームワークのindexterm:[Red Hat Cluster
Suite]<<ch-rhcs,Red Hat Cluster>>と密接に結合されています。この章ではDRBDをGFSとともに使用する方法を Red
Hat Clusterの観点から説明します。

GFS、Pacemaker、Red Hat ClusterはRed Hat Enterprise Linux
(RHEL)と、indexterm:[CentOS]CentOSなどの派生ディストリビューションで入手できます。同じソースからビルドされたパッケージがindexterm:[Debian
GNU/Linux]Debian GNU/Linuxでも入手できます。この章の説明は、Red Hat Enterprise
LinuxシステムでGFSを実行することを前提にしています。

[[s-gfs-create-resource]]
=== GFS用のDRBDリソースの作成

GFSは共有クラスタファイルシステムで、すべてのクラスタノードからストレージに対して同時に読み取り/書き込みアクセスが行われることを前提としています。したがって、GFSファイルシステムを格納するために使用するDRBDリソースは<<s-dual-primary-mode,デュアルプライマリモード>>で設定する必要があります。また、<<s-automatic-split-brain-recovery-configuration,スプリットブレインからの自動回復のための機能>>を利用することをおすすめします。そのためには、以下の設定をリソース設定ファイルindexterm:[drbd.conf]に加えてください。

[source, drbd]
----------------------------
resource <resource> {
  net {
    allow-two-primaries yes;
    after-sb-0pri discard-zero-changes;
    after-sb-1pri discard-secondary;
    after-sb-2pri disconnect;
    ...
  }
  ...
}
----------------------------

[WARNING]
===============================
回復ポリシーを設定することは、事実上、自動データロス設定を行うことです。十分にご理解のうえご使用ください。
===============================


これらのオプションを<<ch-configure,新規に設定したリソース>>に追加したら、<<s-first-time-up,通常通りにリソースを初期化>>できます。リソースのindexterm:[drbd.conf]
`allow-two-primaries` オプションが `yes`
に設定されているので、両ノードの<<s-switch-resource-roles,リソースをプライマリにする>>ことができます。

[[s-gfs-configure-lvm]]
=== DRBDリソースを認識するようにLVMを設定する

GFSでは、ブロックデバイスを利用するためにクラスタ対応版のLVMであるCLVMを使用しています。DRBDでCLVMを使用するために、LVMの設定を確認してください。

* クラスタでのロックを行いますので、 `/etc/lvm/lvm.conf` で以下のオプション設定をしてください。
+
[source, drbd]
----------------------------
locking_type = 3
----------------------------

* DRBDベースの物理ボリュームを認識させるためにDRBDデバイスをスキャンします。 これは、従来の(非クラスタの)LVMのように適用されます。
  詳細は<<s-lvm-drbd-as-pv>>を参照してください。

[[s-gfs-enable]]
=== GFS対応のためのクラスタ設定

新規DRBDリソースを作成し、<<s-rhcs-config,初期クラスタ設定を完了>>したら、両ノードで以下のサービスを有効にして起動する必要があります。

* `cman` (同時に `ccsd` と `fenced` を起動します)

* `clvmd`.



[[s-gfs-create]]
=== GFSファイルシステムの作成

デュアルプライマリのDRBDリソースでGFSファイルシステムを作成するために、まず<<s-lvm-primer,LVMの論理ボリューム>>として初期化する必要があります。

従来のクラスタ非対応のLVM設定とは異なり、CLVMはクラスタ対応のため以下の手順は必ず1ノードでのみ実行してください。
indexterm:[LVM]indexterm:[pvcreate (LVM
command)]indexterm:[LVM]indexterm:[vgcreate (LVM
command)]indexterm:[LVM]indexterm:[lvcreate (LVM command)]

----------------------------
# pvcreate /dev/drbd/by-res/<resource>/0
Physical volume "/dev/drbd<num>" successfully created
# vgcreate <vg-name> /dev/drbd/by-res/<resource>/0
Volume group "<vg-name>" successfully created
# lvcreate --size <size> --name <lv-name> <vg-name>
Logical volume "<lv-name>" created
----------------------------

NOTE: この例ではボリュームリソースが1つの場合を前提にしています。

CLVMは即座に変更を対向ノードに伝えます。indexterm:[LVM]indexterm:[lvdisplay (LVM
command)]indexterm:[LVM]indexterm:[lvs (LVM command)]`lvs`
(または`lvdisplay`)を対向ノードで実行すれば、新規作成の論理ボリュームが表示されます。

indexterm:[GFS]ファイルシステムの作成を行います。
----------------------------
# mkfs -t gfs -p lock_dlm -j 2 /dev/<vg-name>/<lv-name>
----------------------------

GFS2ファイルシステムの場合は以下になります。

----------------------------
# mkfs -t gfs2 -p lock_dlm -j 2 -t <cluster>:<name>
	/dev/<vg-name>/<lv-name>
----------------------------

コマンド中の `-j` オプションは、GFSで保持するジャーナルの数を指定します。これは同時にGFSクラスタ内で同時に `プライマリ`
になるノード数と同じである必要があります。DRBD9までは同時に2つより多い `プライマリ`
ノードをサポートしていないので、ここで設定するのは常に2です。

`-t` オプションはGFS2ファイルシステムでのみ使用できます。ロックテーブル名を定義します。ここでは _<cluster>:<name>_
の形式を使用し、 _<cluster>_ は `/etc/cluster/cluster.conf`
で定義したクラスタ名と一致する必要があります。そのため、そのクラスタメンバーのみがファイルシステムを使用することができます。一方で _<name>_
はクラスタ内で一意の任意のファイルシステム名を使用できます。

[[s-gfs-use]]
=== GFSファイルシステムを使用する

ファイルシステムを作成したら、 `/etc/fstab` に追加することができます。

[source, fstab]
----------------------------
/dev/<vg-name>/<lv-name> <mountpoint> gfs defaults 0 0
----------------------------

GFS2の場合にはファイルシステムタイプを変更します。

[source, fstab]
----------------------------
/dev/<vg-name>/<lv-name> <mountpoint> gfs2 defaults 0 0
----------------------------

この変更は必ずクラスタの両ノードで行ってください。

設定が終わったら `gfs` サービスを(両ノードで)開始することで、新規ファイルシステムをマウントすることができます。 indexterm:[GFS]

----------------------------
# service gfs start
----------------------------

以降、システムの起動時にRHCSサービスと gfs
サービスの前にDRBDが自動起動するよう設定してあれば、従来の共有ストレージのようにGFSファイルシステムを使用することができます。
