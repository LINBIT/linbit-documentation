ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

[[ch-admin-manual]]
== 一般的な管理作業

この章では一般的なオペレーションでの管理作業を説明します。トラブルシューティングについては扱いません。トラブルシューティングについては<<ch-troubleshooting>>を参照ください。

include::configure.adoc[]

[[s-check-status]]
=== DRBDのステータスを確認する

[[s-drbd-overview]]
==== `drbd-overview` でステータスを取得する

DRBDのステータスはindexterm:[drbd-overview] `drbd-overview`
ユーティリティで簡単に確認できます(読みやすくするために改行を挿入してあります)。

----------------------------
nina# drbd-overview
  0:r0/0  Connected(*) Seco(*)/Prim(nina) UpTo(*)/Disk(nono) 
            /mnt ext3 1008M 18M 940M 2% 
  1:r1/0  Connected(*) Secondary(*)       UpTo(*)/Disk(nono) 
  5:r2/0  Connected(*) Seco(*)/Prim(nini) UpTo(*)/Disk(nono) 
  6:r2/1  Connected(*) Seco(*)/Prim(nini) UpTo(*)/Disk(nono) 
----------------------------

この出力の意味は以下の通りです。

. `r0` ボリュームの `0` がホスト `nina` (ローカルホスト)で `プライマリ` で、 `ext3`
. ファイルシステムで `/mnt` にマウントされている。
. ホスト `nono` はすべてのリソースに対して `ディスクレス`
  (<<s-drbd-client,DRBDクライアント>>)。他のすべてのホストは全リソースに対して `UpToDate` 。
. `r1` はすべてのホストに対して `セカンダリ` (つまり未使用)。
. `r2` はホスト `nini` で使用している。

`drbd-overview` は厳密かつ簡潔な出力を目的としています。しかしこれらは本来は相反するものです。より詳細な出力が必要な場合には
`drbdsetup status` を直接確認したほうが分かりやすいでしょう。


[[s-proc-drbd]]
==== `/proc/drbd` でのステータス情報

NOTE: ''/proc/drbd''
は非推奨です。8.4系でこの機能を取り除く予定はありませんが、他の方法を使用することをおすすめします。<<s-drbdadm-status>>や、より便利なモニタリングとして<<s-drbdsetup-events2>>など

indexterm:[/proc/drbd]`/proc/drbd` はDRBDモジュールの基本情報を表示する仮想ファイルです。
DRBD8.4まで広く使用されていましたが、DRBD9の情報量を表示するためには対応できません。

----------------------------
$ cat /proc/drbd
version: 9.0.0 (api:1/proto:86-110) FIXME
GIT-hash: XXX build by linbit@buildsystem.linbit, 2011-10-12 09:07:35
----------------------------

1行目にはシステムで使用するDRBDの `バージョン` を表示します。2行目にはビルド特有の情報を表示します。


[[s-drbdadm-status]]
==== `drbdadm` でのステータス情報

indexterm:[drbdadmステータス]一番シンプルなものとして、1つのリソースのステータスを表示します。

----------------------------
# drbdadm status home
home role:Secondary
  disk:UpToDate
  nina role:Secondary
    disk:UpToDate
  nino role:Secondary
    disk:UpToDate
  nono connection:Connecting
----------------------------

ここではリソース `home` がローカルと `nina` と `nino` にあり、 `UpToDate` で `セカンダリ`
であることを示しています。つまり、3ノードが同じデータをストレージデバイスに持ち、現在はどのノードでもデバイスを使用していないという意味です。

ノード `nono` は接続していません。 _Connecting_
のステータスになっています。詳細は<<s-connection-states>>を参照してください。


`drbdsetup` に `--verbose` および/または `--statistics` の引数を付けると、より詳細な情報を得ることができます。

----------------------------
# drbdsetup status home --verbose --statistics
home node-id:1 role:Secondary suspended:no
    write-ordering:none
  volume:0 minor:0 disk:UpToDate
      size:1048412 read:0 written:1048412 al-writes:0 bm-writes:48 upper-pending:0
                                        lower-pending:0 al-suspended:no blocked:no
  nina local:ipv4:10.9.9.111:7001 peer:ipv4:10.9.9.103:7010 node-id:0
                                               connection:Connected role:Secondary
      congested:no
    volume:0 replication:Connected disk:UpToDate resync-suspended:no
        received:1048412 sent:0 out-of-sync:0 pending:0 unacked:0
  nino local:ipv4:10.9.9.111:7021 peer:ipv4:10.9.9.129:7012 node-id:2
                                               connection:Connected role:Secondary
      congested:no
    volume:0 replication:Connected disk:UpToDate resync-suspended:no
        received:0 sent:0 out-of-sync:0 pending:0 unacked:0
  nono local:ipv4:10.9.9.111:7013 peer:ipv4:10.9.9.138:7031 node-id:3
                                                           connection:Connecting
----------------------------


この例では、ローカルノードについては多少異なりますが、このリソースで使用しているノードすべてを数行ごとにブロックで表示しています。以下で詳細を説明します。

各ブロックの最初の行は `node-id` です。(現在のリソースに対してのものです。ホストは異なるリソースには異なる `node-id` がつきます)
また、 `role` (<<s-roles>>参照)も表示されています。

次に重要な行が、 `volume` の表示がある行です。通常は0から始まる数字ですが、設定によっては別のIDをつけることもできます。この行では
`replication` でindexterm:[connection
state]コネクションステータス(<<s-connection-states>>参照)を、 `disk`
でリモートのディスク状態indexterm:[disk state](<<s-disk-states>>参照)が表示されます。
また、ボリュームの統計情報が少し表示される行もあります。データの `received` 、 `sent` , `out-of-sync`
などです。詳細は <<s-performance-indicators>>と<<s-conn-info>>を参照してください。

ローカルノードでは、最初の行はリソース名を表示します。この例では `home` です。最初の行には常にローカルノードが表示されますので、
`Connection` やアドレス情報は表示されません。

より詳細な情報については、 `drbd.conf` マニュアルページをご覧ください。

この例のブロックになっている他の4行は、すべての設定のあるDRBDデバイスごとになっており、最初にデバイスマイナー番号がついています。この場合にはデバイス
`/dev/drbd0` に対応して `0` です。

リソースごとの出力には様々なリソースに関する情報が含まれています。


[[s-drbdsetup-events2]]
==== `drbdsetup events2` を使用した一回のみ、またはリアルタイムでの監視

NOTE: この機能はユーザスペースのDRBDが8.9.3より後のバージョンでのみ使用できます

これはDRBDから情報を取得するための下位機能です。監視など自動化ツールでの使用に向いています。

一番シンプルな使用方法では、以下のように現在のステータスのみを表示します(端末上で実行した場合には色も付いています)。

---------------
# drbdsetup events2 --now r0
exists resource name:r0 role:Secondary suspended:no
exists connection name:r0 peer-node-id:1 conn-name:remote-host connection:Connected role:Secondary
exists device name:r0 volume:0 minor:7 disk:UpToDate
exists device name:r0 volume:1 minor:8 disk:UpToDate
exists peer-device name:r0 peer-node-id:1 conn-name:remote-host volume:0
    replication:Established peer-disk:UpToDate resync-suspended:no
exists peer-device name:r0 peer-node-id:1 conn-name:remote-host volume:1
    replication:Established peer-disk:UpToDate resync-suspended:no
exists -
---------------

''--now'' を付けないで実行した場合には動作し続け、以下のように更新を続けます。

-----------------
# drbdsetup events2 r0
...
change connection name:r0 peer-node-id:1 conn-name:remote-host connection:StandAlone
change connection name:r0 peer-node-id:1 conn-name:remote-host connection:Unconnected
change connection name:r0 peer-node-id:1 conn-name:remote-host connection:Connecting
-----------------

そして監視用途に、''--statistics''という別の引数もあります。これはパフォーマンスその他のカウンタを作成するものです。

''drbdsetup'' の詳細な出力(読みやすいように一部の行は改行しています)

-----------------
# drbdsetup events2 --statistics --now r0
exists resource name:r0 role:Secondary suspended:no write-ordering:drain
exists connection name:r0 peer-node-id:1 conn-name:remote-host connection:Connected
                                                        role:Secondary congested:no
exists device name:r0 volume:0 minor:7 disk:UpToDate size:6291228 read:6397188
            written:131844 al-writes:34 bm-writes:0 upper-pending:0 lower-pending:0
                                                         al-suspended:no blocked:no
exists device name:r0 volume:1 minor:8 disk:UpToDate size:104854364 read:5910680
          written:6634548 al-writes:417 bm-writes:0 upper-pending:0 lower-pending:0
                                                         al-suspended:no blocked:no
exists peer-device name:r0 peer-node-id:1 conn-name:remote-host volume:0
          replication:Established peer-disk:UpToDate resync-suspended:no received:0
                                      sent:131844 out-of-sync:0 pending:0 unacked:0
exists peer-device name:r0 peer-node-id:1 conn-name:remote-host volume:1
          replication:Established peer-disk:UpToDate resync-suspended:no received:0
                                     sent:6634548 out-of-sync:0 pending:0 unacked:0
exists -
-----------------

''--timestamp'' パラメータも便利な機能です。


[[s-connection-states]]
==== コネクションステータス

indexterm:[drbdadm cstate]indexterm:[connection state]リソースのコネクションステータスは
`drbdadm cstate` コマンドで確認することができます。

----------------------------
# drbdadm cstate <resource>
Connected
Connected
StandAlone
----------------------------

確認したいのが１つのコネクションステータスだけの場合にはコネクション名を指定してください。

デフォルトでは設定ファイルに記載のある対向ノードのホスト名です。

----------------------------
# drbdadm cstate <peer>:<resource>
Connected
----------------------------

リソースのコネクションステータスには次のようなものがあります。

._StandAlone_
indexterm:[connection
state]ネットワーク構成は使用できません。リソースがまだ接続されていない、管理上の理由で切断している(`drbdadm
disconnect`を使用)、認証の失敗またはスプリットブレインにより接続が解除された、のいずれかが考えられます。

._Disconnecting_
indexterm:[connection state]切断中の一時的な状態です。次の状態は `StandAlone` です。

._Unconnected_
indexterm:[connection state, Unconnected]接続を試行する前の一時的な状態です。次に考えられる状態は、
_Connecting_ です。

._Timeout_
indexterm:[connection state]対向ノードとの通信のタイムアウト後の一時的な状態です。次の状態は `Unconnected`
です。

._BrokenPipe_
indexterm:[connection state]対向ノードとの接続が失われた後の一時的な状態です。次の状態は `Unconnected` です。

._NetworkFailure_
indexterm:[connection state]対向ノードとの接続が失われた後の一時的な状態です。次の状態は `Unconnected` です。

._ProtocolError_
indexterm:[connection state]対向ノードとの接続が失われた後の一時的な状態です。次の状態は `Unconnected` です。

._TearDown_
indexterm:[connection state]一時的な状態です。対向ノードが接続を閉じています。次の状態は `Unconnected` です。

._Connecting_
indexterm:[connection state, Connecting]対向ノードがネットワーク上で可視になるまでノードが待機します。

._Connected_
indexterm:[connection state,
Connected]DRBDの接続が確立され、データミラー化がアクティブになっています。これが正常な状態です。

[[s-replication-states]]
==== 複製ステータス

各ボリュームは接続を介した複製ステータスを持ちます。可能な複製ステータスは以下になります。

._Off_
indexterm:[replication state, Off]ボリュームはこの接続を通して複製されていません。接続が _Connected_
になっていません。す。次の状態は `Unconnected` です。

._Established_
indexterm:[replication state,
Established]このボリュームへのすべての書き込みがオンラインで複製されています。これは通常の状態です。

._StartingSyncS_
indexterm:[replication
state,StartingSyncS]管理者により開始されたフル同期が始まっています。次に考えられる状態は `SyncSource` または
`PausedSyncS` です。

._StartingSyncT_
indexterm:[replication state, StartingSyncT]管理者により開始されたフル同期が始まっています。次の状態は
_WFSyncUUID_ です。

._WFBitMapS_
indexterm:[replication state, WFBitMapS]部分同期が始まっています。次に考えられる状態は _SyncSource_
または _PausedSyncS_ です。

._WFBitMapT_
indexterm:[replication state, WFBitMapT]部分同期が始まっています。次に考えられる状態は `WFSyncUUID`
です。

._WFSyncUUID_
indexterm:[replication state, WFSyncUUID]同期が開始されるところです。次に考えられる状態は
_SyncTarget_ または _PausedSyncT_ です。

._SyncSource_
indexterm:[replication state, SyncSource]現在、ローカルノードを同期元にして同期を実行中です。

._SyncTarget_
indexterm:[replication state, SyncTarget]現在、ローカルノードを同期先にして同期を実行中です。

._PausedSyncS_
indexterm:[replication state,
PausedSyncS]ローカルノードが進行中の同期の同期元ですが、現在は同期が一時停止しています。原因として、別の同期プロセスの完了との依存関係、または
`drbdadm pause-sync` を使用して手動で同期が中断されたことが考えられます。

._PausedSyncT_
indexterm:[replication state,
PausedSyncT]ローカルノードが進行中の同期の同期先ですが、現在は同期が一時停止しています。原因として、別の同期プロセスの完了との依存関係、または
`drbdadm pause-sync` を使用して手動で同期が中断されたことが考えられます。

._VerifyS_
indexterm:[replication state, VerifyS]ローカルノードを照合元にして、オンラインデバイスの照合を実行中です。

._VerifyT_
indexterm:[replication state, VerifyT]現在、ローカルノードを照合先にして、オンラインデバイスの照合を実行中です。

._Ahead_
indexterm:[replication state, Ahead]リンクが負荷に対応できないので、データの複製が中断しました。このステータスは
`on-congestion` オプションの設定で有効にできます(<<s-configure-congestion-policy>>を参照)。

._Behind_
indexterm:[replication state,
Behind]リンクが負荷に対応できないので、データの複製が対抗ノードによって中断されました。このステータスは対抗ノードの
`on-congestion` オプション設定で有効にできま>す(<<s-configure-congestion-policy>>を参照)。

[[s-roles]]
==== リソースのロール

indexterm:[resource]リソースのロールはindexterm:[drbdadm role]`drbdadm role`
コマンドを実行することで確認できます。

----------------------------
# drbdadm role <resource>
Primary
----------------------------

以下のいずれかのリソースのロールが表示されます。

._Primary_
リソースは現在プライマリロールで読み書き加能です。2つのノードの一方だけがこのロールになることができます。ただし、<<s-dual-primary-mode,デュアルプライマリモード>>の場合は例外です。

._Secondary_
リソースは現在セカンダリロールです。対向ノードから正常に更新を受け取ることができますが(切断モード以外の場合)、このリソースに対して読み書きは実行できません。1つまたは両ノードがこのロールになることができます。

._Unknown_
リソースのロールが現在不明です。ローカルリソースロールがこの状態になることはありません。切断モードの場合に、対向ノードのリソースロールにのみ表示されます。


[[s-disk-states]]
==== ディスク状態

リソースのディスク状態は `drbdadm dstate` コマンドを実行することで確認できます。

----------------------------
# drbdadm dstate <resource>
UpToDate
----------------------------

ディスク状態は以下のいずれかです。

._Diskless_
indexterm:[disk
state]DRBDドライバにローカルブロックデバイスが割り当てられていません。原因として、リソースが下位デバイスに接続されなかった、 `drbdadm
detach` を使用して手動でリソースを切り離した、または下位レベルのI/Oエラーにより自動的に切り離されたことが考えられます。

._Attaching_
indexterm:[disk state]メタデータ読み取り中の一時的な状態です。

._Detaching_
indexterm:[disk state, Detaching]切断され、進行中のIO処理が完了するのを待っている一時的な状態。

._Failed_
indexterm:[disk state]ローカルブロックデバイスがI/O障害を報告した後の一時的な状態です。次の状態は `Diskless` です。

._Negotiating_
indexterm:[disk state]すでに `Connected` のDRBDデバイスで `attach` が実行された場合の一時的状態です。

._Inconsistent_
indexterm:[disk
state]データが一致しません。新規リソースを作成した直後に(初期フル同期の前に)両方のノードがこの状態になります。また、同期中には片方のノード(同期先)がこの状態になります。

._Outdated_
indexterm:[disk state]リソースデータは一致していますが、<<s-outdate,無効>>です。

._DUnknown_
indexterm:[disk state]ネットワーク接続を使用できない場合に、対向ノードディスクにこの状態が使用されます。

._Consistent_
indexterm:[disk state]接続していない状態でノードのデータが一致しています。接続が確立すると、データが _UpToDate_ か
_Outdated_ か判断されます。

._UpToDate_
indexterm:[disk state]データが一致していて最新の状態です。これが正常な状態です。


[[s-conn-info]]
==== 接続情報データ

._local_
ネットワークファミリ、ローカルアドレス、対向ノードから接続を許可されたポートを表示します。

._peer_
ネットワークファミリ、ローカルアドレス、接続に使用しているポートを表示します。

._congested_
データのTCP送信バッファを80%より多く使用している場合にこのフラグがつきます。


[[s-performance-indicators]]
==== パフォーマンス指標

indexterm:[performance indicators] `drbdadm status` の出力には、以下のカウンタと指標があります。

._send_ (network send)
ネットワークを通じて相手に送信された正味データ量(kibyte単位)。

._receive_ (network receive)
ネットワークを通じて相手から受信した正味データ量(kibyte単位)。

._read_ (disk write)
ローカルのハードディスクに書き込んだ正味データ量(kibyte単位)。

._written_ (disk read)
ローカルのハードディスクから読み込んだ正味データ量(kibyte単位)。

._al-writes_ (activity log)
メタデータのアクティビティログエリアのアップデート数。

._bm-writes_ (bit map)
メタデータのビットマップエリアのアップデート数。

._lower-pending_ (local count)
DRBDが発行したローカルI/Oサブシステムへのオープンリクエストの数。

._pending_
相手側に送信したが相手から応答がないリクエスト数。

._unacked_ (unacknowledged)
ネットワーク接続を通じて相手側が受信したが応答がまだないリクエスト数。

._upper-pending_ (application pending)
まだDRBDから応答がないDRBDへのブロックI/Oリクエスト数。

._write-ordering_ (write order)
現在使用中の書き込み順序制御方法： `b`(barrier)、 `f`(flush)、 `d`(drain)、 `n`(none)

._out-of-sync_
indexterm:[out-of-sync]indexterm:[oos]現在同期していない正味ストレージ容量(kibyte単位)

._resync-suspended_
現在再同期が中断されているかどうか。値は `no` 、 `user` 、 `peer` 、 `dependency` のいずれかです。

._blocked_
indexterm:[blocked]ローカルI/Oの輻輳を示します。
--

  * _no_:輻輳なし
  * _uppeer_ : ファイルシステムなどDRBD _より上位_ のI/Oがブロックされている。代表的なものには以下がある。 **
    管理者によるI/O中断。 `drbdadm` コマンドの `suspend-io` を参照。 ** アタッチ/デタッチ時の一時的なブロック。 **
    バッファーの枯渇。<<p-performance>>参照。 ** ビットマップIO待ち
  * _lower_: 下位デバイスの輻輳
--

_upper、lower_ の値を見ることも可能です。


[[s-enable-disable]]
=== リソースの有効化と無効化

[[s-enable-resource]]
==== リソースの有効化

indexterm:[resource]通常、自動的にすべての設定済みDRBDリソースが有効になります。これは、

* クラスタ構成に応じたクラスタ管理アプリケーションの操作によるものであり、

* またはシステム起動時の `/etc/init.d/drbd` によっても有効になります。

手動でリソースを起動する必要がある場合には、以下のコマンドの実行によって行うことができます。

----------------------------
# drbdadm up <resource>
----------------------------

他の場合と同様に、特定のリソース名の代わりにキーワード `all` を使用すれば、 `/etc/drbd.conf`
で設定しているすべてのリソースを一度に有効にできます。

[[s-disable-resource]]
==== リソースを無効にする

indexterm:[resource]特定のリソースを一時的に無効にするには、次のコマンドを実行します。

----------------------------
# drbdadm down <resource>
----------------------------

ここでも、リソース名の代わりにキーワード `all` を使用して、1回で `/etc/drbd.conf`
に記述しているすべてのリソースを一時的に無効にできます。

[[s-reconfigure]]
=== リソースの再設定

indexterm:[resource]DRBDの動作中にリソースを再設定することができます。次の手順を行います。

* `/etc/drbd.conf` のリソース設定を変更します。

* 両方のノードで `/etc/drbd.conf` ファイルを同期します。

* indexterm:[drbdadm]`drbdadm adjust <resource>`コマンドを 両ノードで実行します。

`drbdadm adjust` は `drbdsetup` を通じて実行中のリソースを調整します。保留中の `drbdsetup`
呼び出しを確認するには、 `drbdadm` を `-d` (dry-run,予行演習)オプションを付けて実行します。

NOTE: `/etc/drbd.conf` の `common` セクションを変更して一度にすべてのリソースに反映させたいときには `drbdadm adjust
all` を実行します。

[[s-switch-resource-roles]]
=== リソースの昇格と降格

indexterm:[resource]手動で<<s-resource-roles,リソースロール>>をセカンダリからプライマリに切り替える(昇格)、またはその逆に切り替える(降格)には、次のコマンドを実行します。

----------------------------
# drbdadm primary <resource>
# drbdadm secondary <resource>
----------------------------

DRBDが<<s-single-primary-mode,シングルプライマリモード>>(DRBDのデフォルト)で、<<s-connection-states,コネクションステータス>>が
`Connected`
の場合、任意のタイミングでどちらか1つのノード上でのみリソースはプライマリロールになれます。したがって、あるリソースが他のノードに対してプライマリロールになっているときに
`drbdadm primary <resource>` を実行すると、エラーが発生します。

リソースが<<s-dual-primary-mode,デュアルプライマリモード>>に対応するよう設定している場合には、両方のノードをプライマリロールに切り替えることができます。これは、例えば仮想マシンのオンラインマイグレーションの際に利用できます。


[[s-manual-fail-over]]
=== 基本的な手動フェイルオーバ

Pacemakerを使わず、パッシブ/アクティブ構成でフェイルオーバを手動で制御するには次のようにします。

現在のプライマリノードでDRBDデバイスを使用するすべてのアプリケーションとサービスを停止し、リソースをセカンダリに降格します。

----------------------------
# umount /dev/drbd/by-res/<resource>/<vol-nr>
# drbdadm secondary <resource>
----------------------------

プライマリにしたいノードでリソースを昇格してデバイスをマウントします。

----------------------------
# drbdadm primary <resource>
# mount /dev/drbd/by-res/<resource>/<vol-nr> <mountpoint>
----------------------------

`自動プロモート`
機能を使用している場合はロール(`プライマリ`/`セカンダリ`)を手動で変更する必要はありません。それぞれサービス停止とアンマウント、マウントの操作のみ必要です。


[[s-upgrading-drbd]]
=== DRBDのアップグレード

DRBDのアップグレードは非常にシンプルな手順です。この章ではDRBD8.4から9.0へのアップグレード手順を説明します。DRBD9系内でのアップグレードの手順についてはより簡単ですので<<s-upgrade-within-9,以下>>の項目を参照ください。

[[s-upgrade-overview]]
==== 概要

8.4から9.0へのアップグレード手順の概要は以下の通りです。

  * <<s-updating-your-repo,新しいリポジトリ>>を設定する。(LINBITのパッケージを使っている場合)
  * 現在の状態が<<s-upgrade-check,問題ない>>事を確認する。
  * クラスタ管理ソフトを<<s-upgrade-pausing-the-cluster,停止>>する。
  * <<s-Upgrading-the-packages,新しいバージョン>>をインストールする。
  * 2ノード以上を移動させたい場合は、追加メタデータ用のスペースを加えるために下位レベルデバイスをリサイズする必要がある。この点については<<ch-lvm,LVMの章>>参照。
  * リソースの設定を取り除き、DRBD8.4をアンロードし、そして<<s-upgrade-reload-kernel-mod,v9のカーネルモジュールをロード>>する。
  * `v09` の形式に<<s-upgrade-convert,DRBDメタデータをコンバート>>する。ビットマップ数も同様の手順。
  * <<s-upgrade-start-drbd,リソースを起動>>をして完了。


[[s-updating-your-repo]]
==== リポジトリのアップデート

8.4と9.0のブランチ間の様々な変更のため、各々に別のリポジトリを作成しています。各サーバでリポジトリのアップデートを行います。

[[s-RHEL-systems]]
===== RHEL/CentOSのシステム

`/etc/yum.repos.d/linbit.repo` ファイルに以下の変更を反映します。

----------------------------
[drbd-9.0]
name=DRBD 9.0
baseurl=http://packages.linbit.com/<hash>/yum/rhel7/drbd-9.0/<arch>
gpgcheck=0
----------------------------

NOTE: <hash>と<arch>の箇所は置き換えてください。<hash>はLINBITサポートから提供されるものです。

[[s-Debian-Systems]]
===== Debian/Ubuntuのシステム

`/etc/apt/sources.list`(または `/etc/apt/sources.d/` にあるファイル)に以下の変更を反映します。

----------------------------
deb http://packages.linbit.com/<hash>/ stretch drbd-9.0
----------------------------

`wheezy` リリースを使用していない場合でも、また他のリリースの場合でも、この変更は必要です。

NOTE: <hash>の箇所は置き換えてください。<hash>はLINBITサポートから提供されるものです。


次にDRBDの署名キーを信頼済みキーに追加してもよいでしょう。

----------------------------
# gpg --keyserver subkeys.pgp.net --recv-keys  0x282B6E23
# gpg --export -a 282B6E23 | apt-key add -
----------------------------

最後に `apt-get update` を実行してリポジトリのアップデートを認識させます。

----------------------------
# apt-get update
----------------------------

[[s-upgrade-check]]
==== DRBDの状態を確認する

リソースが同期していることを確認します。`cat /proc/drbd` が `UpToDate/UpToDate`
を出力しています。(これは、DRBD8系 _以下_ でのみ有効です)

----------------------------
bob# cat /proc/drbd

version: 8.4.9-1 (api:1/proto:86-101)
GIT-hash: e081fb0570183db40caa29b26cb8ee907e9a7db3 build by linbit@buildsystem, 2016-11-18 14:49:21

 0: cs:Connected ro:Secondary/Secondary ds:UpToDate/UpToDate C r-----
    ns:0 nr:211852 dw:211852 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:d oos:0
----------------------------


[[s-upgrade-pausing-the-cluster]]
==== クラスタを停止する

リソースが同期している事を確認したら、セカンダリノードをアップグレードします。この手順は手動で行うことができます。またはPacemakerを使用している場合にはノードをスタンバイモードにしておきます。両手順は以下の通りです。Pacemakerの動作中は手動では操作しないでください。

* 手動手順
----------------------------
bob# /etc/init.d/drbd stop
----------------------------

* Pacemaker

セカンダリノードをスタンバイモードにします。この例では `bob` はセカンダリです。

----------------------------
bob# crm node standby bob
----------------------------

NOTE: クラスタの状態を 'crm_mon -rf' で確認することができます。または、リソースの状態を、もし "Unconfigured"
と表示されていなければ 'cat /proc/drbd' で確認することができます。


[[s-Upgrading-the-packages]]
==== パッケージのアップグレード

パッケージをyumまたはaptでアップデートします。

----------------------------
bob# yum upgrade
----------------------------

----------------------------
bob# apt-get upgrade
----------------------------

アップグレードが完了すると、最新のDRBD9のカーネルモジュールとdrbd-utilsがセカンダリノードの `bob` にインストールされています。

しかしカーネルモジュールはまだ有効化されていません。

[[s-upgrade-reload-kernel-mod]]
==== 新しいカーネルモジュールのロード

現時点でDRBDモジュールは使用していませんので、以下コマンドでアンロードします。

-------------
bob# rmmod drbd
-------------

" `ERROR : Module drbd is in use` "
のようなメッセージが出る場合は、まだすべてのリソースが正常に停止していません。<<s-upgrading-drbd>>を再実行し、そして/またはどのリソースがまだ稼働しているのか確認するため
`drbdadm down all` を実行します。

よくあるアンロードを妨げる原因には以下のものがあります。

  * DRBDが下位デバイスのファイルシステムにNFSエクスポートがある( `exportfs -v` の出力を確認)
  * ファイルシステムをマウント中 - `grep drbd /proc/mounts` を確認
  * ループバックデバイスがアクティブ ( `losetup -l` )
  * デバイスマッパーが直接または間接的にDRBDを使用している。( `dmsetup ls --tree` )
  * DRBDデバイスが物理ボリュームとなっていて、そのLVMがアクティブ ( `pvs` )

上記はよくあるケースであり、すべてのケースを網羅するものではないという事に注意ください。


これで、DRBDモジュールをロードすることができます。

-------
bob# modprobe drbd
-------

`/proc/drbd`
の出力を確認し、正しく(新しい)バージョンがロードされたか確認してください。もしインストールされているパッケージが間違ったカーネルバージョンのものだった場合には、
`modprobe` は有効ですが、古いバージョンが残っていれば再び有効にすることもできます。

'cat /proc/drbd' の出力は9.0.xを表示し、次のようになっているでしょう。

----------------------------
version: 9.0.0 (api:2/proto:86-110)
GIT-hash: 768965a7f158d966bd3bd4ff1014af7b3d9ff10c build by root@bob, 2015-09-03 13:58:02
Transports (api:10): tcp (1.0.0)
----------------------------

NOTE: プライマリノードのaliceでは、 'cat /proc/drbd' はアップグレードをするまでは以前のバージョンを表示します。


[[s-migrating_your_configuration_files]]
==== 設定ファイルの移行

DRBD
9.0は8.4の設定ファイルに後方互換性がありますが、一部の構文は変更しています。変更点の全一覧は<<s-recent-changes-config>>を参照してください。'drbdadm
dump all'
コマンドを使えば古い設定を簡単に移すことができます。これは、新しいリソース設定ファイルに続いて新しいグローバル設定も両方とも出力します。この出力を使って適宜変更してください。

[[s-upgrade-convert]]

==== メタデータの変更 

次にオンディスクのメタデータを新しいバージョンに変更します。この手順はとても簡単で、1つコマンドを実行して2つの質問に答えるだけです。


たくさんのノードを変更したい場合には、追加のビットマップに十分なスペースを確保するため、すでに下位デバイスの容量を拡張していることでしょう。この場合には以下のコマンドを追加の引数
`--max-peers=<N>` を付けて実行します。対向ノードの数(予定)を決める時には、<<s-drbd-client>>も考慮に入れてください。

DRBDのメタデータのアップグレードはとても簡単で、1つコマンドを実行して2つの質問に答えるだけです。

-----------------
# drbdadm create-md <resource>
You want me to create a v09 style flexible-size internal meta data block.
There appears to be a v08 flexible-size internal meta data block
already in place on <disk> at byte offset <offset>

Valid v08 meta-data found, convert to v09?
[need to type 'yes' to confirm] yes

md_offset <offsets...>
al_offset <offsets...>
bm_offset <offsets...>

Found some data

 ==> This might destroy existing data! <==

Do you want to proceed?
[need to type 'yes' to confirm] yes

Writing meta data...
New drbd meta data block successfully created.
success
-----------------

もちろん、リソース名を `all` にすることも可能です。また、以下のようにすれば質問されるのを回避することができます(コマンド内の順序が重要です)。

--------
drbdadm -v --max-peers=<N>  -- --force create-md <resources>
--------


[[s-upgrade-start-drbd]]
==== DRBDを再び起動する

あとはDRBDデバイスを再びupにして起動するだけです。単純に、 `drbdadm up all` で済みます。

次は、クラスタ管理ソフトを使用しているか、手動でリソースを管理しているかによって方法が異なります。

* 手動
+
--
----------------------------
bob# /etc/init.d/drbd start
----------------------------
--

* Pacemaker
+
--
----------------------------
# crm node online bob
----------------------------

これによってDRBDは他のノードに接続し、再同期プロセスが開始します。
--

すべてのリソースが2つのノードで `UpToDate` になったら、アップグレードしたノード(ここでは `bob`
)にアプリケーションを移動させることができます。そして、まだ8.4が稼働しているノードで同じ手順を行ってください。


[[s-upgrade-within-9]]
==== DRBD9からDRBD9

既にDRBD9を使っている場合には<<s-Upgrading-the-packages,新しいバージョンのパッケージをインストール>>することができます。クラスタノードを<<s-upgrade-pausing-the-cluster,`スタンバイ`>>にして、カーネルモジュールを<<s-upgrade-reload-kernel-mod,アンロード/リロード>>し、<<s-upgrade-start-drbd,リソースを起動>>、そしてクラスタノードを再度
`online` にします。footnote:[少なくとも、過去にあった書き込み時の状態;)]

個々の手順については上述の項の通りですので、ここでは省略します。


[[s-enable-dual-primary]]
=== デュアルプライマリモードを有効にする

デュアルプライマリモードではリソースが複数ノードで同時にプライマリになることができます。永続的でも一時的なものでも可能です。

[NOTE]
===============================
デュアルプライマリモードではリソースが同期レプリケート(プロトコルC)で設定されていることが必要です。そのためレイテンシに過敏となり、WAN環境には向いていません。

さらに、両リソースが常にプライマリとなるので、いかなるノード間のネットワーク不通でもスプリットブレインが発生します。
===============================

WARNING: DRBD9.0.xではデュアルプライマリをサポートしていません。"動いてしまう"
かもしれませんが、動作は保証できません。設定が変更になる可能性が高いためです。例えば `allow-two-primaries`
は間違いになります。9.1系では複数プライマリが可能となりサポートする予定です。footnote:[ロードマップをご参照ください:
http://drbd.linbit.com/home/roadmap/]

[[s-enable-dual-primary-permanent]]
==== 永続的なデュアルプライマリモード

indexterm:[multiple primaries]indexterm:[dual-primary
mode]デュアルプライマリモードを有効にするため、リソース設定の `net` セクションで、 `allow-two-primaries` オプションを
`yes` に指定します。

[source, drbd]
----------------------------
resource <resource>
  net {
    protocol C;
    allow-two-primaries yes;
    fencing resource-and-stonith;
  }
  handlers {
    fence-peer "...";
    unfence-peer "...";
  }
  ...
}
----------------------------

そして、両ノード間で設定を同期することを忘れないでください。両ノードで `drbdadm adjust <resource>` を実行してください。

これで `drbdadm primary <resource>` で、両ノードを同時にプライマリのロールにすることができます。

CAUTION: 適切なフェンシングポリシーを常に実装すべきです。フェンシングなしで 'allow-two-primaries'
を設定するのは危険です。これはフェンシングなしで、シングルプライマリを使うことより危険になります。

[[s-enable-dual-primary-temporary]]
==== 一時的なデュアルプライマリモード

通常はシングルプライマリで稼動しているリソースを、一時的にデュアルプライマリモードを有効にするには次のコマンドを実行してください。

----------------------------
# drbdadm net-options --protocol=C --allow-two-primaries <resource>
----------------------------

一時的なデュアルプライマリモードを終えるには、上記と同じコマンドを実行します。ただし `--allow-two-primaries=no`
としてください(また、適切であれば希望するレプリケーションプロトコルにも)。


[[s-use-online-verify]]
=== オンラインデバイス照合の使用

[[s-online-verify-enable]]
==== オンライン照合を有効にする

indexterm:[on-line device
verification]<<s-online-verify,オンラインデバイス照合>>はデフォルトでは有効になっていません。有効にするには
`/etc/drbd.conf` のリソース設定に以下の行を追加します。

[source, drbd]
----------------------------
resource <resource>
  net {
    verify-alg <algorithm>;
  }
  ...
}
----------------------------

_<algorithm>_ は、システムのカーネル構成内のカーネルcrypto
APIでサポートされる任意のメッセージダイジェストアルゴリズムです。通常は `sha1` 、 `md5` 、 `crc32c` から選択します。

既存のリソースに対してこの変更を行う場合は、 `drbd.conf` を対向ノードと同期し、両方のノードで `drbdadm adjust
<resource>` を実行します。

[[s-online-verify-invoke]]
==== オンライン照合の実行

indexterm:[on-line device verification]オンライン照合を有効にしたら、次のコマンドでオンライン照合を開始します。

----------------------------
# drbdadm verify <resource>
----------------------------

コマンドを実行すると、DRBDが `_<resource>_`
に対してオンライン照合を実行します。同期していないブロックを検出した場合は、ブロックに非同期のマークを付け、カーネルログにメッセージを書き込みます。このときにデバイスを使用しているアプリケーションは中断なく動作し続けます。また、<<s-switch-resource-roles,リソースロールの切り替え>>も行うことができます。

照合中に同期していないブロックが検出された場合は、照合の完了後に、次のコマンド使用して再同期できます。

----------------------------
# drbdadm disconnect <resource>
# drbdadm connect <resource>
----------------------------


[[s-online-verify-automate]]
==== 自動オンライン照合

indexterm:[on-line device
verification]通常は、オンラインデバイス照合を自動的に実行するほうが便利です。自動化は簡単です。*一方* のノードに
`/etc/cron.d/drbd-verify` という名前で、次のような内容のファイルを作成します。

[source, drbd]
----------------------------
42 0 * * 0    root    /sbin/drbdadm verify <resource>
----------------------------

これにより、毎週日曜日の午前0時42分に、 `cron`
がデバイス照合を呼び出します。そのため、月曜の朝にリソース状態をチェックして結果を確認することができます。デバイスが大きくて32時間では足りなかった場合、
コネクションステータスが `VerifyS` または `VerifyT` になっています。これは `verify`
がまだ動作中であることを意味しています。

オンライン照合をすべてのリソースで有効にした場合（例えば `/etc/drbd.d/global_common.conf` の `common`
セクションに `verify-alg <アルゴリズム>` を追加するなど)には、以下のようにします。

[source, drbd]
----------------------------
42 0 * * 0    root    /sbin/drbdadm verify all
----------------------------


[[s-configure-sync-rate]]
=== 同期速度の設定

indexterm:[synchronization]バックグラウンド同期中は同期先のデータとの一貫性が一時的に失われるため、同期はできるだけ短時間で完了させるべきです。ただし、すべての帯域幅がバックグラウンド同期に占有されてしまうと、フォアグラウンドレプリケーションに使用できなくなり、アプリケーションのパフォーマンス低下につながります。これは避ける必要があります。同期用の帯域幅はハードウェアに合わせて設定する必要があります。

IMPORTANT: 同期速度をセカンダリノードの最大書き込みスループットを上回る速度に設定しても意味がありません。デバイス同期の速度をどれほど高速に設定しても、セカンダリノードがそのI/Oサブシステムの能力より高速に書き込みを行うことは不可能です。

また、同じ理由で、同期速度をレプリケーションネットワークの帯域幅の能力を上回る速度に設定しても意味がありません。


[[s-estimating_a_synchronization_speed]]
==== 同期速度の計算

TIP: 概算としては使用可能なレプリケーション帯域幅の30%程度を想定するのがよいでしょう。400MB/sの書き込みスループットを維持できるI/Oサブシステム、および110MB/sのネットワークスループットを維持できるギガビットイーサネットネットワークの場合は、ネットワークがボトルネックになります。速度は次のように計算できます。

[[eq-sync-rate-example1]]
.syncer 速度の例(有効帯域幅が110MB/sの場合)
image::images/sync-rate-example1.svg[]

この結果、 `resynce-rate` オプションの推奨値は `33M` になります。

一方、最大スループットが80MB/sのI/Oサブシステム、およびギガビットイーサネット接続を使用する場合は、I/Oサブシステムが律速要因になります。速度は次のように計算できます。

[[eq-sync-rate-example2]]
.syncer 速度の例(有効帯域幅が80MB/sの場合)
image::images/sync-rate-example2.svg[]

この場合、 `resync-rate` オプションの推奨値は `24M` です。

同じようにして800MB/sのストレージ速度で10Gbeのネットワークコネクションであれば、〜240MB/sの同期速度が得られます。

[[s-configure-sync-rate-variable]]
==== 可変同期速度設定

複数のDRBDリソースが1つのレプリケーション/同期ネットワークを共有する場合、同期が固定レートであることは最適なアプローチではありません。そのためDRBD
8.4.0から可変同期速度がデフォルトで有効になっています。このモードではDRBDが自動制御のループアルゴリズムで同期速度を決定して調整を行います。このアルゴリズムはフォアグラウンド同期に常に十分な帯域幅を確保し、バックグラウンド同期がフォアグラウンドのI/Oに与える影響を少なくします。

最適な可変同期速度の設定は、使用できるネットワーク帯域幅、アプリケーションのI/Oパターンやリンクの輻輳によって変わります。<<s-drbd-proxy,DRBD
Proxy>>の有無によっても適切な設定は異なります。DRBDの機能を最適化するためにコンサルタントを利用するのもよいでしょう。以下は(DRBD
Proxy使用を想定した環境での)設定の _一例_ です。

[source, drbd]
----------------------------
resource <resource> {
  disk {
    c-plan-ahead 5;
    c-max-rate 10M;
    c-fill-target 2M;
  }
}
----------------------------

TIP: `c-fill-target` の初期値は _BDP✕2_ がよいでしょう。 _BDP_ とはレプリケーションリンク上の帯域幅遅延積(Bandwidth
Delay Product)です。

例えば、1GBit/sの接続の場合、200µsのレイテンシになります。footnote:[概算は `ping`
の結果を使用しています]1GBit/sは約120MB/sです。120掛ける200*10^-6^秒は24000Byteです。この値はMB単位まで丸めてもよいでしょう。

別の例をあげると、100MBitのWAN接続で200msのレイテンシなら12MB/s掛ける0.2sです。2.5MBくらいになります。`c-fill-target`
の初期値は3MBがよいでしょう。

他の設定項目については `drbd.conf` のマニュアルページを参照してください。


[[s-configure-sync-rate-permanent]]
==== 永続的な固定同期速度の設定

ごく限られた状況footnote:[ベンチマークなど]では、固定同期速度を使うことがあるでしょう。この場合には、まず `c-plan-ahead 0;`
にして可変同期速度調節の機能をオフにします。

そして、リソースがバックグラウンド再同期に使用する最大帯域幅をリソースの `resync-rate` オプションによって決定します。この設定はリソースの
`/etc/drbd.conf` の `disk` セクションに記載します。

[source, drbd]
----------------------------
resource <resource>
  disk {
    resync-rate 40M;
    ...
  }
  ...
}
----------------------------

同期速度の設定は1秒あたりの _バイト_ 単位であり _ビット_ 単位でない点に注意してください。デフォルトの単位は _kibibyte_ で、
`4096` であれば `4MiB` となります。

NOTE: これはあくまでDRBDが行おうとする速度にすぎません。低スループットのボトルネック(ネットワークやストレージの速度)がある場合、設定した速度(いわゆる"理想値")には達しません。


[[s-some_more_hints_about_synchronization]]
==== 同期に関するヒント

同期すべきデータ領域の一部が不要になった場合、例えば、対向ノードとの接続が切れている時にデータを削除した場合などには<<s-trim-discard>>の有効性が感じられるかもしれません。

`c-min-rate` は間違われやすいです。これは同期速度の *最低値* ではなくて、この速度以上の速度低下を *意図的に*
行わないという制限です。ネットワークとストレージの速度、ネットワーク遅延(これは回線共有による影響が大きいでしょう)、アプリケーションIO（関与することは難しいかもしれません）に応じた同期速度の管理まで
*手が及ぶか* どうかによります。


[[s-configure-checksum-sync]]
=== チェックサムベース同期の設定

indexterm:[checksum-based synchronization]<<p-checksum-sync,
チェックサムベース同期>>はデフォルトでは有効になっていません。有効にするには、 `/etc/drbd.conf` のリソース構成に次の行を追加します。

[source, drbd]
----------------------------
resource <resource>
  net {
    csums-alg <algorithm>;
  }
  ...
}
----------------------------

_<algorithm>_ は、システムのカーネル構成内のカーネルcrypto
APIでサポートされる任意のメッセージダイジェストアルゴリズムです。通常は `sha1` 、 `md5` 、 `crc32c` から選択します。

既存のリソースに対してこの変更を行う場合は、 `drbd.conf` を対向ノードと同期し、両方のノードで `drbdadm adjust
<resource>` を実行します。

[[s-configure-congestion-policy]]
=== 輻輳ポリシーと中断したレプリケーションの構成

レプリケーション帯域幅が大きく変動する環境(WANレプリケーション設定で典型的)の場合、レプリケーションリンクは時に輻輳します。デフォルト設定では、プライマリノードのI/Oのブロックを引き起こし、望ましくない場合があります。

その代わりに、進行中の同期を suspend (中断)に設定し、プライマリのデータセットをセカンダリから _pull ahead_
(引き離す)にします。このモードではDRBDはレプリケーションチャネルを開いたままにし、切断モードにはしません。しかし十分な帯域幅が利用できるようになるまで実際にはレプリケートを行いません。

次の例は、DRBD Proxy構成のためのものです。

[source, drbd]
----------------------------
resource <resource> {
  net {
    on-congestion pull-ahead;
    congestion-fill 2G;
    congestion-extents 2000;
    ...
  }
  ...
}
----------------------------

通常は `congestion-fill` と `congestion-extents` を `pull-ahead`
オプションと合わせて設定するのがよい方法でしょう。

`congestion-fill` の値は以下の値の90%にするとよいでしょう。

* DRBD Proxy越しの同期の場合のDRBD Proxyのバッファメモリの割り当て、 または
* DRBD Proxy構成でない環境でのTCPネットワークの送信バッファ

`congestion-extents` の値は、影響するリソースの `al-extents` に設定した値の90%がよいでしょう。


[[s-configure-io-error-behavior]]
=== I/Oエラー処理方針の設定

indexterm:[I/O errors]indexterm:[drbd.conf]DRBDが
<<s-handling-disk-errors,下位レベルI/Oエラーを処理する際の方針>>は、リソースの `/etc/drbd.conf` の
`disk` セクションの `on-io-error` で指定します。

[source, drbd]
----------------------------
resource <resource> {
  disk {
    on-io-error <strategy>;
    ...
  }
  ...
}
----------------------------

すべてのリソースのグローバルI/Oエラー処理方針を定義したい場合は、これを `common` セクションで設定します。

_<strategy>_ は以下のいずれかを指定します。

.`detach`
これがデフォルトで、推奨オプションです。下位レベルI/Oエラーが発生すると、DRBDはそのノードの下位デバイスを切り離し、ディスクレスモードで動作を継続します。

.`pass-on`
上位層にI/Oエラーを通知します。プライマリノードの場合は、マウントされたファイルシステムに通知されます。セカンダリノードの場合は無視されます(セカンダリノードには通知すべき上位層がないため)。

.`call-local-io-error`
ローカルI/Oエラーハンドラとして定義されたコマンドを呼び出します。このオプションを使うには、対応する `local-io-error`
ハンドラをリソースの `handlers` セクションに定義する必要があります。`local-io-error`
で呼び出されるコマンド(またはスクリプト)にI/Oエラー処理を実装するかどうかは管理者の判断です。

NOTE: DRBDの以前のバージョン(8.0以前)にはもう1つのオプション panic
があり、これを使用すると、ローカルI/Oエラーが発生するたびにカーネルパニックによりノードがクラスタから強制的に削除されました。このオプションは現在は使用できませんが、
`local-io-error` / `call-local-io-error`
インタフェースを使用すると同じような動作を実現します。ただし、この動作の意味を十分理解した上で使用してください。


次のコマンドで、実行中のリソースのI/Oエラー処理方針を再構成することができます。

  * `/etc/drbd.d/<resource>.res` のリソース構成の編集
  
  * 構成の対向ノードへのコピー
  
  * 両ノードでの `drbdadm adjust <resource>` の実行


[[s-configure-integrity-check]]
=== レプリケーショントラフィックの整合性チェックを設定

indexterm:[replication traffic integrity
checking]<<s-integrity-check,レプリケーショントラフィックの整合性チェック>>
はデフォルトでは有効になっていません。有効にする場合は、 `/etc/drbd.conf` のリソース構成に次の行を追加します。

[source, drbd]
----------------------------
resource <resource>
  net {
    data-integrity-alg <algorithm>;
  }
  ...
}
----------------------------

_<algorithm>_ は、システムのカーネル構成内のカーネルcrypto
APIでサポートされる任意のメッセージダイジェストアルゴリズムです。通常は `sha1` 、 `md5` 、 `crc32c` から選択します。

既存のリソースに対してこの変更を行う場合は、 `drbd.conf` を対向ノードと同期し、両方のノードで `drbdadm adjust
<resource>` を実行します。

WARNING: この機能は本番環境での使用は想定していません。データ破壊の問題や、通信経路(ネットワークハードウェア、ドライバ、スイッチ)に障害があるかどうかを診断する場合にのみ使用してください。

[[s-resizing]]
=== リソースのサイズ変更

[[s-growing-online]]
==== オンライン拡張

indexterm:[resource]動作中(オンライン)に下位ブロックデバイスを拡張できる場合は、これらのデバイスをベースとするDRBDデバイスについても動作中にサイズを拡張することができます。その際に、次の2つの条件を満たす必要があります。

. 影響を受けるリソースの下位デバイスが、 LVMやEVMSなどの論理ボリューム管理サブシステムによって管理されている。

. 現在、リソースのコネクションステータスが `Connected` になっている。

両方のノードの下位ブロックデバイスを拡張したら、一方のノードだけがプライマリ状態であることを確認してください。プライマリノードで次のように入力します。

----------------------------
# drbdadm resize <resource>
----------------------------

新しいセクションの同期がトリガーされます。同期はプライマリノードからセカンダリノードへ実行されます。

追加する領域がクリーンな場合には、追加領域の同期を--assume-cleanオプションでスキップできます。

----------------------------
# drbdadm -- --assume-clean resize <resource>
----------------------------

[[s-growing-offline]]
==== オフライン拡張する

indexterm:[resource]<<s-external-meta-data,外部メタデータ>>を使っている場合、DRBD停止中に両ノードの下位ブロックデバイスを拡張すると、新しいサイズが自動的に認識されます。管理者による作業は必要ありません。両方のノードで次にDRBDを起動した際に、DRBDデバイスのサイズが新しいサイズになり、ネットワーク接続が正常に確立します。

DRBDリソースで<<s-internal-meta-data,内部メタデータ>>を使用している場合は、リソースのサイズを変更する前に、メタデータを拡張されるデバイス領域の後ろの方に移動させる必要があります。これを行うには次の手順を実行します。

WARNING: これは高度な手順です。慎重に検討した上で実行してください。

* DRBDリソースを停止します。

[source, drbd]

----------------------------
# drbdadm down <resource>
----------------------------

* 縮小する前に、メタデータをテキストファイルに保存します。

----------------------------
# drbdadm dump-md <resource> > /tmp/metadata
----------------------------

各ノードごとにそれぞれのダンプファイルを作成する必要があります。この手順は、両方のノードでそれぞれ実行します。一方のノードのメタデータのダンプを対向ノードに
*コピーしないでください* 。 DRBDは *動作しなくなります* 。

* 両方のノードの下位ブロックデバイスを拡大します。

* `/tmp/metadata` ファイルのサイズ情報( `la-size-sect` )を書き換えます。`la-size-sect`
  は、必ずセクタ単位で指定する必要があります。

* メタデータ領域の再初期化をします。

----------------------------
# drbdadm create-md <resource>
----------------------------

* 両ノードで修正したメタデータをインポートします。

----------------------------
# drbdmeta_cmd=$(drbdadm -d dump-md <resource>)
# ${drbdmeta_cmd/dump-md/restore-md} /tmp/metadata
Valid meta-data in place, overwrite? [need to type 'yes' to confirm]
yes
Successfully restored meta data
----------------------------

NOTE: この例では `bash`
パラメータ置換を使用しています。他のシェルの場合、機能する場合もしない場合もあります。現在使用しているシェルが分からない場合は、 `SHELL`
環境変数を確認してください。

* 再度DRBDリソースを起動します。

----------------------------
# drbdadm up <resource>
----------------------------

* 一方のノードでDRBDリソースを昇格します。

----------------------------
# drbdadm primary <resource>
----------------------------

* 最後に、拡張したDRBDデバイスを活用するために、ファイルシステムを拡張します。


[[s-shrinking-online]]
==== オンライン縮小


WARNING: 警告 : オンラインでの縮小は外部メタデータ使用の場合のみサポートしています。

indexterm:[resource]DRBDデバイスを縮小する前に、DRBDの上位層(通常はファイルシステム)を縮小 _しなければいけません_
。ファイルシステムが実際に使用している容量を、DRBDが知ることはできないため、データが失われないように注意する必要があります。

NOTE: ファイルシステムをオンラインで縮小できるかどうかは、使用しているファイルシステムによって異なります。ほとんどのファイルシステムはオンラインでの縮小をサポートしません。XFSは縮小そのものをサポートしません。

オンラインでDRBDを縮小するには、その上位に常駐するファイルシステムを縮小した_後に_、次のコマンドを実行します。

[source, drbd]
----------------------------
# drbdadm resize --size=<new-size> <resource>
----------------------------

_<new-size>_
には通常の乗数サフィックス(K、M、Gなど)を使用できます。DRBDを縮小したら、DRBDに含まれるブロックデバイスも縮小できます(デバイスが縮小をサポートする場合)。

NOTE: DRBDのメタデータがボリュームの想定される箇所に *実際に* 書き込まれるように下位デバイスのリサイズ後に `drbdadm resize
<resource>` を実行してもよいでしょう。

[[s-shrinking-offline]]
==== オフライン縮小

indexterm:[resource]DRBDが停止しているときに下位ブロックデバイスを縮小すると、次にそのブロックデバイスを接続しようとしてもDRBDが拒否します。これは、ブロックデバイスが小さすぎる(外部メタデータを使用する場合)、またはメタデータを見つけられない(内部メタデータを使用する場合)ことが原因です。この問題を回避するには、次の手順を行います
(<<s-shrinking-online,オンライン縮小>>を使用出来ない場合)。


WARNING: これは高度な手順です。慎重に検討した上で実行してください。

* DRBDがまだ動作している状態で、一方のノードのファイルシステムを縮小します。

* DRBDリソースを停止します。

----------------------------
# drbdadm down <resource>
----------------------------

* 縮小する前に、メタデータをテキストファイルに保存します。

----------------------------
# drbdadm dump-md <resource> > /tmp/metadata
----------------------------

各ノードごとにそれぞれのダンプファイルを作成する必要があります。この手順は、両方のノードでそれぞれ実行します。一方のノードのメタデータのダンプを対向ノードに
*コピーしないでください* 。 DRBDは *動作しなくなります* 。

* 両方のノードの下位ブロックデバイスを縮小します。

* `/tmp/metadata` ファイルのサイズ情報( `la-size-sect` )を書き換えます。`la-size-sect`
  は、必ずセクタ単位で指定する必要があります。

* *内部メタデータを使用している場合は、メタデータ領域を再初期化します(この時点では、縮小によりおそらく内部メタデータが失われています)。
+
----------------------------
# drbdadm create-md <resource>
----------------------------

* 両ノードで修正したメタデータをインポートします。
+
----------------------------
# drbdmeta_cmd=$(drbdadm -d dump-md <resource>)
# ${drbdmeta_cmd/dump-md/restore-md} /tmp/metadata
Valid meta-data in place, overwrite? [need to type 'yes' to confirm]
yes
Successfully restored meta data
----------------------------

NOTE: この例では `bash`
パラメータ置換を使用しています。他のシェルの場合、機能する場合もしない場合もあります。現在使用しているシェルが分からない場合は、 `SHELL`
環境変数を確認してください。

* 再度DRBDリソースを起動します。
+
----------------------------
# drbdadm up <resource>
----------------------------


[[s-disable-flushes]]
=== 下位デバイスのフラッシュを無効にする

CAUTION: バッテリバックアップ書き込みキャッシュ(BBWC)を備えたデバイスでDRBDを実行している場合にのみ、デバイスのフラッシュを無効にできます。ほとんどのストレージコントローラは、バッテリが消耗すると書き込みキャッシュを自動的に無効にし、バッテリが完全になくなると即時書き込み(ライトスルー)モードに切り替える機能を備えています。このような機能を有効にすることを強くお勧めします。

BBWC機能を使用していない、またはバッテリが消耗した状態でBBWCを使用しているときに、DRBDのフラッシュを無効にすると、
_データが失われるおそれがあります_ したがって、これはお勧めできません。

DRBDは<<s-disk-flush-support,下位デバイスのフラッシュ>>を、レプリケートされたデータセットとDRBD独自のメタデータについて、個別に有効と無効を切り替える機能を備えています。この2つのオプションはデフォルトで有効になっています。このオプションのいずれか(または両方)を無効にしたい場合は、DRBD設定ファイルの
`/etc/drbd.conf` の `disk` セクションで設定できます。

レプリケートされたデータセットのディスクフラッシュを無効にするには、構成に次の行を記述します。

[source, drbd]
----------------------------
resource <resource>
  disk {
    disk-flushes no;
    ...
  }
  ...
}
----------------------------


DRBDのメタデータのディスクフラッシュを無効にするには、次の行を記述します。

[source, drbd]
----------------------------
resource <resource>
  disk {
    md-flushes no;
    ...
  }
  ...
}
----------------------------

リソースの構成を修正し、また、もちろん両ノードの `/etc/drbd.conf`
を同期したら、両ノードで次のコマンドを実行して、これらの設定を有効にします。

----------------------------
# drbdadm adjust <resource>
----------------------------


1台のサーバのみがBBWCがある場合footnote:[例えばDRサイトでは異なるハードウェアを使うでしょう]には、ホストセクションに以下のような設定をしてください。

[source, drbd]
----------------------------
resource <resource> {
  disk {
    ... common settings ...
  }

  on host-1 {
    disk {
      md-flushes no;
    }
    ...
  }
  ...
}
----------------------------


[[s-configure-split-brain-behavior]]
=== スプリットブレイン時の動作の設定

[[s-split-brain-notification]]
==== スプリットブレインの通知

スプリットブレインが _検出される_ と、DRBDはつねに `split-brain`
ハンドラを呼び出します(設定されていれば)。このハンドラを設定するには、リソース構成に次の項目を追加します。

----------------------------
resource <resource>
  handlers {
    split-brain <handler>;
    ...
  }
  ...
}
----------------------------

_<handler>_ はシステムに存在する任意の実行可能ファイルです。

DRBDディストリビューションでは `/usr/lib/drbd/notify-split-brain.sh`
という名前のスプリットブレイン対策用のハンドラスクリプトを提供しています。これは指定したアドレスに電子メールで通知を送信するだけのシンプルなものです。`root@localhost`
(このアドレス宛のメールは実際のシステム管理者に転送されると仮定)にメッセージを送信するようにハンドラを設定するには、 `split-brain
handler` を次のように記述します。

----------------------------
resource <resource>
  handlers {
    split-brain "/usr/lib/drbd/notify-split-brain.sh root";
    ...
  }
  ...
}
----------------------------

実行中のリソースで上記の変更を行い(ノード間で設定ファイルを同期すれば)、後はハンドラを有効にするための他の操作は必要ありません。次にスプリットブレインが発生すると、DRBDが新しく設定したハンドラを呼び出します。

[[s-automatic-split-brain-recovery-configuration]]
==== スプリットブレインからの自動復旧ポリシー

CAUTION: スプリットブレイン(またはその他)のシナリオの結果、データの相違状況を自動的に解決するDRBDの構成は、潜在的な *自動データ損失*
を構成することを意味します。その意味をよく理解し、それを意味しない場合は設定しないでください。

TIP: むしろ、フェンシングポリシー、クォーラム設定、クラスタマネージャの統合、クラスタマネージャの冗長化通信リンクなどを調べて、まず最初にデータの相違状況をつくらないようにすべてきです。

スプリットブレインからの自動復旧ポリシーには、状況に応じた複数のオプションが用意されています。DRBDは、スプリットブレインを検出したときのプライマリロールのノードの数にもとづいてスプリットブレイン回復手続きを適用します。そのために、DRBDはリソース設定ファイルの
net セクションの次のキーワードを読み取ります。

.`after-sb-0pri`
スプリットブレインが検出されたときに両ノードともセカンダリロールの場合に適用されるポリシーを定義します。次のキーワードを指定できます。

* `disconnect`: 自動復旧は実行されません。 `split-brain` ハンドラスクリプト(設定されている場合)を呼び出し、
  コネクションを切断して切断モードで続行します。

* `discard-younger-primary`: 最後にプライマリロールだったホストに加えられた変更内容を破棄して ロールバックします。

* `discard-least-changes`:変更が少なかったほうのホストの変更内容を破棄してロールバックします。

* `discard-zero-changes`: 変更がなかったホストがある場合は、 他方に加えられたすべての変更内容を適用して 続行します。

.`after-sb-1pri`
スプリットブレインが検出されたときにどちらか1つのノードがプライマリロールである場合に適用されるポリシーを定義します。次のキーワードを指定できます。

* `disconnect`: `after-sb-0pri` と同様に `split-brain` ハンドラスクリプト(構成されている場合)を呼び出し、
  コネクションを切断して切断モードで続行します。

* `consensus`: `after-sb-0pri` で設定したものと同じ復旧ポリシーが適用されます。 これらのポリシーを適用した後で、
  スプリットブレインの犠牲ノードを選択できる場合は自動的に解決します。それ以外の場合は、 `disconnect` を指定した場合と同様に動作します。

* `call-pri-lost-after-sb`: `after-sb-0pri` で指定した復旧ポリシーが適用されます。
  これらのポリシーを適用した後で、 スプリットブレインの犠牲ノードを選択できる場合は、犠牲ノードで `pri-lost-after-sb`
  ハンドラを起動します このハンドラは `handlers` セクションで設定する必要があります。 また、クラスタからノードを強制的に削除します。

* `discard-secondary`: 現在のセカンダリロールのホストを、 スプリットブレインの犠牲ノードにします。

.`after-sb-2pri`
スプリットブレインが検出されたときに両ノードともプライマリロールである場合に適用されるポリシーを定義します。このオプションは
`after-sb-1pri` と同じキーワードを受け入れます。ただし、 `discard-secondary` と `consensus`
は除きます。

NOTE: 上記の3つのオプションで、DRBDは他のキーワードも認識しますが、それらはめったに使用されないためここでは省略します。ここで説明されていないスプリットブレインの復旧キーワードに関しては
`drbd.conf` のマニュアルページを参照ください。

たとえば、デュアルプライマリモードでGFSまたはOCFS2ファイルシステムのブロックデバイスとして機能するリソースの場合、次のように復旧ポリシーを定義できます。

----------------------------
resource <resource> {
  handlers {
    split-brain "/usr/lib/drbd/notify-split-brain.sh root"
    ...
  }
  net {
    after-sb-0pri discard-zero-changes;
    after-sb-1pri discard-secondary;
    after-sb-2pri disconnect;
    ...
  }
  ...
}
----------------------------


[[s-three-nodes]]
=== スタック3ノード構成の作成

3ノード構成では、1つのDRBDデバイスを別のデバイスの上に _スタック_ (積み重ね)します。

NOTE: DRBD9.xでは1つの階層で複数ノードを使用できるのでスタッキングは非推奨です。詳細は <<s-drbdconf-conns>>をご参照ください。

[[s-stacking-considerations]]
==== デバイススタックの留意事項

次のような事項に注意する必要があります。

* スタックデバイス側が利用可能です。1つのDRBDデバイス `/dev/drbd0` を設定して、その上位にスタックデバイス `/dev/drbd10`
  があるとします。この場合は、 `/dev/drbd10` がマウントして使用するデバイスになります 。

* 下位のDRBDデバイス _および_ スタックDRBDデバイス(上位DRBDデバイス)の両方にそれぞれメタデータが存在します 。上位デバイスには、必ず
  <<s-internal-meta-data,内部メタデータ>>を使用してください。このため、 3ノード構成時の使用可能なディスク領域は、
  2ノード構成に比べてわずかに小さくなります。

* スタックした上位デバイスを実行するには、下位のデバイスが プライマリロールになっている必要があります。

* バックアップノードにデータを同期するには、 アクティブなノードのスタックデバイスがプライマリモードで動作している必要があります。


[[s-three-node-config]]
==== スタックリソースの設定

次の例では `alice` 、 `bob` 、 `charlie` という名前のノードがあり、 `alice` と `bob`
が2ノードクラスタを構成し、 `charlie` がバックアップノードになっています。

[source, drbd]
----------------------------
resource r0 {
  protocol C;
  device    /dev/drbd0;
  disk      /dev/sda6;
  meta-disk internal;

  on alice {
    address    10.0.0.1:7788;
  }

  on bob {
    address   10.0.0.2:7788;
  }
}

resource r0-U {
  protocol A;

  stacked-on-top-of r0 {
    device     /dev/drbd10;
    address    192.168.42.1:7789;
  }

  on charlie {
    device     /dev/drbd10;
    disk       /dev/hda6;
    address    192.168.42.2:7789; # Public IP of the backup node
    meta-disk  internal;
  }
}
----------------------------

他の `drbd.conf`
設定ファイルと同様に、この設定ファイルもクラスタのすべてのノード(この場合は3つ)に配布する必要があります。非スタックリソースの設定にはない、次のキーワードにご注意ください。

.`stacked-on-top-of`
このオプションによって、DRBDに含まれるリソースがスタックリソースであることをDRBDに知らせます。これは、通常リソース設定内にある `on`
セクションのうちの１つを置き換えます。`stacked-on-top-of` は下位レベルのリソースには使用しないでください。


NOTE: スタックリソースに<<fp-protocol-a,Protocol
A>>を使用することは必須ではありません。アプリケーションに応じて任意のDRBDのレプリケーションプロトコルを選択できます。

[[f-single-stacked]]
.単一スタックの構成
image::images/single-stacked.svg[]


[[s-three-node-enable]]
==== スタックリソースを有効にする 

スタックリソースを有効にするには、まず、下位レベルリソースを有効にしてどちらか一方をプライマリに昇格します。
----------------------------
drbdadm up r0
drbdadm primary r0
----------------------------

非スタックリソースと同様に、スタックリソースの場合もDRBDメタデータを作成する必要があります。次のコマンドで実行します。次に、スタックリソースを有効にします。

----------------------------
# drbdadm create-md --stacked r0-U
----------------------------

この後でバックアップノードのリソースを起動し、3ノードレプリケーションを有効にします。

----------------------------
# drbdadm up --stacked r0-U
# drbdadm primary --stacked r0-U
----------------------------

この後でバックアップノードのリソースを起動し、3ノードレプリケーションを有効にします。

----------------------------
# drbdadm create-md r0-U
# drbdadm up r0-U
----------------------------

クラスタ管理システムを使えばスタックリソースの管理を自動化できます。Pacemakerクラスタ管理フレームワークで管理する方法については、<<s-pacemaker-stacked-resources>>を参照してください。

[[s-permanently-diskless-nodes]]
=== 永続的なディスクレスノード

ノードはしばしDRBDの中で永続的にディスクレスになるときがあります。以下はディスクをもつ３つのノードと永続的なディスクレスノードの構成例です。

-------------------------------------
resource kvm-mail {
  device      /dev/drbd6;
  disk        /dev/vg/kvm-mail;
  meta-disk   internal;

  on store1 {
    address   10.1.10.1:7006;
    node-id   0;
  }
  on store2 {
    address   10.1.10.2:7006;
    node-id   1;
  }
  on store3 {
    address   10.1.10.3:7006;
    node-id   2;
  }

  on for-later-rebalancing {
    address   10.1.10.4:7006;
    node-id   3;
  }

  # DRBD "client"
  floating 10.1.11.6:8006 {
    disk      none;
    node-id   4;
  }

  # rest omitted for brevity
  ...
}
-------------------------------------

永続的なディスクレスノードはビットマップスロットが割り当てられません。このようなノードのステータスは、エラーでも予期せぬ状態でもないので緑で表示されます。

[NOTE]
====================
DRBDクライアントはワイアデータ通信の簡単な実装ですが、iSCSIの _Persistent Reservations_
のような高度な機能はありません。しかし、仮想マシン等で使われる _read_, _write_, _trim_/_discard_, _resize_
のような基本的なIOのみが必要な場合には、適切に動作します。
====================

[[s-rebalance-workflow]]
=== データ再配置

indexterm:[rebalance]以下の例では、すべてのデータ領域は3台に冗長化するポリシーを想定しています。したがって最小構成で3台のサーバが必要です。

しかし、データ量が増えてくると、サーバ追加の必要性に迫られます。その際には、また新たに3台のサーバを購入する必要はなく、１つのノードだけを追加をしてデータを
_再配置_ することができます。


.DRBDデータ再配置
image::images/rebalance.svg[]

上の図では、3ノードの各々に25TiBのボリュームがある合計75TiBの状態から、4ノードで合計100TiBの状態にしています。

データを再配置する時には、 _新しい_ ノードとDRBDリソースを削除したいノードを選択する必要があります。現在 _アクティブ_
なノードからリソースを削除する場合には注意が必要です。 DRBDが `プライマリ`
のノードからリソースを削除する場合にはサービスを移行するか、そのノードのリソースを<<s-drbd-client,DRBDクライアント>>として稼働させるかにしてください。通常は
`セカンダリ` のノードから選ぶほうが簡単です(それができない場合もあるでしょうが)。


[[s-prepare_a_bitmap_slot]]
==== ビットマップスロットの用意

移動するリソースには一時的に未使用の<<s-quick-sync-bitmap,ビットマップスロット>>用のスペースが必要です。

<<s-first-time-up,`drbdadm create-md` の時>>にもう一つ割り当てます。または、 `drbdadm`
がもう１つスロットを取っておくように設定にプレースホルダを設けてもよいでしょう。

---------
resource r0 {
  ...
  on for-later-rebalancing {
    address   10.254.254.254:65533;
    node-id   3;
  }
}
---------

[NOTE]
=================
このスロットを実際に利用するには以下が必要です。

. メタデータをダンプする,
. メタデータ領域を拡大する,
. ダンプファイルを編集する,
. メタデータをロードする

今後のバージョンの `drbdadm` ではショートカットキーが用意されます。 `drbdadm resize --peers N`
が使用できるようになる予定であり、カーネルがメタデータを書き換えられるようになります。
=================


[[s-preparing_and_activating_the_new_node]]
==== 新しいノードの用意と有効化

まずは新しいノードに( `lvcreate` 等で)下位のストレージリュームを作成する必要があります。
次に設定ファイルのプレイスホルダーに現在のホスト名、アドレス、ストレージのパスを書き込みます。そしてすべての必要なノードにリソース設定をコピーします。


新しいノードでメタデータを次のようにして(一度だけ)初期化します

-------------
# drbdadm create-md <resource>
v09 Magic number not found
Writing meta data...
initialising activity log
NOT initializing bitmap
New drbd meta data block sucessfully created.
-------------


[[s-starting_the_initial_sync]]
==== 初期同期の開始

新しいノードでデータを受け取ります。

そのために以下のようにして既存のノードでネットワークコネクションを定義します。

-------------------------------------
# drbdadm adjust <resource>
-------------------------------------

そして新しいノードでDRBDデバイスを開始します。

-------------------------------------
# drbdadm up <resource>
-------------------------------------


[[s-check_connectivity]]
==== 接続確認

この時に、新しいノードで以下のコマンドを実行してください。
------------
# drbdadm status <resource>
------------

他のノード _すべて_ が接続しているか確認します。

[[s-after_the_initial_sync]]
==== 初期同期後

新しいノードが `UpToDate` になったらすぐに、設定中の他のノードのうち１つが `for-later-rebalancing`
に変更できるようになり、次のマイグレーション用にキープしておけるようになります。　

NOTE: 次の再配置用に小さなビットマップスロットしかない新規ノードに `drbdadm create-md`
をすることにはリスクがあると思う方もいることでしょう。あらかじめIPアドレスとホスト名を用意しておくのが簡単な方法でしょう。

再度変更した設定をコピーして、以下のコマンドを

-------------------------------------
# drbdadm adjust <resource>
-------------------------------------

全ノードで実行します。


[[s-cleaning_up]]
==== クリーニング

今までデータのあったノードでは、もうリソースを使用しない場合にはDRBDデバイスを次のようにして停止できます。

-------------------
# drbdadm down <resource>
-------------------

これで下位デバイスを使用しなくなり、他の用途で使用することができるようになります。論理ボリュームであれば lvremove
でボリュームグループに戻すことができます。

[[s-conclusion_and_further_steps]]
==== まとめと他のステップ

１つのリソースを新しいノードに移動しました。同様の手順で１つまたはそれ以上のリソースを、クラスタ内の既存の2つまたは3つのノードの空きスペースに行う事も可能です。

再び3重の冗長化を行うのに必要な空き容量のあるノードが揃えば、新しいリソースを設定することが出来ます。

なお、上記の手順をDRBD Manageを使用して行う場合については<<s-dm-rebalance>>をご参照ください。


[[s-configuring-quorum]]
=== クォーラム設定

スプリットブレインや複製データの相違を防ぐために、フェンシングを構成する必要があります。フェンシングのすべてのオプションは、最後には冗長な通信路に依存します。それはノードが対応ノードのIPMIネットワークインタフェースに接続する形かもしれません。crm-fence-peerスクリプトの場合、DRBDのネットワークリンクが切断されたときに、pacemaker
の通信が有効であることが必要になります。

一方クォーラムは完全に異なるアプローチをとります。基本的な考え方は、通信できるノードの数がノード総数の半分を超える場合、クラスタパーティションは複製されたデータセットを変更できるということです。そのようなパーティションのノードは、クォーラを持つといいます。言い換えると、クォーラムを持たないノードは、複製されたデータを変更しないことを保証します。これはデータの相違を作成しないことを意味します。

DRBDのクォーラムの実装は、`quorum` リソースに `majority` , `all` または数値を設定することで有効にできます。
`majority` の動作が、前の文章で説明した動作になります。

[[s-guaranteed_minimal_redundancy]]
==== 保証された最低限の冗長化

デフォルトでは、ディスクを持つすべてのノードがクォーラム選挙で投票権を持ちます。言い換えると、ディスクレスノードだけが持ちません。従って、3ノードクラスタでは、
_Inconsistent_ ディスクを持つ2つのノード、 _UpToDate_ を持つ1つノードは、クォーラム投票権を持つことができます。
`quorum-minimum-redundancy` を設定することによって、`UpToDate`
ノードだけがクォーラム選挙で投票権を持つように変更することもできます。このオプションは `quorum` オプションと同じ引数をとります。

このオプションを使用すると、サービスを開始する前に必要な再同期操作が完了するまで待機する、ということも表現できます。
したがって、データの最低限の冗長性がサービスの可用性よりも保証されることを好む方法です。金融データとサービスなどはその一例にです。

以下に5ノードクラスタの設定例を示します。パーティションは少なくとも3つのノードが必要であり、そのうち2つは `UpToDate`
である必要があります。
-------------------------------------
resource quorum-demo {
  quorum majority;
  quorum-minimum-redundancy 2;
  ...
}
-------------------------------------

[[s-actions_on_loss_of_quorum]]
==== クォーラムを失った時の動作

サービスを実行しているノードがクォーラムを失うと、すぐにデータセットの書き込み操作を中止する必要があります。これはIOがすぐにすべてのIO要求をエラーで完了し始めることを意味します。通常、これは、正常なシャットダウンが不可能であることを意味します。これはデータセットをさらに変更する必要があるためです。IOエラーはブロックレベルからファイルシステムに、ファイルシステムからユーザースペースアプリケーションに伝わります。

理想的には、IOエラーの場合、アプリケーションを単に終了させることです。これはその後、Pacemaker
がファイルシステムをアンマウントし、DRBDリソースを降格させセカンダリロールに移すことを可能にします。もしそうであるなら、`on-no-quorum`
リソースに `io-error` を設定してください。以下はその設定例です。

-------------------------------------
resource quorum-demo {
  quorum majority;
  on-no-quorum io-error;
  ...
}
-------------------------------------

アプリケーションが最初のIOエラーで終了しない場合は、IOをフリーズさせノードをリブートさせることもできます。以下はその設定例です。

-------------------------------------
resource quorum-demo {
  quorum majority;
  on-no-quorum suspend-io;
  ...

  handlers {
    quorum-lost "echo b > /proc/sysrq-trigger";
  }
  ...
}
-------------------------------------

